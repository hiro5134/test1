<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });

</script>

# [My kicks](#mykicks)

## <a href="#index">Index</a><a id="index"></a>
* [Chapter 1](#cp1)
    * [Library](#lib)
    * [Higher API](#higherapi)
* [Chapter 2](#cp2)
    * [活性化関数](#mlp)
    * [万能近似定理 Universal Approximation Theorem](#uat)
    * [出力ユニットとタスク](#unit)
    * [損失関数](#loss)
* [Chapter 3](#cp3)
    * [バッチ処理](#batch)
    * [ミニバッチ処理](#minibatch)
    * [最適化手法](#adapt)
        * [最急降下法](#gd)
        * [確率的最急降下法](#sgd)
        * [モメンタム](#momentum)
        * [ネステロフのモメンタム](#momentum2)
        * [AdaGrad](#adagrad)
        * [RMSProp](#rmsp)
        * [Adam](#adam)
* [Chapter 4](#cp4)

## <a id = "cp1">Chapter 1</a>
1. ## <a id="lib">Library</a>
    | Lib | Graph | Company | Year | memo |
    | - | - | - | - | - |
    | TensorFlow | Define and Run | Google | 2015 | 計算グラフを自分で定義 |
    | Chainer | Define by Run | Preferred Networks | 2015 | 2019年開発終了 |
    | PyTorch | Define by Run | Facebook | 2016 | 計算方法がnumpyに似てる |
    | TensorFlow 2 | Define by Run | Google | 2019 | シンプルな実装可能 |
    | Apache MXNet | Define and Run <br> Define by Run| AWS | 2017 | 静的・動的どちらも可能|
    | Caffe2 | Define and Run | Facebook | 2017 | 2018年PyTorchへ統合 |
    | Congnitive Toolkit (CNTK) | Define and Run | Microsoft | 2016 | 音声認識特化 |
    | theano | Define and Run | モントリオール大学 | 2007 | 2017年開発終了 |
1. ## <a id="higherapi">Higher API</a>
    | API | Company | Year | memo |
    | - | - | - | - |
    | Keras |  Google | 2015 | TF用のAPI(TF2から標準API) |
    | Gluon | AWS, Microsoft | 2017 | MXNet用のAPI |

## <a id = "cp2">Chapter 2</a>
1. ## <a id="activation">活性化関数</a>
    中間層の表現を非線形にするため．  
    表現力の向上
    - step関数  
        勾配が0になり最適化が難しい
    - sigmoid関数
        0~1  
        勾配消失が起こり，中間層の最適化に不向き
    - softmax関数  
        複数の入力を扱える  
        出力の合計が1
        勾配消失が起こり，中間層の最適化に不向き
    - tanh関数  
        -1~1
        勾配消失が起こり，中間層の最適化に不向き
    - ReLU関数  
        x>0で勾配が計算可能で勾配が消失しない  
        x<0での勾配を計算するために以下の派生が存在  
        - Leaky ReLU  
            x<0に緩やかな傾き
        - Randomized ReLU  
            x<0の傾きをランダムに変更(予測時は平均値に固定)
        - Parametric ReLU  
            x<0の傾きを学習
    - MAXOUT関数  
        ReLU関数の一般化  
        要素数kのグループから各グループの最大値を抽出  
        ReLU関数や二次関数などに近似可能
        ReLu関数よりも表現力が高く勾配が消えない   
        活性化関数自体を学習

1. ## <a id="uat">万能近似定理 Universal Approximation Theorem</a>
    3層モデルの中間層のノードを極限まで増やせばあらゆる関数を近似可能  
    →現実的でない  
    →活性化関数により，層を横に伸ばすことで表現力をあげられる（計算コストも少なく収まる）  
    →万能近似定理+活性化関数でDLが飛躍

1. ## <a id="unit">出力ユニットとタスク</a>
    | Task |  | Unit | Activation Func. | Loss Func. | Probability | memo |
    | - | - | - | - | - | - | - |
    | 回帰 |  | 線形ユニット | 恒等関数 | MSE | ガウス分布 | 連続値を予測<br>目的変数（連続値）の分布をガウス分布と仮定<br>出力値は目的変数の分布（条件付きガウス分布）の平均値を返そうとする|
    | 分類 | 二値分類 | シグモイドユニット | sigmoid関数 | Binary Cross Entropy | ベルヌーイ分布 | ラベルが1である確率を予測<br>目的変数の分布は二項分布<br>ベルヌーイ分布を出力|
    | | 多値分類 | softmaxユニット | softmax関数 | Cross Entropy | マルチヌーイ分布 | ラベルごとに確率を予測 |

1. ## <a id="loss">損失関数</a>
    尤度の最大化　損失関数の最小化　となるように設計

## <a id = "cp3">Chapter 3</a>
1. ## <a id="batch">バッチ処理</a>
    オンライン学習 入力１つ
    バッチ処理　入力nこ　あらゆる場所がn次元になる
        sum撮るときはdim=1
        基本サンプルが行，説明変数が列なので列方向に足していくことでサンプルごとのsumが取れる

1. ## <a id="minibatch">ミニバッチ処理</a>
    バッチ処理　データ数が膨大であれば計算量も膨大にありパラメータの更新が遅い
    →データを分割してバッチ処理をする
    | Task | 勾配推定 | 収束速度 |
    | - | - | - |
    | バッチサイズ大 | 正確 | 遅い |
    | バッチサイズ小 | 誤差を高める | 早い |
    　　
1. ## <a id="adapt">最適化手法</a>
    1. ## <a id="gd">最急降下法 Gradient descent</a>
        全体像はわからないが，その地点での最短経路方向に更新
        →求めた勾配方向とは逆向きに，その大きさ*η(学習率)を更新
        →最急降下法ではlocal minimaに陥る可能性

        数式
        $$
          w_{t+1} = w_t -\eta\left(
            \frac{\partial L}{\partiel w}
            \right)_{w=w_t}
        $$
        ハイパーパラメータ
        $\eta$ : 学習率

        η小　時間がかかる　最適な場所を見つけられる可能性
        η大　早い　値がブレて最適な場所を見つけられない可能性
    1. ## <a id="sgd">SGD 確率的勾配降下法 stochastic gradient descent</a>
        確率的な山の一地点で最短経路方向に更新
        ミニバッチ抽出にランダム性を持たせ，損失関数が確率的になる
        →手法は最急降下法と同じ

        数式
        →最急降下法と同じ
        ハイパーパラメータ
        →最急降下法と同じ

        最急降下法の弱点を克服
        （これまで）学習率を確率的に変化
        →損失関数が複雑すぎてうまくいかない

        そこでSDGはデータを確率的に変える
        →データをシャッフルして，それぞれの損失関数の挙動を変える
        →あるミニバッチでは上がり，他方では下がる，などができ最適な場所を見つけられる

        $$
          \bm{W} = \bm{W} - \eta\frac{\partial L}{\partial \bm{W}}
        $$

        一方で損失関数の形状が急峻な場合，振動してしまう

    1. ## <a id="momentum">モメンタム</a>
        移動平均を導入（慣性項）
        精度を保ちつつ，収束速度を早くする

        数式
        $$
          v_t = \beta v_{t-1} + (1-\beta)\nabla_W L(W_{t-1})\\
          W_t = W_{t-1} - \alpha v_t
        $$
        ハイパーパラメータ
        $\alpha$ : 学習率(どれだけ更新させるか)
        $\beta$ : 前回までの勾配の割合

        SDG＋慣性の法則→モメンタム
        →現在の勾配に「前回までの勾配」を加えたベクトルを更新

        Momentum SGD
        各方向の勾配の大きさの減衰和を用いて，次元毎に学習率を変える手法．

    1. ## <a id="momentum2">ネステロフのモメンタム</a>
        慣性項なしで更新した場合に慣性項をたしあわせ
        →ブレーキの役割

        数式
        $$
          v_t = \beta v_{t-1} + (1-\beta)\nabla_W L(W_{t-1} - \beta v_{t-1})\\
          W_t = W_{t-1} - \alpha v_t
        $$
        ハイパーパラメータ
        $\alpha$ : 学習率(どれだけ更新させるか)
        $\beta$ : 前回までの勾配の割合

        パラメータ更新後の位置で勾配計算．
        モメンタムの慣性項にこの勾配(ブレーキの役割)を加えてベクトルへと更新．
        一歩先の勾配が急峻になっていればその分ブレーキをかける．

        モメンタムとネステロフのモメンタムの違い
        - モメンタム
            現時点での勾配$\nabla_W L(W_{t-1})$と，前回の慣性項$v_t$を別々に計算し加算
        - ネステロフのモメンタム
            前回の慣性項により更新させた地点での勾配$\nabla_W L(W_{t-1} - \beta v_{t-1})$を計算し，前回の慣性項と加算

    1. ## <a id="adagrad">AdaGrad</a>
        過去の勾配の二乗和を記憶して学習率を調整
        →各方向の勾配の大きさを用いて次元毎に学習率を変える手法．学習率は単調に減少する．

        今までの勾配が大きい　学習率小さく更新
        今までの勾配が小さい　学習率大きく更新

        問題点
        学習率が0になる可能性
        →局所解

    1. ## <a id="rmsp">RMSProp</a>
        古い情報を忘れ，新しい情報を反映しやすくした
        →過去の勾配情報と，新しい勾配情報の割合を調整して記憶する
        →過去の勾配の減衰和を用いて更新する手法．学習率は自身でせっていする必要．
        $$
        \begin{align}
          h_t &= \alpha h_{t-1} + (1-\alpha)\left(
            \frac{\partial L}{\partial w}
            \right)_{w=w_t}^2\\ \nonumber
          \eta_t &= \frac{\eta_0}{\sqrt{h_t}}\\
          w_{t+1} &= w_t - \eta_t\left(
            \frac{\partial L}{\partial w}
            \right)_{w=w_t}
        \end{align}
        $$

    1. ## <a id="adam">Adam</a>
        現在のスダンダード
        →勾配ベクトルとその大きさの両者の減衰和を用いて，次元毎に学習率を変える手法．

        イテレーション数でバイアス補正
        →初期段階の不安定さを解消

        $\bm{m}$は速度の概念
        Momentum SDGにおける$\bm{v}$
        減衰率$\beta_1$で過去の勾配情報$\bm{m}$と現在の勾配情報$\frac{\partial L}{\partial \bm{W}}$を調整する
        $$
          \bm{m} = \beta_1\bm{m} + (1 - \beta_1)\frac{\partial L}{\partial \bm{W}}
        $$

        $\bm{v}$過去の勾配の二乗和
        $$
          \bm{v} = \beta_2\bm{v} + (1 - \beta_2)\left(\frac{\partial L}{\partial \bm{W}}\right)^2
        $$

        $\bm{m}, \bm{v}$は減衰率によって減衰させられた勾配の(二乗)和
        0.9が設定されると，勾配の１割しか学習に使われない
        下記で調整
        $$
          \hat{ \bm{m}} = \frac{\bm{m}}{1-\beta_1^t},
          \hat{ \bm{v}} = \frac{\bm{v}}{1-\beta_2^t}
        $$
        tはイテレーション数
        $\beta^t$は0に漸近，$\frac{1}{1-\beta^t}$は1に漸近
        $\bm{m, v}$に過去の情報が蓄積されるまでは勾配を利用
        更新が進むにつれ上式の影響は薄れる

        $\hat{\bm{m}},\hat{\bm{v}}$を用いてパラメータ$\bm{W}$を更新
        $\epsilon$は微小項（0除算のため）
        $$
          \bm{W} = -\eta\frac{\hat{\bm{m}}}{\sqrt{\hat{\bm{v}}}+\epsilon}
        $$

        以上を総合して
        $$
          \bm{W} = -\eta\frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}
          \frac{\bm{m}}{\sqrt{\bm{v}}+\epsilon}
        $$

## <a id = "cp4">Chapter 4</a>
1. ## <a id="">勾配消失問題</a>
    backpropagationにて1より小さい値の掛け算により入力層に近い隠れ層に伝わる勾配がほぼ0になる

    activationに用いられるsigmoidやtanhの微分値は基本＜1であるため

    勾配が小さい→更新がほぼない

    ReLUにより解決の道
    微分値→常に1(x＞0)　→　勾配が消失しにくい
    計算が高速
1. ## <a id="">過学習</a>
    訓練データに過度に適合
    汎化性能が低い

    表現力の高さ→過学習しやすい

    正則化などで解決を図る
    1. ## <a id="">正則化</a>
        機械学習の目的：既知のデータから未知のデータを予測したい
        →既知データに対する損失関数の期待値を減少させる
        →予測値と期待値の差を最小化
        1. ## <a id="">バイアス-バリアンス分解</a>
            損失関数の期待値最小化にあたり，式要素を分解し各項から最小化を図る
            →回帰タスク（損失関数が二乗和誤差）のみ成り立つ

            バリアンス項
            →予測値の分散
            →大ならば過学習

            バイアス項
            →予測値の期待値と目的変数の期待値の差
            →大ならば未学習

            ノイズ項
            →データのノイズ
            →MLでのフィティングには無関係

            バリアンスとバイアスはトレードオフ
            基本的に学習されていることが望まれるため，バリアンスを抑えながら学習を図る
            →正則化
        1. ## <a id="">ノルムペナルティ</a>
            ノルム：ベクトルに対する距離を測る指標
            ハイパパラメータ
            $$
            ||x||_p = \left(|x_1|^p + |x_2|^p + \cdots + |x_D|^p\right)^{\frac{1}{p}}
            $$
            pの値によってL0,L1,L2,L$\infty$が存在

            パラメータが極端な値を取らないように損失関数に制限をかける

            ノルムつき損失関数＝損失関数＋$\lambda$(パラメータのLPノルム)

            ラムダ大　正則化強　過学習抑制　
            抑えすぎると精度が下がる

        1. ## <a id="">ラッソ回帰 Lasso Regression</a>
            L1ノルムによる正則化
            スパースな解を得やすい

            一般に
            通常損失関数とパラメータ制約領域の接点が最適解となりやすい

            ラッソ回帰
            領域が直線のため軸で接しやすい（それぞれのパラメータが0になることが多い）

            $$
              L_{norm}(y(X), t) = L(y(X), t) + \lambda(|w_0| + |w_1| + \cdots + |w_n|)
            $$

        1. ## <a id="">リッジ回帰 Lidge Regression</a>
            L2ノルムによる正則化
            汎化性能の高い解を得やすい

            リッジ回帰
            領域が円状なため，柔軟な値で最適解を取りやすい

            $$
              L_{norm}(y(X), t) = L(y(X), t) + \lambda\sqrt{w_0^2 + w_1^2 + \cdots + w_n^2}
            $$

        1. ## <a id="">elastic Net</a>
            ラッソ回帰とリッジ回帰の折衷案

            $$
              Lasso = \Omega(W) = (|w_0| + |w_1| + \cdots + |w_n|)
            $$
            $$
              Lidge = \Omega(W) = \sqrt{w_0^2 + w_1^2 + \cdots + w_n^2}
            $$
            $$
              elastic Net = \alpha\Omega(W) = \lambda(|w_0| + |w_1| + \cdots + |w_n|) + \frac{(1-\lambda)}{2}\sqrt{w_0^2 + w_1^2 + \cdots + w_n^2}
            $$

        1. ## <a id="">パラメータ拘束</a>
            類似したタスクのモデルのパラメータにノルムペナルティを課す
            モデルA(パラメータ$W^{(A)}$)とモデルB(パラメータ$W^{(B)}$)は類似タスク
            これらの距離を近くする
            下記項を最小化する
            $$
              \lambda\Omega(W^{(A)}), W^{(B)})=\lambda\left( |w_1^{(A)}-w_1^{(B)}|^2 + \cdots + |w_D^{(A)}-w_D^{(B)}|^2\right)^{\frac{1}{2}}
            $$

        1. ## <a id="">パラメータ共有</a>
            モデルの一部のパラメータを同じ値にする
            どこを同じにするかは開発者による
            例えば，MLPのノード間のパラメータを左右対象にする

            パラメータ共有・拘束においても，過剰な表現力に制限をかけるイメージ

        1. ## <a id="">早期終了 Early stopping</a>
            最適化への道を途中で打ち切り，パラメータ制約を行う効果がある

        1. ## <a id="">アンサンブル学習</a>
            複数のモデルを組み合わせて汎化性能向上

            | Name | 特徴 | 正則化効果 | 備考 |
            | - | - | - | - |
            | バギング | 同時多数学習モデル | varianceを下げる | 多数決モデル<br>訓練データから多数のデータセットを作りそれぞれのモデルで学習 |
            | ブースティング | 段階的改善モデル | biasを下げる | 徐々にupdate<br>前のモデルの誤った予測を重みづけ学習 |

        1. ## <a id="">ノイズによる正則化</a>
            ノイズ：ある現象の不確実性から生じるデータや情報

            ノイズを加える位置によって手法が異なる

            入力層：data augmentation
            隠れ層：dropout, dropconect
            目的変数：label smoothing
            1. ## <a id="">Data Augmentation</a>
                学習データに多様性を持たせ，汎化性能を向上させる
                回転拡大縮小切り抜きなど
            1. ## <a id="">Dropout</a>
                訓練時
                指定された割合のノードをランダムに非活性
                →CNNの全結合層等に施す．一般的に50％程度を指定．
                部分的なネットワークのアンサンブル学習
                (in PyTorch 指定した確率pに基づくベルヌーイ分布からのサンプルを持ちに消すノードを選択)

                評価時
                全てのノードを使用
                出力時には1-pを乗算（確率pで消すノードを選択しているため）

                実用性が高い
                学習パラメータが削減されるため計算コスト小さい
                ほぼ全てのモデルに適用可
                訓練データが少ないとうまくいかない傾向
            1. ## <a id="">Dropconnect</a>
                指定された割合のパラメータ（ブランチ　線）ランダムに非活性

                dropoutより優れた性能
                乱数のシードに大きく左右され再現性が困難
            1. ## <a id="">Label Smoothing ラベル平滑化</a>
                目的変数にノイズを加える

                分類タスク(2値分類)
                人のラベル付けに誤りがる可能性
                特定のカテゴリに過剰に適合しやすい（過学習しやすい）

                に対する効果
                多少のラベル付ミスを吸収
                正解カテゴリ以外にも値が入ることで，特定のカテゴリに過剰に適合することが減る

                $$
                  \{0, 0, 1, 0\}
                $$
                に対して各ラベルにノイズを付加(ノイズ*$\alpha$)
                $$
                  \{\alpha\xi_1, \alpha\xi_2, (1-\alpha) + \alpha\xi_3, \alpha\xi_4\}
                $$

    1. ## <a id="">バッチ正規化</a>
        正規化
        平均0, 分散1にスケーリングする
        効果
        スケールの異なる特徴量を同様な基準に扱える
        学習データの分布と検証データの分部生合成を維
        スケールの大きな特徴量に左右されない

        バッチ正気化
        バッチごとに正規化
        効果
        - 学習速度の向上
            - 共変量シフトの減少
            - 大きな学習率でも安定
        - 正則化効果がある
        - NNの重みの初期値に依存しづらい

        (内部)共変量シフト
        学習が難しくなる
        - 各層の入力時のbんぷが学習の過程で変化する現象
            1. パラメータは学習毎に更新
            1. 各層出力分布は学習毎に異なる
            1. 故に各層入力分布が学習毎に異なる→共変量シフトが起こる
        - 共変量シフトで学習が難しくなる理由
            - 学習率を下げる必要．（学習時間が長くなる）
            - パラメータの初期値依存性が高くなる（初期化に注意)

    1. ## <a id="">計算コスト</a>
        1. ## <a id="">高速化</a>
            GPUで並列処理高速化

            - 分散処理
                - データ並列
                    - 同期型
                        全ての並列処理で同じ勾配を用いる
                        勾配を常に一定に保つ
                    - 非同期型
                        アプローチ毎に用いる勾配が異なる
                        勾配を常に更新する
                - モデル並列
        1. ## <a id="">軽量化</a>
            1. ## <a id="">蒸留</a>
                学習済教師モデルの入出力を，軽量な生徒モデルで学習
                soft, hard target lossを最小化
                精度は落ちる反面，軽量なモデルで計算資源を大幅に削減

                soft target loss
                教師モデルと生徒モデルの出力の損失
                hard target loss
                生徒モデルの入出力分布の損失

                知識の蒸留
                精度の高い教師モデルは，粒度の細かい重要な知識を反映できる

                例）猫・犬・ハムスター・へびに分類
                {1, 0, 0, 0}に対して{0.85, 0.1, 0.04, 0.01}を教師モデルが出力
                猫には耳があり，丸い瞳
                犬には多少にているが，蛇には程遠い．→粒度の細かい知識

            1. ## <a id="">枝刈 Pruning</a>
                寄与の小さい重みを0にする（データの流れを切る）
                （値の小さい重みを0にする）

                学習の繰り返しの中で枝刈を実行し，パラメータ削減→精度を保ちつつモデル軽量化

            1. ## <a id="">量子化</a>
                浮動小数点で表現されるパラメータを低ビットで表現（近似）
                精度を落とさずモデル圧縮

                32bitを上位16bitで表現　など

      https://www.anarchive-beta.com/entry/2020/08/16/180000

      https://qiita.com/hikobotch/items/78b53de44069fb19d311

## <a id = "cp5">Chapter 5</a>
1. ## <a id="">画像認識</a>
    - 画像分類
    - 物体検出
    - セグメンテーション

1. ## <a id="">CNN</a>
    1. ## <a id="">畳み込み</a>
        複数チャネル
            カーネルを入力の次元分用意（カラー画像なら3カーネル）
            出力は各チャネルの総和

        複数カーネル
            前段のカーネルをn個用意
            出力が1次元増える

        ミニバッチ処理
            全段の出力がミニバッチ回数分増えそう，出力次元が1次元増える

        - メリット
            - 疎結合
                画像データ特有の特徴を考慮
                特に，離れた陽うくセル同士の関係を無視
            - パラメータ共有
                同じ重みのカーネルを使い回し，メモリ使用量を抑制
        - デメリット
            - データ加工技術は含まれない
                スケーリングや回転といった画像変換はできない
                回転された画像を別途用意

    1. ## <a id="">ハイパーパラメータ</a>
        1. ## <a id="">パディング</a>
            $$
              out = (H + padding*2 - Filter)/stride + 1
            $$
            - メリット
                - 端のデータの抽出
                    パディングがないと，恥の畳み込み回数が少なくなる→重要度が下がる
                - データサイズを調整可

    1. ## <a id="">プーリング</a>
        情報抽出

        学習パラメータなし
            最大値や平均値をとるだけ（マックスプーリングが主，特徴を取りやすいため）
        チャネル数が変化しない
            チャネル毎に独立な計算
        微小な一変化にロバスト

    1. ## <a id="">im2col</a>
        入力データのカーネル適用箇所をバッチサイズ方向に切り出し，それらを行方向に並べる

        カーネルを行方向に展開し，各カーネルを列方向に並べる

        内積を取り，整形して出力データの形にする

        行列計算に帰着（大きなマトリクスの計算を高速に）

    1. ## <a id="">画像における正規化</a>

        1. ## <a id="">batch normalization</a>
            これによりDLの精度が爆発的に向上

            パラメータの勾配計算は，他のパラメータに影響が出ない前提
            偏微分をし，損失を減らす方向へ計算している

            データは適度な広がり・多様性がある方がいい（ガウス分布に沿うような）
            →さまざまなパターンを学習できるため
            →偏ったデータでは勾配が消失する可能性

            ネットワークが深いと非線形変換が何度も行われ，各層の入力データの分布が大きく変わる（内部の共変量シフト）
            →学習が進まない
            →パラメータ初期値に注意を払う必要性

            よって，各ノードの値をミニバッチ単位で正規化（パラメータのスケールを揃える）

            線形変換-非線形変換の間で行われることが多い
            →FC - BN - ReLU - FC - BV - ReLU のように

            パラメータ初期値に払う注意が小さくなる
            正則化の効果も持つ（L2正則化やDropoutの必要性がsちいさくなる）
            バッチサイズが小さすぎると機能しにくい

            スケーリング・シフトを行う
            →必ず平均0分散1になるよりは，学習にあった分布にしたい
            →$\gamma, \beta$の学習によって決まる項を組み合わせ最適な分布にする

            テスト時
            全訓練データ（フルバッチ）の平均・分散で正規化
            →テストデータでやると，１つの予測だけの際に使えない
            ミニバッチの平均・分散と移動平均でフルバッチの平均・分散を推定
            $$
              \mu_{new} = m\mu_{old} + (1 - m)\mu
            $$

        1. ## <a id="">layer normalization</a>
            同じ層のニューロン間で正規化
            一つのサンプルに対して全てのチャネルにまたがって行う
            バッチ正規化と異なり，トレーニング・テストで同じ計算を実行
            オンライン学習やRNNに拡張（1サンプルに対して行うため）

        1. ## <a id="">instance normalization</a>
            各チャネルで独立に画像に対して行う
            画像生成の分野でバッチ正規化の代替として注目
            画像以外の分野へは拡張しづらい
            バッチサイズが十分であれば，バッチ正規化のみで十分である

        1. ## <a id="">group normalization</a>
            チャネルをG個にグルーピングし，layer, instance normalizationの中間的な処理を行う
            物体検出やセグメンテーションで効果を発揮

## <a id = "cp6">Chapter 6</a>
1. ## <a id="">RNN</a>
    回帰型NN

    時系列データの処理
    →循環構造
    →出力を入力に戻す
    →BPができない
    →過去の中間層の情報を次の入力に加える（ループの展開）
    →流れが一方通行になったので（循環でない）BPができる
    →BPTT(Back Propagation Through Time)

    1. ## <a id="">順伝播</a>
        $t$番目の入力と$t-1$層の出力が$t$層の入力となり，それぞれに重みとバイアスを計算し$t+1$層の入力へとつなげる．
        予測値算出・損失計算ではこの出力を活性化関数に通し，ラベルと比較する
        $$
          h_{next} = Act(x_t W_x + h_{pre} W_h + b)\\
          \hat{ y_t} = Act(h_{next} W_o + c)
        $$

    1. ## <a id="">BPTTと教師強制</a>
        BPTT
        時間軸方向の逆伝播

        BPTTの課題
        - 並列処理が不可能
        - 全時刻の中間状態を保存→メモリコストが高い

        教師強制
        前層の中間層の代わりに前層の正解ラベルを用いる
        →計算コスト大幅減
        →並列処理可能（時系列的関係を切り離して学習可能）
        - 参照する前層の情報
            - 通常のRNN：前時刻のRNNユニットの状態（訓練・テスト時）
            - 教師強制
                - 訓練時：前時刻の正解ラベル
                - テスト時：前時刻の出力層の状態

        Truncated BPTT
        通常BPTTの全中間層保存によるメモリコストを改善
        FPの接続は切らず，BPの接続を切る
        →ユニット単位でのBPを実行できる

    1. ## <a id="">深層回帰</a>
        RNNで回帰を深くし，予測能力を高めたい

        - 階層別に回帰
            RNNの状態を複数の層に分解
            各層で回帰的な結合を持たせる
            →下部の層が入力データをより適切な隠れ層の状態変換できる
        - 中間層でより深い接続
            RNNを3つの演算ブロックに切り分け，各ブロックで深い計算を持たせる（MLPなど）
            - 入力 - 隠れ層
            - 隠れ層 - 隠れ層
            - 隠れ層 - 出力層
        - スキップ接続
            3ブロックで深層化することで最適化が困難に
            →前層よりも前の状態と接続し，パラメータ数を抑える
            →データの最短経路が短くなり最適化しやすくなる

1. ## <a id="">再帰型RNN (Tree-RNN)</a>
    回帰結合型NNのもう一つの一般形
    （最近の使用例は少ない）

    子の表現特徴ベクトルを用いて親の表現特徴ベクトルを計算

    応用例
    - 自然言語処理
    - プログラミング言語の意味解析

    - 回帰型NN
        - RNN (Recurrent Neural Network)
            入力を前（あるいは後）から順番に受け取り，表現ベクトルを構成
    - 再帰型NN
        - Tree-RNN
            入力を木構造に沿って処理し，表現ベクトルを構成

    再帰型RNNもRNNと記載されることもあるので内容に注意

1. ## <a id="">長期依存性の解消　LSTM</a>
    長期依存性
    言語処理などにおいて，予測位置から遠い入力が予測に影響することがある
    →通常RNNでは位置が遠ければ依存性も低くなる
    →長期的な依存関係を学習するために「勾配爆発・勾配消失」の解決が必要

    - 勾配爆発
        各時刻でのBPの度に$W_f (if\,\,>1)$が乗されると，勾配が指数関数的に大きくなる
        - 勾配クリッピング
            BPにて勾配の上限を設定
            勾配ベクトル$\Delta\omega^{(t)}$のノルム(L2が一般的)が上限値$g$を超えた時，下式でパラメータ更新の大きさを調整
            $$
              \Delta\omega^{(t)} = \frac{g}{||\Delta\omega^{(t)}||}\Delta\omega^{(t)}
            $$

    - 勾配消失
        各時刻でのBPの度に活性化関数の微分が乗されるので，勾配が指数関数的に小さくなる
        （ReLUを除く）
        - スキップ接続・Leaky接続
            - スキップ接続(skip connection)
                隠れ層の状態をより先の層へ伝える
                →粗い時間スケールで動作し，遠い過去から現在までの情報伝達を効率化
                →これまで捉えられなかった長期依存性に希望がさす
                →通常の接続とスキップ接続両者が存在するため，勾配爆発の可能性は残る
            - Leaky接続
                前時刻からの入力を$\alpha$倍，入力層からの接続を$1-\alpha$倍して接続
                $\alpha$が1に近ければ，過去の記憶を長期間記憶
                $\alpha$が0に近ければ，過去の記憶は急速に破棄される
                $\alpha$は初期値で
                何らかの分布からサンプリングし固定
                適当な値として学習
                のいずれか
                Leakyユニットで異なる時間スケールを持つことは，長期依存性を扱う上で役に立つ
        - ゲート付きRNN
            記憶すべき情報を記憶しきれない
            →メモ（記憶セル）を追加
            →ユニットに入力層，前の隠れ層，前の記憶セルを入力
            →次の隠れ層，記憶セル(CEC)を出力
            （記憶セルは活性化関数を通さないので，勾配消失を防ぐことができる）
            →LSTM
            - 忘却ゲート forget gate
                CECから不要な記憶を忘却
                $$
                  f = \sigma\left( x_t W_x^{(f)} + h_{pre} W_h^{(f)} + b^{(f)}\right)\\
                  c_{pre} = f\odot c_{pre}
                $$
                $\sigma$はsigmoid関数（0~1）で$c_{pre}$の情報を削ぎ落とす
            - インプットゲート input gate
                情報を取捨選択し，CECにメモ
                $$
                  g = \textrm{tanh}\left( x_t W_x^{(g)} + h_{pre} W_h^{(g)} + b^{(g)}\right)\\
                  i = \sigma\left( x_t W_x^{(i)} + h_{pre} W_h^{(i)} + b^{(i)}\right)\\
                  c_{pre} += g\odot i
                $$
                $g$：記憶する情報
                $i$：どれだけメモするかの割合
                もともとあった情報に足すだけ
            - アウトプットゲート output gate
                CECの情報のうち，次の隠れそうにどの程度流すか調整
                $$
                  o = \sigma\left( x_t W_x^{(o)} + h_{pre} W_h^{(o)} + b^{(o)}\right)\\
                  h_{next} = o\odot \textrm{tanh}(c_{pre})
                $$
                $o$は0~1なのでCECをどれだけ次の層に渡すか調整される
            - 補足　ピープホール付きLSTM
                各ゲートでCECを覗き見るLSTM
        - GRU
            LSTMの表現力を保ちつつ，計算コストを低減
            - 残った機能
                隠れ状態に忘却機能と記憶機能を付与
                →勾配消失が起きづらい
            - 失った機能
                忘却・記憶機能がトレードオフ
                CECがなく，メモができない
                →過去の情報が残りづらい

            - reset gate
                過去の隠れ状態をどれだけ反映させるか
                $$
                  r = \sigma\left( x_t W_x^{(r)} + h_{pre} W_h^{(r)} + b^{(r)}\right)\\
                  h_{reset} = r\odot h_{pre}
                $$
                LSTMの忘却でゲートと同様の動き
            - update gate
                $$
                  \tilde{h} = \textrm{tanh}\left( x_t W_x + h_{reset} W_h + b\right)\\
                  z = \sigma\left( x_t W_x^{(z)} + h_{pre} W_h^{(z)} + b^{(z)}\right)\\
                  h_{next} = (1-z)\odot h_{pre} + z\odot \tilde{h}
                $$
                $\textrm{tanh}$：記憶する情報
                $\sigma$：記憶する割合
                $z$：$\sigma$
                $(1-z)\odot h_{pre}$：forget機能
                $z\odot \tilde{h}$：input機能

                forget, inputがトレードオフ

    - RNN系ユニットまとめ
        - simple RNN
            時系列データを用いる
            過去の情報を循環
        - LSTMユニット
            長期依存性（特に勾配消失）の解消
            CECによるメモを導入
        - GRUユニット
            LSTMの計算コスト低減

1. ## <a id="">双方向RNN</a>
    順方向RNN，逆方向RNNの結果をマージ
    - メリット
        後ろからの文脈情報を得られる
        →RNNよりも精度が向上することも

    出力と隠れ層の次元が単方向と比べ倍になる
    →順・逆方向の結果それぞれを保持するため

1. ## <a id="">RNN アーキテクチャ</a>
    1. ## <a id="">双方向伝播</a>
        双方向RNNは，simple RNNユニットを二つ重ねて
        互いに逆方向の伝播をさせる
        →ユニットを取り替え，双方向LSTMも可能

    1. ## <a id="">ユニット数を増やす</a>
        どのユニットもユニット数を増やしてNNを深くすることが可能
        縦横に伸ばせる
        →計算コスト高くなる

## <a id = "cp7">Chapter 7</a>
1. ## <a id="">生成モデル</a>
    1. ## <a id="">潜在変数</a>
        潜在的な特徴を表す変数

        次元圧縮後の状態
        e.g.) 説明変数（月間平均気温・湿度）→潜在変数（季節）

    1. ## <a id="">Encoder-Decoder modele</a>
        [データ] - [Encoder] - [潜在変数] - [Decoder] - [生成データ]

        1. ## <a id="">Auto Encoder</a>
            入力と同じ物を出力させる
            →中間層の次元圧縮された情報が重要

            応用例
            - 画像のノイズ除去(Denoising AE)
            - クラスタリング
                Encoderで圧縮した特徴分布上でクラスタリング
                →より重要な特徴をもとにクラスタリングできる
        1. ## <a id="">VAE Variational Auto Encoder</a>
            - AE
                存在するデータを忠実に再現したい
                →潜在変数にあそびがない
            - VAE
                実在しないデータを生成したい
                →潜在変数にランダム性や連続性を持たせ，中間の状態などを作り出せる

            潜在変数が正規分布に従うように調整

            - 学習時
                [Encoder] - [$\mu, \sigma^2$] - [潜在変数]
                この過程で潜在変数の平均$\mu$と分散$\sigma^2$を出力
                (AEでは直接潜在変数を求めていた)

                出力された$\mu, \sigma^2$から正規分布${\mathcal N}(\mu, \sigma^2)$に従う潜在変数をサンプリング

                後の流れは同じ

                BPではランダムにサンプリング($z \verb|~| N(\mu, \sigma^2)$)している点を
                ガウシアンノイズ$\epsilon\verb|~|{\mathcal N}(0,1)$に代替
                FPでの$\epsilon$を記憶すればBP可能
                $z = \mu + \sigma^2\times\epsilon$

            - 損失関数
                $$
                  {\mathcal L} = D_{KL}[q(z|{bm X})||p(z)]
                   - {\mathbb E}_{q(z|\bm{X})}[\textrm{log}\,p(\bm{X}|z)]\\
                   = D_{KL}[{\mathcal N}(\mu(\bm{X}),\Sigma(\bm{X}))||{\mathcal N}(0,1)] + \beta||\bm{Y} - \bm{X}||^2
                $$
                第一項：Encoderが求めた分布と${\mathcal N}(0,1)$との近さ
                第二項：入力データと出力データの近さ(MSE)

                AEの損失関数
                $$
                  {\mathcal L} = \beta||\bm{Y} - \bm{X}||^2
                $$

            - 実用と課題
                - 実装が容易，理論が整っている
                - 入力が画像の場合，VAEのサンプルはややぼやける傾向にあり，原因は不明
                    損失を小さくするために，出力が0,1ではっきりしたものよりも小数の値を持つことで画像がぼやける傾向にある
        1. ## <a id="">GAN Generated Adversarial Network</a>
            VAEの課題
            - 画像がぼやける
                1. ガウス分布による正規化のために，データに制約がかかり出力にノイズが載ったのか
                1. 鮮明な画像(0,1)よりもぼやけた画像(小数)の方が損失(MSE)が下がるのか

            鮮明な画像のために，ノイズの除去・損失関数を変える，が必要．

            GAN
            GeneratorがDiscriminatorを騙すように画像を生成
            [ノイズ z] - [Generator G(z)] - [偽データ $\hat{ x}$ + 真データ x] - [Discriminator D(x)] - [真偽(0,1)]

            生成時
            [ノイズ z] - [Generator G(z)] - [偽データ $\hat{ x}$]
            一様乱数からノイズzをサンプリング
            Generatorで新たなデータを生成する

            - 目的関数
                $$
                  \underset{G}{\textrm{min}}\,\underset{D}{\textrm{max}}\,V(D,G) =
                  \underset{G}{\textrm{min}}\,\underset{D}{\textrm{max}}\,
                  {\mathbb E}_{x\verb|~|p_{data}(x)}\left[\textrm{log}D(x)\right] +
                  {\mathbb E}_{z\verb|~|p_{z}(z)}\left[\textrm{log}(1-D(G(z)))\right]
                $$
                $G(z)$：ノイズzからGが生成したデータ
                $D(x)$：訓練データxが実在するデータである確率
                - DはVを最大化
                    - first term
                        訓練データ分布$p_{data}(x)$から得たデータxが「訓練データである」と判別する確率$D(x)$の最大化
                    - second term
                        生成データ分布$p_z(x)$から得たデータ$G(x)$が「生成データである」と判別する確率$(1 - D(G(z)))$の最大化
                - GはVを最小化
                    - first term
                        関係なし(Dが訓練データをどう判別するかは関係ない)
                    - second term
                        生成データ分布$p_z(x)$から得たデータ$G(x)$が「生成データである」とDに判別させる確率$(1 - D(G(z)))$の最小化

                - 最適化手法はSGD(確率的勾配降下法)
                    m : ミニバッチサイズ
                    1-1. m個のノイズzとデータxをサンプリング
                    1-2. SGDでDiscriminator更新
                        $$
                          \nabla_{\theta_d}\frac{1}{m}\sum_{i=1}^{m}
                          \left[\textrm{log}D(\bm{x}^{(i)}) +
                          \textrm{log}\left(1 - D\left(G(\bm{z}^{(i)})\right)\right)\right]
                        $$
                    2-1. mこのノイズzをサンプリング
                    2-2. SGDでGenerator更新
                        $$
                          \nabla_{\theta_g}\frac{1}{m}\sum_{i=1}^{m}
                          \left[\textrm{log}\left(1 - D\left(G(\bm{z}^{(i)})\right)\right)\right]
                        $$
                    これらを全てのミニバッチに対して繰り返す

                - 学習が困難
                    - D-Gが拮抗するはずが，Dが圧勝し勾配が消失する
                        →Dを小さいネットワークにする
                        →DのDropout rateを大きめにする
                        →Unrolled GANを用いる
                    - mode collapse
                        →Minibatch Discriminator
                        →Wasserstein GAN

                    - Unrolled GAN
                        DをKステップ学習させた後に，Gを学習させる
                        →GがKステップ分先取り学習できる(良い勾配情報で学習できる)
                        →学習のバランスをとる(GよりDの方が早く学習してしまう問題を解決)

                        効果：Gに「質の良い勾配情報(hint)」を与えることになり，学習のバランスが取りやすくなる

                    - mode collapse(モード崩壊)
                        Gの能力不足により，全体の分布を近似しきれなく，ある一つのデータ(最頻値=mode)だけを出力しようとしている状態

                        対策
                        - Minibatch Discriminator
                            Dにmode collapseが起きているかのヒントを与える処理
                            →同バッチ内の画像間類似度を計算する
                            →mode collapseが起きている場合，同じ様なデータが多くなる=類似度が高くなる
                            →fakeデータと判別しやすくなる
                        - Wasserstein GAN
                            GANとは異なる距離を用いる

                            Gは生成データの分布を訓練データの分布に近づけようとする

                            - 通常GAN
                                JSD(イェンセンシャノンダイバーシティ)
                                →分布の裾が狭い時，全く異なる形状の分布同士でもJSDが低くなる問題
                            - Wasserstein GAN
                                EMD
                                →分布(砂山)を別の分布に移動する際，「どのくらいの量の砂を運ばなければいけないか」を数値化した指標
                                →EMDであれば異なる形状の分布間の距離は大きいと判断してくれる

## <a id = "cp8">Chapter 8</a>
1. ## <a id="">強化学習</a>
    最適な意思決定のルールを求める
    →教師ありでもなしでもない
    明確な正解は存在していないが，大きな目標や行動はある
    1. ## <a id="">用語</a>
        - エージェント
            環境で動く主体
        - 環境
            エージェントが動く範囲や状態
        - 状態
            行動をした後の環境の評価
        - 行動
            次に何をするか
        - 状態遷移確率
            次の行動で状態がどう変わるかの確率分布
            →ほとんどの場合で正確にわからない
        - 報酬
            行動を起こした際に得られる価値
    1. ## <a id="">マルコフ決定過程 MDP</a>
        従来の「状態」のみの確率過程(マルコフ過程)とは異なり，
        「行動」や「報酬」などを追加したものが「マルコフ決定過程」
        →行動選択ルールの最適化のために「行動」や「報酬」が必要

        $s_t$ : 時間ステップtの状態
        $a_t$ : 時間ステップtの行動
        $p(s_{t+1}|s_t,a_t)$ : 状態$s_t$で$a_t$を選択し，状態$s_{t+1}$になる状態遷移確率
        →行動をしてそのままの状態かもしれない．わからないので確率で吸収して表現．
        $r_t$ : 実行した$a_t$において得られる(即時)報酬

        $$
          p(s_{t+1},s_t,a_t) = p(s_{t+1}|s_t,a_t)p(s_t|a_t)p(a_t)\\
          (p(s_t,a_t) = p(s_t|a_t)p(a_t))
        $$
        $p(a,b)$ : aとbの同時確率
        $p(a|b)$ : bの下でaが起こる確率

        1. 時間ステップt=0に初期化．初期状態確率$p_{s_0}$に従い初期状態$s_t$を観測
        1. 状態$s_t$に対して行動$a_t$を選択
        1. 行動$a_t$を実行．得られた報酬$r_t$と状態遷移確率$p(\cdot|s_t,a_t)$により定まる，次の状態$s_{t+1}$を観測
        1. 時間ステップtを一つ進め，手順2に戻る

        - マルコフ性
            次の状態が，現在の状態のみによって決定する性質
            →$s_{t+1}$の決定にあたり，$t=t-1$以前の状態に依存しない．$s_t$のみによって決まる
            $$
              p(s_{t+1}|s_1,\cdots,s_{t-1},s_t) = p(s_{t+1}|s_t)
            $$

    1. ## <a id="">方策</a>
        どの様にして行動を選択するか
        →方策

        最適な方策$\pi$を探索することが強化学習の目的

        [状態 $s_t$] - <方策 $\pi(a_t|s_t)$> - [行動 $a_t$] - <状態遷移確率 $p(s_{t+1}|s_t,a_t)$> - [状態 $s_{t+1}$] - [報酬 $r_t$]

    1. ## <a id="">報酬</a>
        ある状態でとった行動が学習に寄与した価値

        報酬関数は設計者が与えるもの
        $$
          r_t = g(s_t,a_t,s_{t+1})
        $$
        →設計が困難

        加えて，将来的な報酬を考慮する必要性
        →直近では学習に価値をもたらせないが，将来的に価値をもたらす行動を評価したい
        →一方で，将来が遠いほど不正確な評価になる
        →割引率$\gamma$を導入し報酬に反映

        割引累積報酬$R_t$
        $$
          R_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots
          = \underset{K\to \infty}{\textrm{lim}}\sum_{k=0}^{K}\gamma^k r_{t+k}
         $$


    1. ## <a id="">目的関数(価値関数)</a>
        報酬の期待値を目的関数とし，エージェントの行動を表現
        →累積報酬和$R_t$を状態$s_t$の価値とし，「価値の高い状態に遷移する」という方策が作れそう

        方策$\pi$の下での状態$s_t$の価値関数
        →ある方策$\pi$での状態$s$が初期状態$s_0$だった時の累積報酬和$R_0$の期待値が右辺
        $$
          V_{\pi}(s) = \mathbb{E}_\pi \left[R_0|s_0=s\right]
        $$
        ($\mathbb{E}_\pi[X|Y]$は，条件$Y$が与えられた確率変数$X$の$\pi$についての期待値)

        1. ## <a id="">最適な目的関数</a>
            最適な価値関数$V^*(s)$は，ある方策における累積報酬和の期待値の最大値
            $$
              V^*(s) = \underset{\pi}{\textrm{max}}\mathbb{E}_\pi \left[R_0|s_0=s\right]\\
              \cdots\\
              = \underset{\pi_0}{\textrm{max}}\sum_{a_0}\pi_0(a_0|s)
              \left[
              g(s,a_0,s_1) + \gamma\sum_{s_1}p(s_1|s,a_0)V^*(s_1)
              \right]
            $$
            ($\Sigma$ありうる次の状態への遷移確率$\times$その最適価値)

            1. ある状態の最適価値関数$V^*(s)$は，次の状態の最適価値関数$V^*(s^{\prime})$によってのみ再帰的に表現
            1. 最適な行動を選択できること$(\pi_0(a|s)=1)$＝最適な方策

            よって最適な方策$\pi=1$についてのみ考えればよい
            →それ以外の方策は最適でない$=0$
            $$
              V^*(s) = \underset{a}{\textrm{max}}
              \left[
              g(s,a,s^{\prime}) + \gamma\sum_{s^{\prime}}p(s^{\prime}|s,a)V^*(s^{\prime})
              \right]
            $$
            →最適ベルマン方程式

            「価値を最大化する行動をとる」
            ＝
            「価値を最大化する最適な方策を見つける」


        1. ## <a id="">2種類のアプローチ</a>
            - 価値関数ベース
                価値を最大化するような「行動」をする
                価値(=モデル)を基準に，行動の選択は別のアルゴリズムで決定
                価値を正しく見積もることが重要
            - 方策勾配ベース
                価値を最大化する「最適な方策」を見つける
                行動の選択(方策(=モデル))そのものをモデルで決定する

            1. ## <a id="">価値関数ベース</a>
                価値を正確に見積もる
                どのような行動をとるかよりも，その時の状態を目的(勝利など)により近づける
                1. 現在の状態の価値を推測
                1. 一手先の状態を全て列挙．最も価値の高い状態を選択
                1. AI同士で対戦した結果から，状態価値を学習．
                    探索による先読みと組み合わせ，性能の高い方策を実現
            1. ## <a id="">方策勾配ベース</a>
                そもそもの価値関数導入の背景
                累積報酬和$R_t$を状態$s_t$の価値とし，「価値の高い状態へ行く」方策を考えるため

                方策$\pi$の下での状態$s_t$の価値関数
                $$
                  V_{\pi}(s) = \mathbb{E}_{\pi}[R_0|s_0=s]
                $$

                $V_{\pi}(s)$を全て調べることで，高い$V_{\pi}(s)$を持つ状態をたどるルートがわかる
                ＝
                最適な方策が間接的にわかる

                行動価値関数$Q_{\pi}(s,a)$の導入
                $$
                  Q_{\pi}(s,a) = \mathbb{E}_{\pi}[R_0|s_0=s, a_0=a]
                $$
                ある状態における，ある行動をとった時の価値
                →価値関数の一部(価値関数における，ある行動$a$をとった先の価値)
                →行動を一つ一つに分けて考えていく必要があるため，導入

                価値関数：状態$s$であらゆる行動をした時の報酬を全て計算
                行動価値関数：状態$s$である行動$a$をした時の報酬を全て計算

        1. ## <a id="">価値関数ベースのアプローチ</a>
            - 価値反復法
                環境ダイナミクス(状態遷移確率$p$と報酬$r$)が既知の問題で有効
                →最後の状態から最適ベルマン方程式に近づけていく
                e.g.) 迷路には適する．自動運転は不適．
            - 報酬のサンプリング
                未知の環境ダイナミクスを実際に行動をしてサンプルを集める
                →行動をした結果こうなった→遷移確率はこうだろう
                - モンテカルロ法
                    エピソディックに学習する(有限の時間ステップで終了する場合(エピソディック)に有効)
                    →何度も最後までプレイし，報酬をサンプリング
                    $$
                      V(s_t) \leftarrow V(s_t) + \alpha[\underline{R_t} - V(s_t)]\\
                      Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha[\underline{R_t} - Q(s_t,a_t)]
                    $$
                    とりあえずの累積報酬和$R_t$がわかるので，価値関数を仮の$R_t$に近づけていく($\alpha$が1に近い時)⇄価値関数に近づけていく($\alpha$が0に近い時)
                    その時の$R_t$が正しいかもわからないので，$\alpha$を乗じて補正をかける
                    e.g.) 五目並べは適合．テトリスは不適．(1エピソードが長い場合も不適)
                - TD法 Temporal Difference Learning
                    1ステップごとにサンプリング
                    →終了を待たずに，それまでに得た宝珠から価値関数を更新
                    $$
                      V(s_t) \leftarrow V(s_t) + \alpha[\underline{r_{t+1} + \gamma V(s_{t+1})} - V(s_t)]
                    $$
                    下線部がモンテカルロ法と異なる
                    →最後までの状態がわからないため，即時報酬$r_{t+1}$と補正した価値関数$\gamma V(s_{t+1})$で評価

                    - Q関数
                        方策オフ型：実際に進む行動(方策)と価値関数更新に用いる行動(方策)が異なる(こっちの方が直感的)
                        価値関数$V$に代わり，行動価値関数$Q$を用いて行動を選択
                        $$
                          Q_{\pi}(s,a) = \mathbb{E}_{\pi}[R_0|s_0=s,a_0=a]
                        $$
                        行動価値関数$Q$を用いて，未来の状態を計算しなくても最適な行動ができる
                        →学習が安定する(Q関数の収束が早い)
                    - SARSA
                        方策オン型：実際の方策($\epsilon$-greedy)に従って価値を更新
                        （エピソード更新と価値関数更新お方策が同じ）

                        Q学習では予測がある程度正確でないとmax Qが適切に定まらない
                        →五目並べで最初に盤面全てを予測できないのと同じ
                        →$\epsilon$-greedy方策に従って行動価値を学ぶ

                        SARSAはmax Qではなく，現時点での方策に従って遷移する$Q(s_{t+1},a_{t+1})$を更新に使用する
                        →その時点での最も良い選択は探索せず，方策にしたがて価値を更新する
                        →価値関数の更新にランダム性を取り込んでいるので局所界に陥りにくい
            - サンプリングの問題点
                初期値依存性が高い

                モンテカルロ法・TD法の流れ
                1. 価値関数$V$または行動価値関数$Q$に従って行動
                1. 報酬をサンプリング
                1. $V$や$Q$を更新
                偏ったサンプリングでの方策の更新では局所界に陥る
                →一定程度学習した方策を無視して行動する($\epsilon$-greedy方策，ソフトマックス)

                - $\epsilon$-greedy方策
                    確率$\epsilon$で完全にランダムに行動(大体0.01~0.1くらいの確率)

                - ソフトマックス方策
                    状態行動価値$Q(s,a)$をエネルギーと見做し，ボルツマン分布に従い行動を選択（ソフトマックス関数と同じ形）

                    全行動$\mathcal{A}$から行動aが選択される方策$\pi(s_t,a)$
                    $$
                      \pi(s_t,a) = \frac{\textrm{exp}\left(\frac{Q(s_t,a)}{T}\right)}
                      {\sum_{b\in \mathcal{A}}{\textrm{exp}\left(\frac{Q(s_t,b)}{T}\right)}}
                    $$
                    $Q(s,a)$が高いほど選択される
                    正の定数$T$(温度)を適切に設定し，確率に対する$Q(s,a)$の影響を制御する

                    ボルツマン分布
                    低エネルギー状態ほど，指数的に高い確率で出現する現象を表す確率分布
                    指数の肩の符号がソフトマックス分布と異なる
            - 価値関数ベース　まとめ
                - 価値関数
                    ある状態sから方策$\pi$に従って行動した際に，今後どのくらいの報酬を得ることができるかを表す指標
                - 代表例
                    Q学習
                - Q学習 問題点
                    状態・行動が増えすぎると処理できなくなる(全ての行動を考え，maxをとるため)
                    →DNNなどを用いてパラメトリックに価値関数を表現するDQNなどがある

        1. ## <a id="">方策勾配法ベースのアプローチ</a>
            方策を直接予測
            未来の状態を列挙する必要がない
            相性の良い(モンテカルロ木探索→alpha Go　など)と組み合わせることで高い性能の方策を実現

            復習（方策・価値ベースの違い）
            - 価値関数ベース：価値を基準にし，行動の選択は別のアルゴリズム(ソフトマックスや$\epsilon$-greedyなど)で決定
            - 方策勾配ベース：行動の選択そのもの(方策)をモデルで決定

            1. ## <a id="">方策勾配定理</a>
                パラメータ$\theta$を持つ方策を確率モデル$\pi_{\theta}$とする
                目的関数$J(\theta)$を最大化する$\theta$を，
                確率勾配$G_t^{\theta}$($J(\theta)$の$\theta$に関する確率方策勾配)を用いて，
                確率的勾配法に基づいて更新するアプローチ
                $$
                  \theta \leftarrow \theta + \alpha_t G_t^{\theta}\\
                  J(\theta) = V^{\pi_{\theta}}(s)
                  = \sum_{a} Q^{\pi_{\theta}}(s,a)\pi_{\theta}(a|s)
                $$
                $Q$ : 行動の価値
                $\pi$ : その行動をとる確率
                →を掛け合わせたもの(=期待値)の総和→価値関数→目的関数→最大化

                方策勾配定理
                目的関数$J$をパラメータ$\theta$で微分した式が以下で表されること
                $$
                    \nabla_{\theta} J(\theta)=
                    \mathbb{E}_{\pi_{\theta}}[
                    Q^{\pi_{\theta}}(s,a)\nabla_{\theta}\textrm{log}\pi_{\theta}(a|s)
                    ]
                $$
                (現在の方策における価値関数$\times$行動の選択確率の対数の勾配)
                の方策に関する期待値
                →覚える

                この定理により，Q値のみ分かれば累積報酬を座右化させる方策の勾配が求まる

            1. ## <a id="">方策勾配法の工夫</a>
                方策勾配定理を元に方策を更新するが，$Q^{\pi_{\theta}}(s,a)$を正確に求めるのは困難
                →サンプリングした報酬の平均で近似→Q値の推定値
                $$
                  \nabla_{\theta} J(\theta) \approx
                  \sum_t r_t \nabla_{\theta}\textrm{log}\pi_{\theta}(a_t|s_t)
                $$
                →全ての時間ステップについて勾配計算を行うと，時間ステップ数分加算が行われ分散が大きくなる
                →学習が進みにくくなる(更新幅が大きく収束しにくい)
                →ベースライン関数$b(s)$を導入し，差し引くことで即時報酬を丸め込む
                →分散を小さくする
                $$
                  \nabla_{\theta} J(\theta) \approx
                  \left(r_t - b(s_t)\right) \nabla_{\theta}\textrm{log}\pi_{\theta}(a_t|s_t)
                $$
                →ベースライン関数に推定価値関数$\hat{ V}(s)$を用いることもあり，
                $\left(\hat{ Q}(s,a) - \hat{ V}(s)\right)$をアドバンテージ関数$\hat{ A}(s,a)$という．
                ($\hat{ Q}(s,a) \approx r_t$)
                $$
                  \nabla_{\theta} J(\theta) \approx
                  \hat{ A}(s_t,a_t) \nabla_{\theta}\textrm{log}\pi_{\theta}(a_t|s_t)\\
                  = \left(\hat{ Q}(s_t,a_t) - \hat{ V}(s_t)\right)\nabla_{\theta}\textrm{log}\pi_{\theta}(a_t|s_t)
                $$

            1. ## <a id="">具体的なプローチ</a>
                - REINFORCEアルゴリズム
                    実際に得られた報酬の平均を使って近似した値をQ値の推定値とする
                    (方策勾配法を用いても，現状Q値の推定値は必要)

                    TステップからなるMエピソードを行った際の報酬の平均で近似する
                    $$
                      \nabla_{\theta} J(\theta) =
                      \frac{1}{M}\sum_{m=1}^M \frac{1}{T}\sum_{t=1}^T
                      \left(r_{m,t} - b\right) \nabla_{\theta}\textrm{log}\pi_{\theta}(a_t|s_t,\theta)\\
                      \\
                      b = \frac{1}{M}\sum_{m=1}^M \frac{1}{T}\sum_{t=1}^T r_{m,t}
                    $$
                - Actor-Critic
                    状態価値を学習(主にNN)→推定：Critic(推定器)
                    Criticの推定した$\hat{ V}(s_t)$をベースラインにしてActor(行動器)を改善していく

                    - A3C Asynchronous Advantage Actor-Critic
                        CriticをNNで学習したもの
                        $$
                          \nabla_{\theta} J(\theta,\phi) =
                          \left(\hat{ Q}(s,a) - \hat{ V}_{\phi}(s)\right) \nabla_{\theta}\textrm{log}\pi_{\theta}(a|s)
                        $$
                        ($\phi$ : NNのパラメータ)

                        方策が決定的になった際，上式では更新が進みに悔いので
                        下式のようにエントロピー正則化項を加え，A3Cの勾配計算式を定義
                        →起こす行動が固まると，その行動の価値も固まり，QとVの差がほぼ0になる
                        →学習が進まない
                        $$
                          \nabla_{\theta} J(\theta,\phi) =
                          \left(\hat{ Q}(s,a) - \hat{ V}_{\phi}(s)\right) \nabla_{\theta}\textrm{log}\pi_{\theta}(a|s)
                          \underline{- \lambda\sum_a \pi_{\theta}(a|s)\textrm{log}\pi_{\theta}(a|s)}
                        $$
                        ($\lambda( > 0)$は正則化係数)

                        ピーキーな方策の分布にしたくない(正則化項を導入)
                        →エントロピーを導入
                        →エントロピーを大きくする方向に学習が進む

## <a id = "cp9">Chapter 9</a>
1. ## <a id="">DL実用の工夫</a>
    - 事前学習
        大規模なデータセットで学習
    - 転移学習
        事前学習モデルにそうを追加して，追加した層のみ学習
    - ワンショット学習
        ラベル付きデータを一つだけ与えて学習
        ラベルなしデータの潜在的なクラス分けの表現を学習
        特徴空間上でラベル付きの周囲にあるデータのラベルを推論できる(ラベル付きが一つさえあれば良い)
    - ゼロショット学習
        全てのデータがラベルなし
        対象物ではなく，対象物を表現する特徴ベクトルを学習
        →訓練データに存在しない未知のクラスを識別学習
    - 半教師あり学習
        一部のデータのみにラベルをつけて学習
        アノテーションのコストを下げる
        1. ラベル付きで学習
        1. 1.のモデルでラベルなしを予測
        1. 2.の結果から，ラベルなしにラベル付け
            →再度ラベルあり/なしを学習
        学習データが不足の時に有効
        元のデータセットに偏りがあると不適
    - マルチタスク学習
        1つのモデルで複数のタスクを同時にこなせるよう学習
        (メインタスクとその共通のタスクを解くモデル)
        →メインタスクの汎化性能を向上させる
        →メインにとって有用となるはずのサブが特徴量から抜け落ちないように学習

        DLと相性が良い
        e.g.)物体検出　＝　物体のクラスを特定　＋　物体の位置を特定
        の共通タスクを解くモデル

## <a id = "cp10">Chapter 10</a>
1. ## <a id="">画像認識</a>
    2012年 ILSVRCでDLモデルが大差で優勝

    タスク
    - 画像分類
    - 物体検出
    - セグメンテーション

    有名なデータセット
    - 画像分類
        - MNIST
            手書き文字(アメリカの手書き郵便番号から？)
            - 規模：7万(6万/1万)
            - 28 * 28
            - 10クラス
          - Fashion-MNIST
              ファション画像
              - 規模：7万(6万/1万)
              - 28 * 28
              - 10クラス
        - CIFAR-10 / CIFAR-100
            乗り物や動物
            - 規模：6万(5万/1万)
            - 32 * 32
            - 10 / 100クラス
        - Food101
            食べ物(クラス数が多い/画像が大きい)
            - 規模：1000(750/250) / 各クラスごと
            - 最大512 * 512
            - 101クラス
        - ImageNET
            最大のデータセット
            ジャンルを問わないがぞデータセット
            - 規模：1400万以上
            - さまざま
            - 2万クラス
    - セグメンテーション・物体検出
        - Pascal VOC dataset
            基本的なデータセット
            - 規模：1万程度
            - カテゴリ数：20
        - COCO-Common Object in Context
            より難易度の高いデータセット
            - 規模：32.8万
            - カテゴリ数：91

    1. ## <a id="">画像分類</a>
        1. ## <a id="">ILSVRC</a>
            ImageNET Large Scale Visual Recognition Challenge

            ImageNETを用いた大規模がぞ認識コンテスト
            2010 スタート
            2012 DLモデルが優勝(ヒントン率いる)
            2015 人間の認識性能をこえる

            - AlexNet
                2012優勝モデル
                DLの火付け役
                Conv * 5 + FC * 3
            - VGG
                2014登場
                シンプル
                フィルタが3*3→多層化
            - GoogLeNet
                2014優勝モデル
                多重化
                Inception moduleの導入
            - ResNet
                2015優勝モデル
                残差
                Residual Blockの導入
            - DenseNet
                N個の残差
                Dense Blockの導入
            - Mobile Net
                計算量削減
                Depthwise Separable Conv.の採用

        1. ## <a id="">AlexNet</a>
            ILSVRC-2012で優勝
            Conv5層とFC3層

            活性化関数にReLUを採用
            GPUによる実装
            データ拡張
            Dropout
        1. ## <a id="">VGG</a>
            ILSVRC-2014で準優勝
            3 * 3のフィルターのみ採用して多層化
            (以前のフィルタータイズは様々であった + 縦横にpad1をすることで入出力の画像サイズが変わらない)
            Conv13層とFC3層(VGG16)
            (Conv数の違いでVGG19などがある)

            同じ数のフィルターに何回か通し，max poolingすることで
            中間画像を半分に縮小
            max pooling後の畳み込み層の出力チャネル数を二倍に

            - フィルタサイズによる利点
                パラメータ数を抑えつつ，大きなフィルタサイズと同じ(もしくはより広く)受容野をカバー
                5 * 5の畳み込みを1回(25→1)
                    パラメータ次元数：25(=5^2)
                3 * 3の畳み込みを2回(25→9→1)
                    パラメータ次元数：18(=3^2 * 2)
                →3 * 3の畳み込みに2回は5 * 5の1回分と同じ受容野
            - 多層化による利点
                非線形の活性化関数の適用回数を多くし，表現力が向上
        1. ## <a id="">GoogLeNet</a>
            ILSVRC-2014で優勝
            22層
            潤沢な計算リソースが必要で再現性が低い

            - Inception module
                フィルターサイズが異なる小さな畳み込み処理を統合させたブロック
                利点
                大きなフィルターを小さなフィルターたちで近似
                →パラメータ数を抑えつつ表現力を維持
                →密なパラメータを実現

                - 通常のパラメータ数
                    入力ch * kernel * 出力ch
                - Inception modele
                    入力ch * (5 * 5 * 出力1ch + 3 * 3 * 出力2ch + 1 * 1 * 出力3ch)
                    直方体からカーネルサイズが小さくなることでパラメータの体積が小さくなるイメージ
            - Auxiliary loss
                中間層における損失計算・逆伝播
                利点
                中間層における勾配消失防止
                正則化効果期待(ちょっとしたアンサンブル)
            - Global Average Pooling (GAP)
                チャネルごとにフィルターの画素平均をとる
                - 通常
                    学習パラメータ次元数：縦 * 横 * ch * 出力数
                    縦 * 横の部分でパラメータ数が一気に増え，過学習しやすい
                - GAP
                    学習パラメータ次元数：ch * 出力数
                    ch毎に平均とって1つする
            - Pointwise Convolution
                1 * 1フィルターを用いた畳み込み処理
                チャネル方向のみの畳み込み
                →次元数を削減できる
                - 通常(3 * 3)
                    3 * 3 * ch → 1 * 1 * 1の1マスにする
                - Pointwise Convolution
                    1 * 1 * ch → 1 * 1 * 1の1マスにする
                    →次元削減
        1. ## <a id="">ResNet</a>
            ILSVRC-2015で優勝
            152層
            Residual Blockが大きなブレイクスルー

            Residual Block
            [入力 x] - [残差 f(x)] - [出力 f(x)+x]
            入力xはそもそも正解に近く，正解との差f(x)を学習
            入力をフィルターの通したf(x)と
            出力までskip connectionしたxを足し合わせて出力とする
            →勾配消失が起きにくくなる
            →これまではBPの際に何回も微分をして(層の数分)いたが，skip connectionがあるため，仮にf(x)のBPで逆伝播誤差が小さくなっても，大きさそのままのxが戻ってくるため勾配消失が起きにくい

            Residual Blockの構造
            →パラメータ次元数を抑えつつ多層化することで精度を向上
            - Plain アーキテクチャ
                残差の計算に，[3 * 3, ch=64] - [3 * 3, ch=64]
                →パラメータ次元数：73728 (=(64 * 3 * 3 * 64)+(64 * 3 * 3 * 64))
            - Bottleneckアーキテクチャ
                残差の計算に，[1 * 1, ch=64] - [3 * 3, ch=64] - [1 * 1, ch=256]
                →パラメータ次元数：69632 (=(256 * 1 * 1 * 64)+(64 * 3 * 3 * 64)+(64 * 1 * 1 * 256))

            畳み込みに関わらず，skip connectionを入れることで勾配消失を解決
            →他の分野にも使える点がブレイクスルー
        1. ## <a id="">DenseNet</a>
            2016に登場
            ResNetを改良し，レイヤー間の情報伝達を強化
            →自分より前の層と全てskip connectionで繋がっている
            →Dense = 密

            Transition layer
            →Dense blockとDense blockの間
            Denseを通る毎に畳み込みとPoolingを通る
            →特徴マップを圧縮するためにダウンサンプリング(画像サイズを小さく)する
            (Dense block内ではskipの影響でダウンさんオウリングできないため，Dense間で行う)
        1. ## <a id="">MobileNet</a>
            2017に登場
            計算量を削減しつつ性能を発揮

            Depthwise Separable Convolution
            →Depthwiseとpointwise convを組み合わせた畳み込み
            表現力保ちつつ，パラメータ数を削減
            - 通常
                3 * 3 * 3 → 1 * 1 * 1
                →パラメータ次元数：27(=3 * 3 * 3)
                →パラメータ次元数：9(=3 * 3)
                $$
                  D_K\cdot D_K\cdot N\cdot D_F\cdot D_F \cdot M
                $$
            - Depthwise Separable Convolution
                →パラメータ次元数：12(=9 + 3)
                $$
                  D_K\cdot D_K\cdot M\cdot D_F\cdot D_F
                  + M\cdot N \cdot D_F\cdot D_F
                $$
                - Depthwise Conv.
                    平面方向の畳み込み
                    3 * 3 * 3 → 3 * 1 * 1
                - Pointwise Conv.
                    チャネル方向の畳み込み
                    3 * 1 * 1 → 1 * 1 * 1
                    →パラメータ次元数：3

            ハイパーパラメータによる計算量削減
            →計算コストは下がるが精度は犠牲になる
            - Width Multiplier
                モデルサイズの削減
                →チャネルサイズを間引く
                入力・出力チャネル数を(M,N)→($\alpha$M,$\alpha$N)とする
                ($0<\alpha<1$)
            - Resolutional Multiplier
                計算コストの削減
                →画像サイズを小さくする(解像度を荒くする)
                フィルターサイズを($D_F$,$D_F$)→($\rho D_F$,$\rho D_F$)とする
                ($0<\rho<1$)  

            調整後の計算量
            $$
              D_K\cdot D_K\cdot \alpha M\cdot \rho D_F\cdot \rho D_F +
               \alpha M\cdot \alpha N \cdot \rho D_F\cdot \rho D_F
            $$

    1. ## <a id="">物体検出</a>
        どこに何があるか(where, which)
        - 評価指標
            - どこに(where)
                IoU
                予測した領域に対する評価
                どのくらい領域をカバーできたか
                →予測領域∩正解領域 / 予測領域∪正解領域
            - 何が(which)
                accuracy
                分類結果に対する評価

        - ルールベース
        - DL登場以降
        に大別
        1. ## <a id="">Selective Search</a>
            ルールベースのアルゴリズム
            「本物に近い偽物」の候補領域の選定に使用
            - Selective Searchで候補領域(偽)を選定
            - SVMで真偽を混ぜて学習
            - 偽陽性を学習に加えてSVMを再学習

            候補のレベル感を階層でグルーピング
            下位層 細かいオブジェクト(耳・花・部品)
                  オブジェクトくらい(車・木・牛)
            上位層 大きく(空・それ以外)
            ↓
            上位層からは割合多く，下位層からは割合小さく選定
        1. ## <a id="">R-CNN</a>
            2013年登場
            CNNを用いた物体検出モデル
            Selective Searchで候補を探索し，それらをCNNで解析
            最後の分類はSVMで行われていた
            →特徴量の設計・抽出にCNNを用いた

            ROIs毎にCNNに通すので計算・学習速度が遅い
        1. ## <a id="">Fast R-CNN</a>
            2015年に登場
            CNN画像全体から特徴量を抽出→ROIsと対応する候補特徴マップを選定
            →CNNの対象を候補領域毎にするのではなく，画像全体にした(画像にいくつもROIsがある)

            CNNの出力(画像全体の特徴マップ)とROIsを対応させて，候補毎の特徴マップを選定
            →ROI poolingによりリサイズする
            →FCは入力の形が固定のため，全てのROIを同じ大きさにする

            ROI pooling
            FCの入力がN * Nの場合，ROIを縦横N分割にし，各領域でmax poolingなどを行いN * Nの大きさにする

            問題点
            Region proposal(領域提案)が従来アプローチを継承していて，精度が低い
            →ルールベース
            - region proposal自体の精度が低い
                提案を間違えてしまえば，後段のCNNがいくら優秀でも物体検出できない
                →Faster R-CNN
            - region proposalの演算コストが高い
        1. ## <a id="">Faster R-CNN</a>
            2015年に登場
            領域提案にCNNを用いる
            →精度・計算速度向上を実現

            End-to-Endな学習が可能に
            →以前までは途中までにselective searchとかが挟まっていた

            CNNを用いたRegion proposal
            ZFNetやVGG, ResNetで特徴抽出
            →特徴マップの断面(Anchor)毎に9個の候補領域を設ける
            →9個それぞれに，正解領域とのずれ(中心座標(x,y), 縦横の長さ(w,h))・背景フラグと物体フラグ，が存在
            →都合54個の値が出力値となる

        1. ## <a id="">YOLO</a>
            2016年に登場
            Faster R-CNNまでの課題
            認識速度が実用的なレベルでない(GPUをもしいて5fps(1秒間に5枚))
            →検出・抽出を分けたアーキテクチャが限界か

            YOLO(You Only Look Once)
            検出・識別を同時に行う
            →YOLOv2, 3などが存在
            →近年でも使われるモデル

            - 入力画像を複数セルに分割し同一のCNNで行う
                →1セルに対して$B*5+C$の出力
                - セル毎にB個(通常B=2)ずつバウンディングボックスを推定(信頼度を設定)
                    →1セルに対して，$B*5$の出力がある
                    - ボックス中心座標(x,y)，幅高さ(w,h)
                    - confidnce : 物体である確率 * IOU(正解領域との近さ)
                - セル毎に物体クラスを推定
                    物体クラス分類確率Cの推定(バウンディングボックスによらない)
                    →1セルに対してCの出力
            - non-maximal suppressionで，同じクラスとして推定された重複領域を抑制する
                各セルから共通としてもってこっられたバウンディングボックスの中から一番高いconfidenceを残す
                閾値以下のconfidenceを消す

            1. S * Sに分割
            1. a.大きさ推定
                b. 物体推定
            1. 最終決定

        1. ## <a id="">SSD</a>
            2016年に登場
            Single Shot Multibox Detector
            様々なバウンディングボックスを表現可能
            YOLOの親戚
            YOLOより精度向上，処理速度は若干劣る

            YOLOの制約(計算時間を重視した代償)
            - グリットサイズの固定
            - 1セルにつき1つのみ物体識別
            →物体が密集した画像
            →大きさが様々な物体が写る画像
            に弱い

            - バウンディングボックス推定で工夫
                縦横日の異なるデフォルトボックスをいくつか用意
                →後に微調整
            - 分割数を段階的にスケールダウン
                S * Sのように固定せず，少しずつ領域を大きくしていく
                →畳み込み途中のバウンディングボックスが小さい(分割数は大きい)段階でのconfidenceをそれぞれ出力に持ってくる
                (これまでは最後のS * Sの結果しか使っていなかった．S * Sになる以前の特徴マップ(サイズが大きい=グリッドが粗い)のconfidenceも使う)
                →それらを組み合わせることで様々な大きさの物体を検出できる(物体の大きさに合わせたバウンディングボックス)
                (これまでは一定の大きさの物体しか検出できなかった)
            - Hard negatice miningで，画像中の「背景」・「物体」の不均衡度を調整
                背景ばっかり持ってこないように調整
                (画像の中はほとんど背景のため)

    1. ## <a id="">セグメンテーション</a>
        画像に対してピクセルレベルでクラス分類

        画像分類との違い
        - 画像分類
            予測値は一つ(画像毎のクラス)
        - セマンティックセグメンテーション
            予測値は画像サイズ分(ピクセル毎のクラス)

        Encoder-Decoderネットワークと構造は似ている
        入力画像→画像サイズのクラス値を出力

        1. ## <a id="">FCN</a>
            all CNN　全層畳み込みネットワーク
            DeConv層の活用
            最後の全結合層が1 * 1の畳み込み
            →入力の画像を可変にできる(これまでは全結合層の次元数が決まっていたので入力サイズも固定になっていた)

            逆畳み込み Deconvolution
            小さくなった特徴マップを大きくする
            1. stride分だけ空白を設けて特徴マップの要素を配置
            1. kernel size - 1だけ特徴マップの周囲に空白を取る
            1. padding分だけ周囲の空白を削る
            1. 畳み込みを行う

            pooling層の特徴マップの活用
            32 * 32→1 * 1にたたみ込んだ後，32 * 32のdeconv.したい
            →1 * 1から直でも作れる
            →1 * 1を2 * 2にして手前のpoolingとsum．それを32 * 32へdeconv.
            →同様に上の足し合わせた2 * 2を4 * 4にして，poolingの4 * 4とsum．32 * 32へdeconv.
            →ここまでで作られた32 * 32をチャネル方向に1 * 1で畳み込み，ピクセル単位でのクラス分類に用いる

            FCNの課題
            poolingで圧縮した特徴マップを逆畳み込みしているので，
            元画像の空間情報が失われた特徴マップを使うことになる
            →U-Netへ
            特徴マップを一時的に保持し続ける必要があるので，メモリ効率が悪い(U-Netも同様)
            →SegNetへ
        1. ## <a id="">U-Net</a>
            shortcut接続の導入
            →空間情報補完

            目的は異様画像のsegmentation
            shortcutでとも画像の特徴を維持し続ける

        1. ## <a id="">SegNet</a>
            up-pooling使用
            →メモリ効率化

            基本的な構造はU-Netまでと同じ
            shortcutで渡す情報にメモリ効率がよくなる工夫が入る(up-sampling)

            up-sampling
            max-pooling indexを使用して特徴マップをアップサンプリング
            max-pooling index : Encoderのmax-pooling層で抽出したpixel位置を表す添字
            →添字のみの保存で，FCNに比べメモリ効率を大幅に低減
            →max-poolingで取れなかった情報は捨てる

## <a id = "cp11">Chapter 11</a>
1. ## <a id="">自然言語処理</a>
    自然言語：人がコミュニケーションに用いる言語
    - 文書分類
    - 翻訳
    などのタスクが存在

    1. ## <a id="">文書分類タスク</a>
        入力テキストの文脈を学習し，pos/negを分類

        単語はベクトルとして数値化
        →意味を表現

        単語埋め込み(word embedding)
        単語のベクトル化を「単語埋め込み」という
        →単語の意味を保ちながら数値化(ベクトル化)できる
        →MLPを用いて，ベクトル化を行う
        →[単語フラグデータ(疎)] - [MLP] - [単語埋め込み後(密)]
        →オートエンコーダーのように密なベクトルを作ることができる

        学習方法
        →下記二つを正しく行われていれば，正しく意味を理解できている考えられる
        - CBOW(Continuous Bag-of-Words)
            前後いくつかの単語間の単語を予測
            →myとisを入れて，name?が出力
        - Skip-gram
            一単語からその前後の単語を予測する
            →nameを入れて，my?, in?が出力

        Bi-LSTMで解く
        word embedding + Bi-LSTMでneg/pos分類
        →順方向・逆方向それぞれの最後の出力を集約したものをMLPとsigmoid(softmax)に通して分類

        enbedsの系列長が揃っていないと処理が困難になる
        →paddingを行って揃えることも
    1. ## <a id="">翻訳タスク</a>
        1. ## <a id="">モデル</a>
            ルールベースと統計的モデルに大別
            1. RBMT(Rule-Based Machine Translation)
                登録済みのルール(文法)を適用し，訳文を出力
                - メリット
                    - 文語に強い
                        あまり使われない単語なども正確に訳せる
                    - 訳文の解釈性が高い
                        どのように訳されたのかわかりやすい
                    - 専門書向き
                        マニアックな単語にも対応
                - デメリット
                    - 口語が難しい
                    - 辞書に依存
                    - 辞書の生成は人手
            1. SMT(Statistical Machine Translation)
                統計モデルを学習させ訳文を出力
                →DLを用いた翻訳はSMTに該当
                1. 原文と対訳を用意
                1. 関係を学習
                1. 学習した文法関係で訳文を推論
                - メリット
                    - 文語・口語問わず訳せる
                    - データが充実すれば人間の翻訳精度を超える
                    - 文脈を踏まえた翻訳
                        文法のみならず，細かなニュアンスも学習できる
                - デメリット
                    - データ依存
                    - 頻度の少ない用法・語句に弱い
                        マニアックな用語など
                    - 対訳データの収集が難しい
        1. ## <a id="">評価指標</a>
            - 分類タスク
                - precision
                - recall
            - 翻訳タスク
                - Perplexity：流暢かどうか
                - BLEU：意味が正しいか

            1. Perplexity
                「次の単語として絞り込めた候補単語数」を測る
                $$
                  Perplexity = 2^H ;\,\,
                   H = -\frac{1}{N}\sum_{n=1}^N \sum_{t=1}^T\log P(w_{nt})\\
                   \textrm{where;\, N:\,batchsize,\, T:\,words\,in\,one\,sentence}
                $$
                $P(w_{nt})$ : n番目の文章のt番目の単語の出現確率

                - Perplexityが低い→次の単語を絞り込めている
                    →文章の流暢さを表しているとも言える
                - 確率の逆数を算出することで候補単語数を導く
                - 正解データと照らし合わせる必要がない

            1. BLEU
                予測分と正解分の類似度を測る
                →定義式はスクショ参照

                - 前提として「プロの翻訳者の役bん(正解文)に近いほど，その機械翻訳の精度は高い」
                - 値域は0〜1．0.4以上であれば高品質

                注意点
                - 性能指標としての問題点
                    - 字面しか考慮されない
                        類語は全く異なるものとして認識
                        notなどが入る否定分と肯定文の区別がつかない
                    - バリエーションが多い
                        ライブラリによって定義式が異なり一貫性がない
                - 準備段階のコスト
                    - 正解分の用意が必要
                    - n-gramの種類数を指定する必要がある
                        多くの場合は1〜4-gram単位で計算される

                上記のような注意点はあるものの，デファクト・スタンダードである

                N-gram, n-gram
                - N-gram
                    手法の名前
                    ある文章を，連続するn文字/単語の塊に分割する
                - n-gram
                    分割された塊

                N-gram言語モデル
                N-1単語までを事前情報とする
                →前提：あるsentenceが得られる確率は，単語の条件付き確率の重ね合わせとなる
                →仮定：ある単語おの分布は直前のn-gramのみに依存して定まる
                →N-gramで分割された文章(n-gramの配列)を確率分布で表現したモデル
                （言語モデル：目的言語の文章が得られる確率を推論するモデル）
                $$
                  P(\textrm{senence}) = \prod_{i=1}^T P(w_i |\underset{全ての単語}{\underline{w_0,\dots,w_{i-1}}})
                $$
                →ある単語の発生確率は，それまでの単語に由来するはず

                N=1の場合，事前情報なしのモデルとなる
        1. ## <a id="">翻訳タスクとNN</a>
            前項目までで翻訳タスクを解く準備が整った
            - 入力文章をword embeddingで数値化
            - 精度を測る，微分可能な評価指標

            一方で，どのように文章(系列データ)を出力するのか，の問題が残る
            →系列データを出力する「生成モデル」

            e.g.) Encoder-Decoderモデル
            1. Encoderが潜在変数に落とし込み
            1. Decoderが潜在変数を元にデータを生成
            →文章を入れて文章を出すことに適している
            [系列データ] - [Encoder] - [文法文脈] - [Decoder] - [系列データ]

            Encodre, DecoderにはRNN系モデルが使われる
            →RNN, LSTM, BiLSTM, GRUなど
            - Encoder
                「前の単語からの文脈や文法」をエンコード
            - Decoder
                「エンコード情報」をデコードして文章を生成

            Encoderで圧縮する意味
            →入出力文で対応する単語の位置やn-gram数が異なる
            →それに対応できる

            1. ## <a id="">Sequence-to-Sequence (Seq2Seq)</a>
                系列データを入出力とするアーキテクチャ
                [Sequence] - [Encode] - [Context(文脈)] - [Decode] - [Sequence]
                のループ

                機械翻訳，自動要約，質疑応答，メールの自動返信などが応用先

                モデル構造
                - Encoder
                    埋め込みベクトルを順にRNNに入力
                    各タイムステップの出力は捨てる(次の入力に使わない)
                    最終単語の隠れ状態をcontextとする(Decoderへ)
                    →ここに全ての情報が含まれている
                - Decoder
                    最初の隠れ状態はcontextを受け取る
                    最初の入力は開始トークン<SOS>，終了トークン<EOS>が出力されるor指定の長さ以上で終了

                    各タイムステップの出力単語を再度embedding(埋め込みベクトル化)して次の入力へ
                    1. 各層の出力を確率ベクトルpに変換して，次にくる単語を求める
                        MLP(隠れ状態空間×語彙空間)に通し1本のベクトルにする
                        softmax関数を使って，語彙空間の確率分布に変換
                    1. pを離散化して，埋め込みベクトルを得る
                        離散化の例：単語出現確率$\bm{p}^t$が最大のものを1に，他は0に
                        →次にくる確率が最も高い単語
                        →それをyとして次のステップの入力とする

                損失関数
                下式の最小化→Pの最大化
                $$
                  L = -\log P(y^1,\dots,y^{T^{\prime}} | x^1, \dots,x^T)
                $$
                $x^t$ : 入力文のt番目の単語
                $y^t$ : 正解文のt番目の単語

                Seq2Seqの特徴
                - Encoder, Decoderの構造が異なっても良い
                - Decodreにcontextを渡す方法はいくつかある
                    初期値として与える
                    全てのタイムステップで隠れ層に接続する
                - 多くの派生モデル．目的に応じて使い分け
                    汎用性が高い
                    改良
                    - 入力データの反転　Reverse
                        エンコーダの入力文を反転させる
                        →デコードするまでの道のりが短くなるため，学習が進みやすく精度も期待できる
                    - 覗き見 Peeky
                        エンコード結果contextの有効活用
                        →contextを別のRNN系モデルにも「覗かせる」ことで，skip-connectionの効果がある

            1. ## <a id="">Attention</a>
                Seq2Seqのエンコーダの出力の有効活用
                →系列長の課題をクリアできる
                EncoderがDecoderの出力に注意を払う仕組み
                →入力単語と出力単語の対応関係を学習する

                Attention mechanismの仕組み
                出力単語の予測に寄与する入力単語の情報を選ぶ
                →抽出操作は微分不可能のため，BPできない
                →各出力に重みaを掛け合わせて抽出する
                →微分可能な抽出操作ができるようになる

                重みaの求め方
                - 加法注意 Additive Attention
                    重みaを求めるために隠れ層1つのFFNを用いる
                    →現時刻の変換(翻訳)を行うために必要な情報が含まれるように学習する

                    query : Encoderの隠れ状態の集合($\bm{hs} = \bm{h^1},\dots,\bm{h^T}$)
                    key : Decoderの隠れ状態(at 現在)($\bm{h^1}$)
                    ↓
                    FFN
                    ↓
                    結果をsoftmaxに通したものが重みa
                    (重みの種sを受け取り，softmax関数に通し，Attentionの重みを計算)

                    FFNでは全てのtに対して
                    $$
                      \bm{s}^t = v_a^{\textrm{T}}\textrm{tanh}(W_a\bm{h^1} + U_a\bm{h^t})
                    $$
                - 内積注意 D0t-Product Attention
                    内積を用いて重みaを計算

                    $$
                      \bm{s}^t = \textrm{dot}(h_t, h^1)
                    $$
                    種sの計算に上式を利用．
                    その後にsoftmaxに通すのは同じ
                    →割合ベクトルを求める

                    加法注意よりもGPUを用いやすいため，よく用いられる

                重みaとエンコーダの隠れ状態hsの内積をとり，重み付き和hとする
                それをデコーダ(MLP+softmax)に通す
            1. ## <a id="">Attentionつきモデル</a>
                系列同士の類似度を計算する方法の違い
                1. self-Attention
                    一つの系列内にて，各要素が他の要素に対してどのような関係性があるのか見る
                1. Source Target Attention
                    Seq2Seqのように異なる系列間の各要素の類似度を算出

                - Google Neural Machine Translation(GNMT)
                    AttentionつきSeq2Seq
                    特徴
                    - レイヤの多層化
                    - 双方向LSTM
                    - skip-connection

                    重要
                    - モデル並列
                    - データ並列

                    LSTMなどの時系列モデルは，時系列順に学習を行うため，並列計算が難しい
                    →複数GPUでの分散学習
            1. ## <a id="">Transformer</a>
                Attentionのみのneural機械翻訳
                (RNNモデルを用いていない)
                →時系列データの学習においても並列計算でき，収束が早くなった

                従来Attentionは精度向上の補助的な役割であった
                →メイン機構として採用したのがTransformer(Attention is All You Need)

                基本的な考え方
                Self-AttentionとSource Target Attentionの組み合わせで単語間の関連性を学習
                →単語の時系列順序を学習できない？
                →Positional Encoder：単語の順序・位置関係を考慮
                →Masked Multi-Head Attention：未来の情報のカンニングを防ぐ

                - Positional Encoder
                    単語の順序・位置関係の情報をベクトル化して，埋め込みベクトルに付与する

                    これまでの課題
                    Attentionのみでは単語間の時系列順序を学習できない
                    →「位置ベクトル」を埋め込みベクトルに足す
                    →sinとcosで計算される
                    →単語同士の相対的な位置関係を規定できる
                - Masked Multi-Head Attention
                    decoderは系列の後ろの情報(未来の情報)を見れないようにmaskedする
                    →memoryにおける未来の情報を，予測に寄与させてはいけない

                1. ## <a id="">OpenAI GPT</a>
                    TransformerのEncoder(Trm)を単語モデルに応用したアーキテクチャ

                    GPT1,2,3がある
                    →GPT2はあまりに人間そっくりな文章を生成し危険なため，一時公開が中止された

                    問題点
                    次の単語を予測するタスクであるため，未来の単語情報が使えない
                    →BERTで解決
                1. ## <a id="">BERT</a>
                    Bidirectional Encoder Representations from Transformer
                    双方向にTransformerのEncoderを用いた言語モデル
                    言語間の関連性を正確に捉えられ，現代NLPの事前学習のデファクトスタンダード

                    二つのモデルを用いている
                    両者ともアノテーションのコストが低い
                    →インターネット上にある文章をピックアップするだけで大量に学習させることができ，効率が良い
                    - Masked Language Model(MLM)
                        文章の穴埋め問題を解くモデル
                    - Next Sentence Prediction(NSP)
                        2文が渡され，連続した文かを判定

## <a id = "cp12">Chapter 12</a>
1. ## <a id="">音声処理</a>
    音声データ
    →波形
    →アナログな情報である(時系列順に連続的な変化を伴う)
    →デジタル信号へ変換の必要

    1. サンプリング
        時間方向に離散化
        1sにどのくらい標本を取得するか
    1. 量子化
        電圧方向に離散化
        量子化ビット数→何分割にするか
        - 線形量子化
            等間隔で量子化
        - 非線形量子化
            注目部分を細かく量子化
            →それ以外を少しあらく量子化
            →より人間の知覚に近い
            →WaveNetの前処理に用いられている

            $\mu$-law量子化
            WaveNetの前処理で用いられている非線形量子化
            $$
              f(s) = \textrm{sgn}(s)\frac{\ln(1+\mu|s|)}{\ln(1+\mu)}
            $$
            s : 正規化された標本化値(-1〜1)
            $\mu$ : WaveNetでは255
            0付近で広い量子化を行える
    1. 符号化
        量子化された値を2進数にする

    - 音声認識
        入力が音声
        1. 音響分析
            音の特徴量を抽出
            強弱，周波数，間隔
            [入力音声] → [特徴量]
        1. 音響モデル
            音素マッチング
            a-r-i-g-a-t-o
            [特徴量] → [学習済みモデル] → [音素]
        1. 発音辞書
            音の組み合わせをピックアップ
            →単語化
            a-ri-ga-to
            [音素] → [データベース] → [音の組み合わせ]
        1. 言語モデル
            直後の単語の出現頻度をモデル化
            [音の組み合わせ] → [文章(単語の組み合わせ)]
    - 音声合成
        出力が音声
        1. 規則合成
            ルールに基づいて音の性質を変換
            - 音声変換
                入力音声の性質を変換
                →歌声や，キャラクターの音声などに変換
        1. 波形接続方音声合成
            音素を連結して音声を合成
            - text to speech (TTS)
                入力したテキストを音声に変換
        1. 統計的パラメトリック音声合成
            統計的モデルで学習した出力をもとに音声を合成
            →WaveNetはここに含まれる

    タスク
    - 多話者音声合成
        別の人の音声・イントネーションに変換(意味は理解していない)
        多様性を持つ複数の話者の音声の特徴を学習し，イントネーションなどを再現
    - テキスト音声合成(TTS)
        入力テキストから得られた言語的特徴と周波数の特徴を学習し，音声を合成
    - 音楽
        音楽音声の特徴を学習し，調和的な音声を合成
1. ## <a id="">WaveNet</a>
    DeepMindが開発

    前処理
    補助特徴量
    →$\mu$-law量子化(8bits)→符号化で得られたデータをone-hot(256 channels)に変換

    学習時
    CNNを用いた自己回帰モデルの応用
    $$
      p(x) = \prod_{t=1}^T p(x_t|x_1,\dots,x_{t-1})
    $$
    $x_t$の出現確率は$x_{t-1}$までの情報に基づいて定まる
    マルコフ過程を仮定

    基本的な特徴
    時間ごとに畳み込んで行って，256チャネル分類を行う
    - プーリング層がない
    - $-\log p(x)$を最小化するように学習
        最尤推定のように

    重要な特徴
    - Dilated Causal Convolutions
        過去のみの情報に基づいて(Causal Conv.)，飛び飛びでたたみ込む(Dilated Conv.)処理
        - Causal Conv.
            通常の畳み込みでは，縦横など前後を無視して周辺を畳み込んでいた
            →時系列では，前後の後を畳み込みに用いてはいけない
            →未来の情報がわかった上で分析をすることになってしまうため
            →過去の情報のみを参照する機構

            問題点
            受容野を増やすには膨大なレイヤやフィルタが必要
        - Dilated Conv.
            カーネルの要素の間を0でパディング，パラメータ数を増やさず需要野を広くする
            1つとばしで選ぶを層を深くすれば，指数関数的に需要野が広くなる
    - Residual Skip Connections
        入力をDilated Conv.の出力に接続することで，その残差を学習する
        →勾配消失問題回避の目的で使われる
    - Gated Activation Units
        residual block内の計算
        x : Dilated Conv.の出力
        下式のzとskipを足し合わせて次に行く
        $$
          z = \textrm{tanh}(W_{f,k}*x)\odot\sigma(W_{g,k}*x)
        $$
        $*$ : 1$\times$1の畳み込み
        $\odot$ : ベクトルの要素ごとの積
        $\sigma$ : 次に渡す確率

1. ## <a id="">WaveNetの応用</a>
    conditional WaveNet
    追加の情報を付け加える
    - Global Conditioning
        大局的な条件づけ
        →誰が喋っているか
        全てのタイムステップにわたって出力の分布に影響を与える
        活性化関数
        $$
          z = \textrm{tanh}(W_{f,k}*x + V_{f,k}^T h)\odot\sigma(W_{g,k}*x + V_{g,k}^T h)
        $$
        V : 学習可能な線形射影
        h : 時刻に変わらないベクトル
    - Local Conditioning
        局所的な条件づけ
        TTSにおける言語的な特徴
        →「〜は」は「〜ha」と発音するなど
        transpose convolutional network(deconvolution)で
        サンプリング点ごとの影響を畳み込んであげるイメージ
        y(=f(h)) : xと同じステップの新しい写像結果
        活性化関数
        $$
          z = \textrm{tanh}(W_{f,k}*x + V_{f,k}*y)\odot\sigma(W_{g,k}*x + V_{g,k}*y)
        $$

## <a id = "cp13">Chapter 13</a>
1. ## <a id="">生成分野</a>
    多様性と画質
    →トレードオフの関係
    1. ## <a id="">GANs</a>
        1. ノイズから画像へ
            - 2014 GAN
                28 * 28の低解像度画像生成
                GとDの学習バランスが難しいことが課題
                ↓低解像度と高解像度の差を学習
            - 2015 LAPGAN
                Laplacian Pyramid GAN
                256 * 256の中解像度画像生成
                低解像度と高解像度の差を学習
                →簡単に生成できる低解像度画像を用いて，少し解像度の高い画像を生成
                →これを繰り返し，最終的に解像度の高い画像を生成
                複数Gが必要で，計算コストが高い
                ↓CNNを導入し，ピクセル間の関係性を学習
            - 2015 DCGAN
                DSeep Convolutonal GAN
                64 * 64の中解像度画像生成
                GとDをCNNで設計
                →従来は全結合層
                CNNフィルターを可視化することで生成モデルに解釈性を与える
                GPUを使い計算コストを下げる
                画像生成ではスタンダードなモデル
                Batch normalizationを導入し学習を安定
                →活性化関数にReLUだけでなく，tanhやLeaky ReLUを使用(ReLUだけではうまくいかなかった)
                - Generator
                    入力：100次元のノイズz
                    転置畳み込みにより64 * 64にアップサンプリング
                    →全結合層やプーリング層は使用しない
                    →各転置畳み込みの後にbatch normalizationと活性化関数(ReLU)が入る
                    →最終層のみ活性化関数はtanh(-1〜1の出力にして0〜255に変換)
                    (→ReLUだと無限大まで発散してしまう)
                - Discriminator
                    Gのupsamplingを逆にしたdownsampling
                    基本的には画像分類のNN構造を踏襲するが，プーリング層がなく，活性化関数はReLUの代わりにLeaky ReLUを使用
                    →ReLUではBPの更新で，0以下の誤差が上層(G)に伝わらない
                    →ただの画像分類ではなくなることが良いことだったが，今回は上層にGがいてGも学習しなければいけないので伝播が必要
            - 2017 PGGAN
                Progressive Growing GAN
                1024 * 1024を実現
                - progressive growing
                    低解像度画像生成から段階的に公開ぞど画像生成を実現
                    初めは4 * 4から学習し，うまく行ったら8 * 8などのように解像度をあげていく
                計算コストが高い→何回も別のモデルを学習するため

                LAPGANとの違い
                →LAPGANは低解像度から高解像度まで先にモデルを全て組み学習を行う
                →PGGANは低解像度でモデルを学習し，終わったのちに少し高解像度でモデルを深くして学習

                課題
                層を増やした瞬間，Gの出力にランダム要素が強く反映されてしまう
                →追加した層をskipするようにパスを組む
                →skipした方と，新しい層を通った方に$1-\alpha, \alpha$の重みをつけて影響度$\alpha$を段階的に増やす
        1. ノイズ+$\alpha$から画像へ
            classやlatent(潜在変数)情報(メタ的な情報)
            - 2014 cGAN
                Conditional GAN
                2014 GANにclass情報を与える
                ↓classをlatentで代用
            - 2016 InfoGAN
                クラス情報不要
                潜在変数を調整し，画像の微調整ができる
                →ノイズのような潜在変数はいらず，入力する潜在変数によって生成画像も大きく変わることが大事
                →潜在変数とがず分布の相互情報量を評価指標に導入(分布が近づくように学習)
                →生成画像分布に大きな影響を与える潜在変数の獲得を目指す
                cGANのようなラベルづけされたデータは不要
            - 2016 ACGAN
                Auxiliary Classifier GAN
                補助的な分類器を設けた
                →GoogLeNetのAuxiliary lossと発想は同じ
                2014 cGANに「識別性」と「多様性」を評価する指標を導入
                128 * 128
                cGANにclassを追加して入れ，Dは生成画像の真贋判定と，クラス分類のタスクを行う
                識別性の高い＋高解像度の画像生成を目標とした
                結果，「ノイズzは大局的な構造を，classは局所的な特徴を表現できるのではないか」という仮説を提唱した
            - 2018 SAGAN
                Self-Attention GAN
                2017 PGGANにSelf-Attention, TTUR(two time-scale update rule)を導入し，CNNによる計算コストを抑える
                局所的な特徴に注目するConv層には「長期依存性」と「高計算コスト」の課題があると提唱
                Self-Attentionで画像のより大局的な特徴に注目し，さらに計算コストを抑える
                →画像内で「離れていても似たようなピクセル同士」の関連性を抽出できる
                →CNN1層では局所的な特徴にしか注目できず，また層を重ねるごとに長期依存性の課題を抱えていた
                →そのまま適用するとメモリ消費量が莫大になってしまうので1 * 1のConvを用チャネル数を減らしてから適用

                TTUR
                Gの学習率を0.0001，Dの学習率を0.0004と異なる値にする
                →ゆっくり学習を進めるGに対し，Dの学習はパッと進める

                GとDにSpectral Normalizationを適用
                →パラメータの大きさをコントオールし，異常な勾配が伝わることを抑制

                ↓Truncation Trickの導入
            - 2018 BigGAN
                512 * 512
                ImageNetの画像を使用し，1000カテゴリーの画像を生成
                SAGAN + Truncation Trick
                ノイズzの閾値を設け，多様性と画質のバランスを調整
                →これまでは多様性・画質どちらかに寄ったモデルを構築し学習していた
                →BigGANは1つのモデルで，学習後にパラメータをいじるだけでトレードオフの上を移動できる
            - 2018 StyleGAN
                PGGANのGにStyle要素(雰囲気)を与える
                →髪型や輪郭，顔の向きなど
                1024 * 1024
                StyleはFC層 * 8で学習されたもの
                StyleはAdaINによって調整される
                (StyleGANにもTruncation Trickが使われている)
                各層に入力するw(Styleとなる潜在空間)を途中でき切り替えることでスタイルミックスが可能
                Progressive Growingを用いた高解像度画像生成
                低解像度画像にStyle組み込む→髪型・輪郭，メガネなどのスタイルを引き継ぐ
                高解像度画像にStyle組み込む→配色などのスタイルを引き継ぐ

                AdaIN
                スタイル変換用の正規化手法
                正規化されたコンテンツ情報にスタイルを用いた線形変換をかける
                Instance Normalization等の正規化手法と異なり，スタイルとコンテンツ画像の統計量のみで正規化を行う
                →学習データで見たことがないスタイルへの変換も可能
                $$
                  \textrm{AdaIN}(x_i,y) = y_{s,i}\frac{x_i - \mu(x_i)}{\sigma(x_i)} + y_{b,i}
                $$
                $\mu$ : 平均，$\sigma$ : 標準偏差
                HW方向(画像一枚)に対して，チャネルごとに正規化
                →どのチャネルをどの程度強調するかがyによって適用的に(addaptive)変化
                →各解像度のステップにおいて重視されるスタイルが異なるので，様々なスタイルを取り出すことができる
                →訓練対象のパラメータは存在しない
        1. 画像から画像へ
            - 2016 pix2pix
                2014 cGANにconditional vector(ここではclass)
                を画像(輪郭や白黒など)として入力
                GAN画像変換の礎
                    同一のモデル構造で様々なタスクを解くことができる
                1ペアの画像の関連性を学習
                e.g.)白黒→カラー，地図→衛生写真など

                これまで(cGAN)
                →「画像」と「クラス情報」の対応を学習
                pxi2pix
                →「画像」と「条件画像」の対応を学習

                GeneratorにU-Netを使用
                →ピクセル単位での対応が必要(セグメンテーションと似ている)
                Discriminator
                →通常GANでは本物偽物を見分ける目的関数を設計
                →今回は，入出力の一致度を測る指標が別途必要
                →「条件画像と画像のペア」の一致度を保つための制約条件
                →L1損失関数
                →Gの出力が条件画像の全体像に近づく様に学習

                問題点
                ペアの画像を準備する必要
                →CycleGANへ
            - 2017 CycleGAN
                郡から郡への変換を可能に
                2画像群を用意
                →ペア画像を集める必要はなく，ドメインの様々な画像，別ドメインの様々な画像の集合を集めれば良い(ペアでなくても良い)
                →変換して変換した画像(→元の画像群に戻る)と，変換前の元画像との差(cycle-consistency loss)を埋めるように学習する
                →しまうまから馬の変換ができるように
            - 2017 StarGAN
                複数群間の変換が可能に
                一つのモデルで様々な郡へ変換可能に(これまではモデルと変換組が1対1だった)
        1. その他
            - 2016 StackGAN
                2014 cGANにcponditional vectorを文章として入力
                文章から画像生成
                GANを二段で構成
                Stage Ⅰで「スケッチ」し
                →文章から粗い画像を生成するGAN(64 * 64)
                Stage Ⅱで「精製」する
                →1段目の画像を高精度にする(256 * 256)
            - 2017 AnoGAN
                異常検知特化
                GANの入力を変化させて，入力画像にできるだけ近い画像を生成
                入力画像が潜在空間に含まれていない場合(正常画像でない)，うまく画像生成できないため異常と判断する
                →正常画像を作るにはどの様なノイズが良いのか
                →それを学習したGANに異常画像を流すと，どう頑張っても一致することができない→異常と判断

        DCGANにおけるCNNフィルターの効果
            CNNフィルターの効果(何をしているフィルターなのか)を解釈できる
            →「窓フィルター」ありなしの画像を見比べると，画像中の窓がドアや壁に変換されている
            →逆に画像を見比べることで，そのCNNフィルタが何をしているかわかる

        InfoGANにおけるlatent(潜在変数)の役割
            何かした意味を持って投入するわけではなく，学習した結果それを調整してみて何を表すのか測る
            潜在変数を変えることでの出力を見比べ，その潜在変数が何の意味を内在しているのかわかる

## <a id = "cp14">Chapter 14</a>
1. ## <a id="">DQN</a>
    Q学習の限界
    取りうる状態sや行動aが多くなるとQ-tableの収束が困難になる(オセロとか)
    →最適なQ関数を求めるためにNNを用いる(入力(s,a))
    →DQN

    以下の制約を考慮できる
    - ある状態sでは物理的に取り得ない行動aがある
        Q-tableではそれも用意する必要があった
    - 最初から全ての行動がわかっていることは少ない

    2つのNNを用いて，最適行動価値関数$Q^{opt}$を求める手法
    1. Q Networkはenviroment内のagentに対して試行錯誤の行動を促す
    1.1. Q Networkはenviroment + agentがら現状態sを受け取る
    1.2. Q NetworkはQが最大の行動$a^*$を促す
    2. enviroment + agentからReplay Memoryに行動した結果を転送する
    3. Replay MemoryはN step分(ミニバッチのイメージ)の経験を2つのNNに受け渡す
    3.1. Q Networkに現状態と行動(s,a)を私，状態と行動を学習させる
    3.2. 次にとるべき行動はこうであった$s^{\prime}$をTarget Q Networkに渡し，教師ラベル(次の行動)を学習させる
    4. Qで現在での行動価値関数$Q(s,a;\theta)$とtarget Qで次の行動価値関数$Q(s^{\prime},a^{\prime};\theta^-)$の最大値を損失関数に渡し，勾配を計算
    5. 勾配を利用してQ Networkのパラメータ更新

    損失関数の設計
    TD誤差を活用して，二乗誤差関数を設定する

    工夫
    - Experience replay
        一度経験した状態/行動/報酬/遷移先をメモリに蓄積し，学習時にはそのメモリからランダムにサンプリングしてミニバッチを作る手法
        →時系列に相関のあるデータでは，ある特定の経験フローに過学習しやすい
        (イメージ：同じ作業ばかりしていると慣れてしまい，新しい環境に対応しにくくなる)
        →時系列の相関を学習しないモデルを作るために「経験」をシャッフルする
    - fixed target q-network
        メモリからいくつかの経験をまとめたミニバッチを学習している際には，target Q Networkの$\theta$を固定する
        →教師ラベルの役割を担うTarget Q Networkの出力が1ステップごとに変化すると，Q Networkをどこに収束させるべきか定まらず，学習が安定しない
        →$\theta$のパラメータ更新を行わない(ある程度)
    - reward clkipping
        与える報酬を「正なら＋1，負ならー1」のようにclipする
        →報酬の値を丸め込む
        →学習を簡略化する(DQNは学習の収束が難しいので簡略化する)
        →Q値の急激な増加を抑え，勾配が安定することで学習も安定する
        →報酬の代償の区別がないので，タスクによって想定する結果が得られない
1. ## <a id="">ApeX</a>
    アーキテクチャ
    1つのLearnerと複数のActorがReplay Memoryを共有する分散学習
    →複数のActorによって幅広い経験(experience ($s,a,R,s^{\prime}$))が蓄積されるので，「データ分散型の学習」と言える

    Learner
    優先順位付きの経験をもらう
    →複数のActorでReplay Memoryはたくさん貯まる一方，学習するLearnerは1つなので優先順位が必要
    →Actorからパラメータ更新のリクエストを受け付けたら，LearnerのNetworkパラメータを同期

    Actor
    複数のNetwork, Enviromentにより，優先順位付きの経験を探索する
    「経験を探索する学習器」たくさんゲームする
    - それぞれのActorが，それぞれのNetwork, Enviromentを持っている
    - それぞれのEnviromentで「経験」を観測し，initial priorityを計算してReplay Memoryに送る
    - 定期的にLearnerのパラメータ同期をリクエスト

    Replay Memory
    優先順位付きの経験保存場所

    優先順位付き経験再生(優先順位の付け方)
    Prioritized Experience Replay
    学習がより早く進む「経験」を優先的にサンプリング
    →初めましての経験は優先されるべき
    $$
      P(i) = \frac{p_i^{\alpha}}{\sum_k p_k^{\alpha}}\\
      \textrm{where\,} p_i = |\delta_i| + \epsilon\\
      \delta_i\,:\,\textrm{ApeXのTD誤差}
    $$
    間違えた問題は誤差が大きくなりサンプリングされる

    重要度サンプリング
    優先順位付きサンプリングでは分布の偏りが発生する
    偏りの修正に重要度サンプリングを用いる
    →重み更新に，TD誤差$\delta_i$ではなく，重気付けされた$w_i\cdot \delta_i$を用いる
    $$
      w_i = \left(
        \frac{1}{N}\frac{1}{P(i)}
        \right)^{\beta}
    $$

    その他のテクニック
    - Dueling Network
        状態価値関数$(V(s))$を計算するモデルと，Advantage$(Q - V(s))$を計算するモデルに分岐させて，行動価値関数$(Q)$を予測する

        通常
        直接的に行動価値関数Qを予測する

        行動をとった際に，状態が大きく変わる場合とそうでない場合が存在する
        →その時の状況，と行動による影響，に分けて考える

    - Double Deep Q Network(DDQN)
        Q NetworkのQ関数に見積もられる「価値最大行動」から遷移先の行動価値を算出し，損失関数(TD誤差)を計算する
        →DDQNでは推定誤差が減少する(イメージとしては，未来に対する過大評価がなくなる)
        - DQN
            $$
              \delta_t = R_{t+1} + \gamma \underset{a^{\prime}}{\textrm{max}} Q_{target}(s^{\prime},a^{\prime}) - Q(s,a)
            $$
            Target Q NetworkのQ関数で遷移先の行動価値を算出
        - DDQN
            $$
              \delta_t = R_{t+1} + \gamma Q_{target}(s^{\prime},
                \underset{a^{\prime}}{\textrm{argmax}}Q(s^{\prime},a^{\prime} - Q(s,a)
            $$
            Q NetworkのQ関数により見積もられる「価値最大行動」から瀬に先の高津かちを算出
            →最適な行動はこれ，その価値はこれ，とTargetの方だけで決めちゃうと過大評価してしまう(突っ走っちゃう)
            →Qの方で，行動はこれと決めて，その価値はどれくらいか，とTargetに聞く
    - multi step bootstrap target
        Q値の目標値を「n-step先までの累積報酬＋n-step先の状態$S_{t+n}$　でのQ値」として，損失関数(TD誤差)を計算する
        - DQN
            $$
              \delta_t = R_{t+1} + \gamma \underset{a^{\prime}}{\textrm{max}} Q_{target}(s^{\prime},a^{\prime}) - Q(s,a)
            $$
            $R_{t+1}$ : 1-step先までの報酬
            $\gamma \underset{a^{\prime}}{\textrm{max}} Q_{target}(s^{\prime},a^{\prime})$ : 1-step先の状態$s^{\prime}$でのQ値の最大値
        - multi-step bootstrap target
            $$
              \delta_t = R_{t+1} + \gamma^2 R_{t+2} + \cdots + \gamma^{n-1} R_{t+n-1} + \gamma^n \underset{a^{\prime}}{\textrm{max}} Q_{target}(s^{\prime},a^{\prime}) - Q(s,a)
            $$
            $R_{t+1} + \gamma^2 R_{t+2} + \cdots + \gamma^{n-1} R_{t+n-1}$ : n-step先までの累積報酬
            $\gamma^n \underset{a^{\prime}}{\textrm{max}} Q_{target}(s^{\prime},a^{\prime})$ : n-step先の状態$s_{t+n}$でのQ値の最大値
            →先がわからない中で先の累積報酬を足し合わせていくには一長一短があるが，ApeXのTD誤差としては，単純に二つ足し合わせたものを全体の誤差とする
1. ## <a id="">R2D2</a>
    Recurrnet Replay Distributed DQN

    ApeXにLSTMを導入したアーキテクチャ
    最大の特徴：LSTMによって状態sと行動aの時系列を学習
    →状態と行動は時系列のためLSTMでうまくいくのが普通の発想
    →導入がそこまで簡単でなかった(RNNの初期化が難しかった)

    - Stored state
        経験と共に「LSTMの隠れ状態」を保存し，それを学習時のネットワークの初期化に使用
    - Burn-in
        経験再生する前に，「burn-in period」を設けて，初期状態を作り出すためにLSTMを用いる
        →隠れ層の初期状態を適当なものにしないで，スタートする少し前にperiodを設けて，隠れ層を適切な初期状態にする(エンジンをあっためておくイメージ)

    R2D3 (R2D2 from demonstrations)
    R2D2にdemonstrations(人間の熟練者の経験データ)を導入
    actorが経験を積み，replay memoryに経験データを蓄えていた
    →教師データを作らなくてもよかった
    が，有名なゲームであればそもそもデータあるよね
    →$\rho$ : demonstrationsの割合で全体の経験の配分を調整
1. ## <a id="">Alpha Go</a>
    オセロや囲碁は「取りうる状態」「それに応じた行動」のパターンが多すぎる
    →DQNでQ関数を求める
    →それでも多すぎる
    →予め，有力候補の手(状態sにおける行動価値が最大となる行動a)を探索しておきたい
    →一人二役の自己対決シミュレーションを行う(その手法：MCTS モンテカルロ木探索)

    アーキテクチャ
    手をMCTSで探索し，DNNで試す
    - DNN
        活用するイメージ
        碁盤の履歴sを入力として，次の手の確率分布P(各手の選択確率)と，現在の盤面からプレイヤーが勝てる確率vを出力
    - MCTS
        探索するイメージ
        現在の盤面から一番有効な手を探索．
        DNNを活用し，ツリーの各ノードの拡張・評価を行う
        (P : DNNで予測した次の手の確率分布
          V : DNNで予測した現盤面の勝利確率
          Q : ノードの評価値．全子ノードのVの期待値
          U : P/(1＋N(訪問回数)))に比例

          $\pi$ : MCTSで計算した次の手の確率分布)
        1. 選択
            Q＋Uが最大の手順が選択
            →通常，行動価値Qのみが最大となる行動を選択するが，Qが正確である保証はない
            →Uで補正をかける
            →訪問回数が少なければ，その行動の経験が浅く信頼に欠ける
            $$
              U \propto \frac{P}{1+N}
            $$
            選ばれる確率Pが高いが探索回数Nが少ない行動(相対的にUが高い行動)を積極的に採用する．
            →知識の浅い行動を探索し，同時に正確さのかけるQによって謝って行動を起こす確率を低減
        1. 拡張と評価
            ノードが拡張される(考える手番を増やす．打ったことのない手を考えられる)
            sにおける各手の選択確率は，DNNの出力Pに更新される(評価)
        1. Q更新
            ノードがDNNによって増えたので，ノードの先端からBPのように行動価値Qを更新

            Vは現盤面から将来的に勝利できる確率(DNN予測値)
            →Vの部分木ごとの期待値がQとして計算される

            Qは「このまま最適手を打ち続けたら勝てそうか」を示す行動指針
            →数手先を読み勝ち筋を考えるプロ棋士の思考に似ている

            (DNNはMCTSによるシミュレーションによって生成されたエピソード(経験データ)を用いて学習を繰り返し，より信頼度の高いVを生成している．MCTSはそのVから算出されたQを活用しているので，自身の経験を生かした立ち回りができると言える)
        選択で最大を選択してるため，全てについて考えなくても良い

    学習フロー
    1. MCTSによる自己対局
        MCTSによって計算された打ち手の選択確率$\pi_t$から次の一手$a_t$が選択される
        盤面をMCTSに入力して最も良さそうな手を得る(その探索でDNNで評価していく)→打つ
        打った状態の盤面をMCTSに入力して最も良さそうな手を得る→打つ
        の繰り返し
        盤面$s_t$毎に「MCTSのDNN活用」が行われる
    1. DNNによる学習
        DNN出力$(\bm{p}_t,v_t)$をMCTS出力$(\bm{\pi}_t,z)$に近づけるように学習する
        →これまで学習してきたMCTSの出力を学び，DNNでパッと出せるようにしておく
        $\bm{p}$ : 次の手の確率分布
        v : 現在の盤面からプレイヤーが勝てる確率
        $\bm{\pi}$ : MCTSで計算した次の手の確率分布
        z : MCTSによる自己対局の勝敗

        DNNの学習＝損失関数$l$の最小化
        $$
          (\bm{p}, v) = f_{\theta}(s)\,and\,l \\= (z-v)^2 - \pi^{\textrm{T}}\log\bm{p} + c||\theta||^2
        $$
        ＝二乗誤差ークロスエントロピーーL2正則化項
    1. (評価)学習前DNN vs. 学習後DNN
        採択のされ方
        1. 新旧囲碁対決400戦
        1. 55％以上勝利したAlphaGoが次の学習フローに採択される
    1に戻る
