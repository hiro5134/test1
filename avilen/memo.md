<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });

</script>

# [My kicks](#mykicks)

## <a href="#index">Index</a><a id="index"></a>
* [Chapter 1](#cp1)
    * [Library](#lib)
    * [Higher API](#higherapi)
* [Chapter 2](#cp2)
    * [活性化関数](#mlp)
    * [万能近似定理 Universal Approximation Theorem](#uat)
    * [出力ユニットとタスク](#unit)
    * [損失関数](#loss)
* [Chapter 3](#cp3)
    * [バッチ処理](#batch)
    * [ミニバッチ処理](#minibatch)
    * [最適化手法](#adapt)
        * [最急降下法](#gd)
        * [確率的最急降下法](#sgd)
        * [モメンタム](#momentum)
        * [ネステロフのモメンタム](#momentum2)
        * [AdaGrad](#adagrad)
        * [RMSProp](#rmsp)
        * [Adam](#adam)
* [Chapter 4](#cp4)

## <a id = "cp1">Chapter 1</a>
1. ## <a id="lib">Library</a>
    | Lib | Graph | Company | Year | memo |
    | - | - | - | - | - |
    | TensorFlow | Define and Run | Google | 2015 | 計算グラフを自分で定義 |
    | Chainer | Define by Run | Preferred Networks | 2015 | 2019年開発終了 |
    | PyTorch | Define by Run | Facebook | 2016 | 計算方法がnumpyに似てる |
    | TensorFlow 2 | Define by Run | Google | 2019 | シンプルな実装可能 |
    | Apache MXNet | Define and Run <br> Define by Run| AWS | 2017 | 静的・動的どちらも可能|
    | Caffe2 | Define and Run | Facebook | 2017 | 2018年PyTorchへ統合 |
    | Congnitive Toolkit (CNTK) | Define and Run | Microsoft | 2016 | 音声認識特化 |
    | theano | Define and Run | モントリオール大学 | 2007 | 2017年開発終了 |
1. ## <a id="higherapi">Higher API</a>
    | API | Company | Year | memo |
    | - | - | - | - |
    | Keras |  Google | 2015 | TF用のAPI(TF2から標準API) |
    | Gluon | AWS, Microsoft | 2017 | MXNet用のAPI |

## <a id = "cp2">Chapter 2</a>
1. ## <a id="activation">活性化関数</a>
    中間層の表現を非線形にするため．  
    表現力の向上
    - step関数  
        勾配が0になり最適化が難しい
    - sigmoid関数
        0~1  
        勾配消失が起こり，中間層の最適化に不向き
    - softmax関数  
        複数の入力を扱える  
        出力の合計が1
        勾配消失が起こり，中間層の最適化に不向き
    - tanh関数  
        -1~1
        勾配消失が起こり，中間層の最適化に不向き
    - ReLU関数  
        x>0で勾配が計算可能で勾配が消失しない  
        x<0での勾配を計算するために以下の派生が存在  
        - Leaky ReLU  
            x<0に緩やかな傾き
        - Randomized ReLU  
            x<0の傾きをランダムに変更(予測時は平均値に固定)
        - Parametric ReLU  
            x<0の傾きを学習
    - MAXOUT関数  
        ReLU関数の一般化  
        要素数kのグループから各グループの最大値を抽出  
        ReLU関数や二次関数などに近似可能
        ReLu関数よりも表現力が高く勾配が消えない   
        活性化関数自体を学習

1. ## <a id="uat">万能近似定理 Universal Approximation Theorem</a>
    3層モデルの中間層のノードを極限まで増やせばあらゆる関数を近似可能  
    →現実的でない  
    →活性化関数により，層を横に伸ばすことで表現力をあげられる（計算コストも少なく収まる）  
    →万能近似定理+活性化関数でDLが飛躍

1. ## <a id="unit">出力ユニットとタスク</a>
    | Task |  | Unit | Activation Func. | Loss Func. | Probability | memo |
    | - | - | - | - | - | - | - |
    | 回帰 |  | 線形ユニット | 恒等関数 | MSE | ガウス分布 | 連続値を予測<br>目的変数（連続値）の分布をガウス分布と仮定<br>出力値は目的変数の分布（条件付きガウス分布）の平均値を返そうとする|
    | 分類 | 二値分類 | シグモイドユニット | sigmoid関数 | Binary Cross Entropy | ベルヌーイ分布 | ラベルが1である確率を予測<br>目的変数の分布は二項分布<br>ベルヌーイ分布を出力|
    | | 多値分類 | softmaxユニット | softmax関数 | Cross Entropy | | ラベルごとに確率を予測 |

1. ## <a id="loss">損失関数</a>
    尤度の最大化　損失関数の最小化　となるように設計

## <a id = "cp3">Chapter 3</a>
1. ## <a id="batch">バッチ処理</a>
    オンライン学習 入力１つ
    バッチ処理　入力nこ　あらゆる場所がn次元になる
        sum撮るときはdim=1
        基本サンプルが行，説明変数が列なので列方向に足していくことでサンプルごとのsumが取れる

1. ## <a id="minibatch">ミニバッチ処理</a>
    バッチ処理　データ数が膨大であれば計算量も膨大にありパラメータの更新が遅い
    →データを分割してバッチ処理をする
    | Task | 勾配推定 | 収束速度 |
    | - | - | - |
    | バッチサイズ大 | 正確 | 遅い |
    | バッチサイズ小 | 誤差を高める | 早い |
    　　
1. ## <a id="adapt">最適化手法</a>
    1. ## <a id="gd">最急降下法 Gradient descent</a>
        全体像はわからないが，その地点での最短経路方向に更新
        →求めた勾配方向とは逆向きに，その大きさ*η(学習率)を更新

        最急降下法ではlocal minimaに陥る可能性

        η小　時間がかかる　最適な場所を見つけられる可能性
        η大　早い　値がブレて最適な場所を見つけられない可能性
    1. ## <a id="sgd">SGD 確率的勾配降下法 stochastic gradient descent</a>
        確率的な山の一地点で最短経路方向に更新
        ミニバッチ抽出にランダム性を持たせ，損失関数が確率的になる

        最急降下法の弱点を克服
        （これまで）学習率を確率的に変化
        →損失関数が複雑すぎてうまくいかない

        そこでSDGはデータを確率的に変える
        →データをシャッフルして，それぞれの損失関数の挙動を変える
        →あるミニバッチでは上がり，他方では下がる，などができ最適な場所を見つけられる

        $$
          {\bm W} = {\bm W} - \eta\frac{\partial L}{\partial {\bm W}}
        $$

        一方で損失関数の形状が急峻な場合，振動してしまう

    1. ## <a id="momentum">モメンタム</a>
        移動平均を導入（慣性項）
        精度を保ちつつ，収束速度を早くする

        SDG＋慣性の法則→モメンタム

    1. ## <a id="momentum2">ネステロフのモメンタム</a>
        慣性項なしで更新した場合に慣性項をたしあわせ
        →ブレーキの役割

    1. ## <a id="adagrad">AdaGrad</a>
        過去の購買の二乗和を記憶して学習率を調整

        今までの勾配が大きい　学習率小さく更新
        今までの勾配が小さい　学習率大きく更新

        問題点
        学習率が0になる可能性
        →局所解

    1. ## <a id="rmsp">RMSProp</a>
        古い情報を忘れ，新しい情報を反映しやすくした

    1. ## <a id="adam">Adam</a>
        現在のスダンダード

        イテレーション数でバイアス補正
        →初期段階の不安定さを解消

        ${\bm m}$は速度の概念
        Momentum SDGにおける${\bm v}$
        減衰率$\beta_1$で過去の勾配情報${\bm m}$と現在の勾配情報$\frac{\partial L}{\partial {\bm W}}$を調整する
        $$
          {\bm m} = \beta_1{\bm m} + (1 - \beta_1)\frac{\partial L}{\partial {\bm W}}
        $$

        ${\bm v}$過去の勾配の二乗和
        $$
          {\bm v} = \beta_2{\bm v} + (1 - \beta_2)\left(\frac{\partial L}{\partial {\bm W}}\right)^2
        $$

        ${\bm m}, {\bm v}$は減衰率によって減衰させられた勾配の(二乗)和
        0.9が設定されると，勾配の１割しか学習に使われない
        下記で調整
        $$
          {\hat{\bm m}} = \frac{{\bm m}}{1-\beta_1^t},
          {\hat{\bm v}} = \frac{{\bm v}}{1-\beta_2^t}
        $$
        tはイテレーション数
        $\beta^t$は0に漸近，$\frac{1}{1-\beta^t}$は1に漸近
        ${\bm m, v}$に過去の情報が蓄積されるまでは勾配を利用
        更新が進むにつれ上式の影響は薄れる

        ${\hat{\bm m}},{\hat{\bm v}}$を用いてパラメータ${\bm W}$を更新
        $\epsilon$は微小項（0除算のため）
        $$
          {\bm W} = -\eta\frac{{\hat{\bm m}}}{\sqrt{{\hat{\bm v}}}+\epsilon}
        $$

        以上を総合して
        $$
          {\bm W} = -\eta\frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}
          \frac{{\bm m}}{\sqrt{{\bm v}}+\epsilon}
        $$

## <a id = "cp4">Chapter 4</a>
1. ## <a id="">勾配消失問題</a>
    backpropagationにて1より小さい値の掛け算により入力層に近い隠れ層に伝わる勾配がほぼ0になる

    activationに用いられるsigmoidやtanhの微分値は基本＜1であるため

    勾配が小さい→更新がほぼない

    ReLUにより解決の道
    微分値→常に1(x＞0)　→　勾配が消失しにくい
    計算が高速
1. ## <a id="">過学習</a>
    訓練データに過度に適合
    汎化性能が低い

    表現力の高さ→過学習しやすい

    正則化などで解決を図る
    1. ## <a id="">正則化</a>
        機械学習の目的：既知のデータから未知のデータを予測したい
        →既知データに対する損失関数の期待値を減少させる
        →予測値と期待値の差を最小化
        1. ## <a id="">バイアス-バリアンス分解</a>
            損失関数の期待値最小化にあたり，式要素を分解し各項から最小化を図る
            →回帰タスク（損失関数が二乗和誤差）のみ成り立つ

            バリアンス項
            →予測値の分散
            →大ならば過学習

            バイアス項
            →予測値の期待値と目的変数の期待値の差
            →大ならば未学習

            ノイズ項
            →データのノイズ
            →MLでのフィティングには無関係

            バリアンスとバイアスはトレードオフ
            基本的に学習されていることが望まれるため，バリアンスを抑えながら学習を図る
            →正則化
        1. ## <a id="">ノルムペナルティ</a>
            ノルム：ベクトルに対する距離を測る指標
            ハイパパラメータ
            $$
            ||x||_p = \left(|x_1|^p + |x_2|^p + \cdots + |x_D|^p\right)^{\frac{1}{p}}
            $$
            pの値によってL0,L1,L2,L$\infty$が存在

            パラメータが極端な値を取らないように損失関数に制限をかける

            ノルムつき損失関数＝損失関数＋$\lambda$(パラメータのLPノルム)

            ラムダ大　正則化強　過学習抑制　
            抑えすぎると精度が下がる

        1. ## <a id="">ラッソ回帰 Lasso Regression</a>
            L1ノルムによる正則化
            スパースな解を得やすい

            一般に
            通常損失関数とパラメータ制約領域の接点が最適解となりやすい

            ラッソ回帰
            領域が直線のため軸で接しやすい（それぞれのパラメータが0になることが多い）

            $$
              L_{norm}(y(X), t) = L(y(X), t) + \lambda(|w_0| + |w_1| + \cdots + |w_n|)
            $$

        1. ## <a id="">リッジ回帰 Lidge Regression</a>
            L2ノルムによる正則化
            汎化性能の高い解を得やすい

            リッジ回帰
            領域が円状なため，柔軟な値で最適解を取りやすい

            $$
              L_{norm}(y(X), t) = L(y(X), t) + \lambda\sqrt{w_0^2 + w_1^2 + \cdots + w_n^2}
            $$

        1. ## <a id="">elastic Net</a>
            ラッソ回帰とリッジ回帰の折衷案

            $$
              Lasso = \Omega(W) = (|w_0| + |w_1| + \cdots + |w_n|)
            $$
            $$
              Lidge = \Omega(W) = \sqrt{w_0^2 + w_1^2 + \cdots + w_n^2}
            $$
            $$
              elastic Net = \alpha\Omega(W) = \lambda(|w_0| + |w_1| + \cdots + |w_n|) + \frac{(1-\lambda)}{2}\sqrt{w_0^2 + w_1^2 + \cdots + w_n^2}
            $$

        1. ## <a id="">パラメータ拘束</a>
            類似したタスクのモデルのパラメータにノルムペナルティを課す
            モデルA(パラメータ$W^{(A)}$)とモデルB(パラメータ$W^{(B)}$)は類似タスク
            これらの距離を近くする
            下記項を最小化する
            $$
              \lambda\Omega(W^{(A)}), W^{(B)})=\lambda\left( |w_1^{(A)}-w_1^{(B)}|^2 + \cdots + |w_D^{(A)}-w_D^{(B)}|^2\right)^{\frac{1}{2}}
            $$

        1. ## <a id="">パラメータ共有</a>
            モデルの一部のパラメータを同じ値にする
            どこを同じにするかは開発者による
            例えば，MLPのノード間のパラメータを左右対象にする

            パラメータ共有・拘束においても，過剰な表現力に制限をかけるイメージ

        1. ## <a id="">早期終了 Early stopping</a>
            最適化への道を途中で打ち切り，パラメータ制約を行う効果がある

        1. ## <a id="">アンサンブル学習</a>
            複数のモデルを組み合わせて汎化性能向上

            | Name | 特徴 | 正則化効果 | 備考 |
            | - | - | - | - |
            | バギング | 同時多数学習モデル | varianceを下げる | 多数決モデル<br>訓練データから多数のデータセットを作りそれぞれのモデルで学習 |
            | ブースティング | 段階的改善モデル | biasを下げる | 徐々にupdate<br>前のモデルの誤った予測を重みづけ学習 |

        1. ## <a id="">ノイズによる正則化</a>
            ノイズ：ある現象の不確実性から生じるデータや情報

            ノイズを加える位置によって手法が異なる

            入力層：data augmentation
            隠れ層：dropout, dropconect
            目的変数：label smoothing
            1. ## <a id="">Data Augmentation</a>
                学習データに多様性を持たせ，汎化性能を向上させる
                回転拡大縮小切り抜きなど
            1. ## <a id="">Dropout</a>
                訓練時
                指定された割合のノードをランダムに非活性
                部分的なネットワークのアンサンブル学習
                (in PyTorch 指定した確率pに基づくベルヌーイ分布からのサンプルを持ちに消すノードを選択)

                評価時
                全てのノードを使用
                出力時には1-pを乗算（確率pで消すノードを選択しているため）

                実用性が高い
                学習パラメータが削減されるため計算コスト小さい
                ほぼ全てのモデルに適用可
                訓練データが少ないとうまくいかない傾向
            1. ## <a id="">Dropconnect</a>
                指定された割合のパラメータ（ブランチ　線）ランダムに非活性

                dropoutより優れた性能
                乱数のシードに大きく左右され再現性が困難
            1. ## <a id="">Label Smoothing ラベル平滑化</a>
                目的変数にノイズを加える

                分類タスク(2値分類)
                人のラベル付けに誤りがる可能性
                特定のカテゴリに過剰に適合しやすい（過学習しやすい）

                に対する効果
                多少のラベル付ミスを吸収
                正解カテゴリ以外にも値が入ることで，特定のカテゴリに過剰に適合することが減る

                $$
                  \{0, 0, 1, 0\}
                $$
                に対して各ラベルにノイズを付加(ノイズ*$\alpha$)
                $$
                  \{\alpha\xi_1, \alpha\xi_2, (1-\alpha) + \alpha\xi_3, \alpha\xi_4\}
                $$

    1. ## <a id="">バッチ正規化</a>
        正規化
        平均0, 分散1にスケーリングする
        効果
        スケールの異なる特徴量を同様な基準に扱える
        学習データの分布と検証データの分部生合成を維
        スケールの大きな特徴量に左右されない

        バッチ正気化
        バッチごとに正規化
        効果
        - 学習速度の向上
            - 共変量シフトの減少
            - 大きな学習率でも安定
        - 正則化効果がある
        - NNの重みの初期値に依存しづらい

        (内部)共変量シフト
        学習が難しくなる
        - 各層の入力時のbんぷが学習の過程で変化する現象
            1. パラメータは学習毎に更新
            1. 各層出力分布は学習毎に異なる
            1. 故に各層入力分布が学習毎に異なる→共変量シフトが起こる
        - 共変量シフトで学習が難しくなる理由
            - 学習率を下げる必要．（学習時間が長くなる）
            - パラメータの初期値依存性が高くなる（初期化に注意)

    1. ## <a id="">計算コスト</a>
        1. ## <a id="">高速化</a>
            GPUで並列処理高速化

            - 分散処理
                - データ並列
                    - 同期型
                        全ての並列処理で同じ勾配を用いる
                        勾配を常に一定に保つ
                    - 非同期型
                        アプローチ毎に用いる勾配が異なる
                        勾配を常に更新する
                - モデル並列
        1. ## <a id="">軽量化</a>
            1. ## <a id="">蒸留</a>
                学習済教師モデルの入出力を，軽量な生徒モデルで学習
                soft, hard target lossを最小化
                精度は落ちる反面，軽量なモデルで計算資源を大幅に削減

                soft target loss
                教師モデルと生徒モデルの出力の損失
                hard target loss
                生徒モデルの入出力分布の損失

                知識の蒸留
                精度の高い教師モデルは，粒度の細かい重要な知識を反映できる

                例）猫・犬・ハムスター・へびに分類
                {1, 0, 0, 0}に対して{0.85, 0.1, 0.04, 0.01}を教師モデルが出力
                猫には耳があり，丸い瞳
                犬には多少にているが，蛇には程遠い．→粒度の細かい知識

            1. ## <a id="">枝刈 Pruning</a>
                寄与の小さい重みを0にする（データの流れを切る）
                （値の小さい重みを0にする）

                学習の繰り返しの中で枝刈を実行し，パラメータ削減→精度を保ちつつモデル軽量化

            1. ## <a id="">量子化</a>
                浮動小数点で表現されるパラメータを低ビットで表現（近似）
                精度を落とさずモデル圧縮

                32bitを上位16bitで表現　など

      https://www.anarchive-beta.com/entry/2020/08/16/180000

      https://qiita.com/hikobotch/items/78b53de44069fb19d311

## <a id = "cp5">Chapter 5</a>
1. ## <a id="">画像認識</a>
    - 画像分類
    - 物体検出
    - セグメンテーション

1. ## <a id="">CNN</a>
    1. ## <a id="">畳み込み</a>
        複数チャネル
            カーネルを入力の次元分用意（カラー画像なら3カーネル）
            出力は各チャネルの総和

        複数カーネル
            前段のカーネルをn個用意
            出力が1次元増える

        ミニバッチ処理
            全段の出力がミニバッチ回数分増えそう，出力次元が1次元増える

        - メリット
            - 疎結合
                画像データ特有の特徴を考慮
                特に，離れた陽うくセル同士の関係を無視
            - パラメータ共有
                同じ重みのカーネルを使い回し，メモリ使用量を抑制
        - デメリット
            - データ加工技術は含まれない
                スケーリングや回転といった画像変換はできない
                回転された画像を別途用意

    1. ## <a id="">ハイパーパラメータ</a>
        1. ## <a id="">パディング</a>
            $$
              out = (H + padding*2 - Filter)/stride + 1
            $$
            - メリット
                - 端のデータの抽出
                    パディングがないと，恥の畳み込み回数が少なくなる→重要度が下がる
                - データサイズを調整可

    1. ## <a id="">プーリング</a>
        情報抽出

        学習パラメータなし
            最大値や平均値をとるだけ（マックスプーリングが主，特徴を取りやすいため）
        チャネル数が変化しない
            チャネル毎に独立な計算
        微小な一変化にロバスト

    1. ## <a id="">im2col</a>
        入力データのカーネル適用箇所をバッチサイズ方向に切り出し，それらを行方向に並べる

        カーネルを行方向に展開し，各カーネルを列方向に並べる

        内積を取り，整形して出力データの形にする

        行列計算に帰着（大きなマトリクスの計算を高速に）

    1. ## <a id="">画像における正規化</a>

        1. ## <a id="">batch normalization</a>
            これによりDLの精度が爆発的に向上

            パラメータの勾配計算は，他のパラメータに影響が出ない前提
            偏微分をし，損失を減らす方向へ計算している

            データは適度な広がり・多様性がある方がいい（ガウス分布に沿うような）
            →さまざまなパターンを学習できるため
            →偏ったデータでは勾配が消失する可能性

            ネットワークが深いと非線形変換が何度も行われ，各層の入力データの分布が大きく変わる（内部の共変量シフト）
            →学習が進まない
            →パラメータ初期値に注意を払う必要性

            よって，各ノードの値をミニバッチ単位で正規化（パラメータのスケールを揃える）

            線形変換-非線形変換の間で行われることが多い
            →FC - BN - ReLU - FC - BV - ReLU のように

            パラメータ初期値に払う注意が小さくなる
            正則化の効果も持つ（L2正則化やDropoutの必要性がsちいさくなる）
            バッチサイズが小さすぎると機能しにくい

            スケーリング・シフトを行う
            →必ず平均0分散1になるよりは，学習にあった分布にしたい
            →$\gamma, \beta$の学習によって決まる項を組み合わせ最適な分布にする

            テスト時
            全訓練データ（フルバッチ）の平均・分散で正規化
            →テストデータでやると，１つの予測だけの際に使えない
            ミニバッチの平均・分散と移動平均でフルバッチの平均・分散を推定
            $$
              \mu_{new} = m\mu_{old} + (1 - m)\mu
            $$

        1. ## <a id="">layer normalization</a>
            同じ層のニューロン間で正規化
            一つのサンプルに対して全てのチャネルにまたがって行う
            バッチ正規化と異なり，トレーニング・テストで同じ計算を実行
            オンライン学習やRNNに拡張（1サンプルに対して行うため）

        1. ## <a id="">instance normalization</a>
            各チャネルで独立に画像に対して行う
            画像生成の分野でバッチ正規化の代替として注目
            画像以外の分野へは拡張しづらい
            バッチサイズが十分であれば，バッチ正規化のみで十分である

        1. ## <a id="">group normalization</a>
            チャネルをG個にグルーピングし，layer, instance normalizationの中間的な処理を行う
            物体検出やセグメンテーションで効果を発揮

## <a id = "cp6">Chapter 6</a>
1. ## <a id="">RNN</a>
    回帰型NN

    時系列データの処理
    →循環構造
    →出力を入力に戻す
    →BPができない
    →過去の中間層の情報を次の入力に加える（ループの展開）
    →流れが一方通行になったので（循環でない）BPができる
    →BPTT(Back Propagation Through Time)

    1. ## <a id="">順伝播</a>
        $t$番目の入力と$t-1$層の出力が$t$層の入力となり，それぞれに重みとバイアスを計算し$t+1$層の入力へとつなげる．
        予測値算出・損失計算ではこの出力を活性化関数に通し，ラベルと比較する
        $$
          h_{next} = Act(x_t W_x + h_{pre} W_h + b)\\
          {\hat y_t} = Act(h_{next} W_o + c)
        $$

    1. ## <a id="">BPTTと教師強制</a>
        BPTT
        時間軸方向の逆伝播

        BPTTの課題
        - 並列処理が不可能
        - 全時刻の中間状態を保存→メモリコストが高い

        教師強制
        前層の中間層の代わりに前層の正解ラベルを用いる
        →計算コスト大幅減
        →並列処理可能（時系列的関係を切り離して学習可能）
        - 参照する前層の情報
            - 通常のRNN：前時刻のRNNユニットの状態（訓練・テスト時）
            - 教師強制
                - 訓練時：前時刻の正解ラベル
                - テスト時：前時刻の出力層の状態

        Truncated BPTT
        通常BPTTの全中間層保存によるメモリコストを改善
        FPの接続は切らず，BPの接続を切る
        →ユニット単位でのBPを実行できる

    1. ## <a id="">深層回帰</a>
        RNNで回帰を深くし，予測能力を高めたい

        - 階層別に回帰
            RNNの状態を複数の層に分解
            各層で回帰的な結合を持たせる
            →下部の層が入力データをより適切な隠れ層の状態変換できる
        - 中間層でより深い接続
            RNNを3つの演算ブロックに切り分け，各ブロックで深い計算を持たせる（MLPなど）
            - 入力 - 隠れ層
            - 隠れ層 - 隠れ層
            - 隠れ層 - 出力層
        - スキップ接続
            3ブロックで深層化することで最適化が困難に
            →前層よりも前の状態と接続し，パラメータ数を抑える
            →データの最短経路が短くなり最適化しやすくなる

1. ## <a id="">再帰型RNN (Tree-RNN)</a>
    回帰結合型NNのもう一つの一般形
    （最近の使用例は少ない）

    子の表現特徴ベクトルを用いて親の表現特徴ベクトルを計算

    応用例
    - 自然言語処理
    - プログラミング言語の意味解析

    - 回帰型NN
        - RNN (Recurrent Neural Network)
            入力を前（あるいは後）から順番に受け取り，表現ベクトルを構成
    - 再帰型NN
        - Tree-RNN
            入力を木構造に沿って処理し，表現ベクトルを構成

    再帰型RNNもRNNと記載されることもあるので内容に注意

1. ## <a id="">長期依存性の解消　LSTM</a>
    長期依存性
    言語処理などにおいて，予測位置から遠い入力が予測に影響することがある
    →通常RNNでは位置が遠ければ依存性も低くなる
    →長期的な依存関係を学習するために「勾配爆発・勾配消失」の解決が必要

    - 勾配爆発
        各時刻でのBPの度に$W_f (if\,\,>1)$が乗されると，勾配が指数関数的に大きくなる
        - 勾配クリッピング
            BPにて勾配の上限を設定
            勾配ベクトル$\Delta\omega^{(t)}$のノルム(L2が一般的)が上限値$g$を超えた時，下式でパラメータ更新の大きさを調整
            $$
              \Delta\omega^{(t)} = \frac{g}{||\Delta\omega^{(t)}||}\Delta\omega^{(t)}
            $$

    - 勾配消失
        各時刻でのBPの度に活性化関数の微分が乗されるので，勾配が指数関数的に小さくなる
        （ReLUを除く）
        - スキップ接続・Leaky接続
            - スキップ接続(skip connection)
                隠れ層の状態をより先の層へ伝える
                →粗い時間スケールで動作し，遠い過去から現在までの情報伝達を効率化
                →これまで捉えられなかった長期依存性に希望がさす
                →通常の接続とスキップ接続両者が存在するため，勾配爆発の可能性は残る
            - Leaky接続
                前時刻からの入力を$\alpha$倍，入力層からの接続を$1-\alpha$倍して接続
                $\alpha$が1に近ければ，過去の記憶を長期間記憶
                $\alpha$が0に近ければ，過去の記憶は急速に破棄される
                $\alpha$は初期値で
                何らかの分布からサンプリングし固定
                適当な値として学習
                のいずれか
                Leakyユニットで異なる時間スケールを持つことは，長期依存性を扱う上で役に立つ
        - ゲート付きRNN
            記憶すべき情報を記憶しきれない
            →メモ（記憶セル）を追加
            →ユニットに入力層，前の隠れ層，前の記憶セルを入力
            →次の隠れ層，記憶セル(CEC)を出力
            （記憶セルは活性化関数を通さないので，勾配消失を防ぐことができる）
            →LSTM
            - 忘却ゲート forget gate
                CECから不要な記憶を忘却
                $$
                  f = \sigma\left( x_t W_x^{(f)} + h_{pre} W_h^{(f)} + b^{(f)}\right)\\
                  c_{pre} = f\odot c_{pre}
                $$
                $\sigma$はsigmoid関数（0~1）で$c_{pre}$の情報を削ぎ落とす
            - インプットゲート input gate
                情報を取捨選択し，CECにメモ
                $$
                  g = \textrm{tanh}\left( x_t W_x^{(g)} + h_{pre} W_h^{(g)} + b^{(g)}\right)\\
                  i = \sigma\left( x_t W_x^{(i)} + h_{pre} W_h^{(i)} + b^{(i)}\right)\\
                  c_{pre} += g\odot i
                $$
                $g$：記憶する情報
                $i$：どれだけメモするかの割合
                もともとあった情報に足すだけ
            - アウトプットゲート output gate
                CECの情報のうち，次の隠れそうにどの程度流すか調整
                $$
                  o = \sigma\left( x_t W_x^{(o)} + h_{pre} W_h^{(o)} + b^{(o)}\right)\\
                  h_{next} = o\odot \textrm{tanh}(c_{pre})
                $$
                $o$は0~1なのでCECをどれだけ次の層に渡すか調整される
            - 補足　ピープホール付きLSTM
                各ゲートでCECを覗き見るLSTM
        - GRU
            LSTMの表現力を保ちつつ，計算コストを低減
            - 残った機能
                隠れ状態に忘却機能と記憶機能を付与
                →勾配消失が起きづらい
            - 失った機能
                忘却・記憶機能がトレードオフ
                CECがなく，メモができない
                →過去の情報が残りづらい

            - reset gate
                過去の隠れ状態をどれだけ反映させるか
                $$
                  r = \sigma\left( x_t W_x^{(r)} + h_{pre} W_h^{(r)} + b^{(r)}\right)\\
                  h_{reset} = r\odot h_{pre}
                $$
                LSTMの忘却でゲートと同様の動き
            - update gate
                $$
                  \tilde{h} = \textrm{tanh}\left( x_t W_x + h_{reset} W_h + b\right)\\
                  z = \sigma\left( x_t W_x^{(z)} + h_{pre} W_h^{(z)} + b^{(z)}\right)\\
                  h_{next} = (1-z)\odot h_{pre} + z\odot \tilde{h}
                $$
                $\textrm{tanh}$：記憶する情報
                $\sigma$：記憶する割合
                $z$：$\sigma$
                $(1-z)\odot h_{pre}$：forget機能
                $z\odot \tilde{h}$：input機能

                forget, inputがトレードオフ

    - RNN系ユニットまとめ
        - simple RNN
            時系列データを用いる
            過去の情報を循環
        - LSTMユニット
            長期依存性（特に勾配消失）の解消
            CECによるメモを導入
        - GRUユニット
            LSTMの計算コスト低減

1. ## <a id="">双方向RNN</a>
    順方向RNN，逆方向RNNの結果をマージ
    - メリット
        後ろからの文脈情報を得られる
        →RNNよりも精度が向上することも

    出力と隠れ層の次元が単方向と比べ倍になる
    →順・逆方向の結果それぞれを保持するため

1. ## <a id="">RNN アーキテクチャ</a>
    1. ## <a id="">双方向伝播</a>
        双方向RNNは，simple RNNユニットを二つ重ねて
        互いに逆方向の伝播をさせる
        →ユニットを取り替え，双方向LSTMも可能

    1. ## <a id="">ユニット数を増やす</a>
        どのユニットもユニット数を増やしてNNを深くすることが可能
        縦横に伸ばせる
        →計算コスト高くなる

## <a id = "cp7">Chapter 7</a>
1. ## <a id="">生成モデル</a>
    1. ## <a id="">潜在変数</a>
        潜在的な特徴を表す変数

        次元圧縮後の状態
        e.g.) 説明変数（月間平均気温・湿度）→潜在変数（季節）

    1. ## <a id="">Encoder-Decoder modele</a>
        [データ] - [Encoder] - [潜在変数] - [Decoder] - [生成データ]

        1. ## <a id="">Auto Encoder</a>
            入力と同じ物を出力させる
            →中間層の次元圧縮された情報が重要

            応用例
            - 画像のノイズ除去(Denoising AE)
            - クラスタリング
                Encoderで圧縮した特徴分布上でクラスタリング
                →より重要な特徴をもとにクラスタリングできる
        1. ## <a id="">VAE Variational Auto Encoder</a>
            - AE
                存在するデータを忠実に再現したい
                →潜在変数にあそびがない
            - VAE
                実在しないデータを生成したい
                →潜在変数にランダム性や連続性を持たせ，中間の状態などを作り出せる

            潜在変数が正規分布に従うように調整

            - 学習時
                [Encoder] - [$\mu, \sigma^2$] - [潜在変数]
                この過程で潜在変数の平均$\mu$と分散$\sigma^2$を出力
                (AEでは直接潜在変数を求めていた)

                出力された$\mu, \sigma^2$から正規分布${\mathcal N}(\mu, \sigma^2)$に従う潜在変数をサンプリング

                後の流れは同じ

                BPではランダムにサンプリング($z \verb|~| N(\mu, \sigma^2)$)している点を
                ガウシアンノイズ$\epsilon\verb|~|{\mathcal N}(0,1)$に代替
                FPでの$\epsilon$を記憶すればBP可能
                $z = \mu + \sigma^2\times\epsilon$

            - 損失関数
                $$
                  {\mathcal L} = D_{KL}[q(z|{bm X})||p(z)]
                   - {\mathbb E}_{q(z|{\bm X})}[\textrm{log}\,p({\bm X}|z)]\\
                   = D_{KL}[{\mathcal N}(\mu({\bm X}),\Sigma({\bm X}))||{\mathcal N}(0,1)] + \beta||{\bm Y} - {\bm X}||^2
                $$
                第一項：Encoderが求めた分布と${\mathcal N}(0,1)$との近さ
                第二項：入力データと出力データの近さ(MSE)

                AEの損失関数
                $$
                  {\mathcal L} = \beta||{\bm Y} - {\bm X}||^2
                $$

            - 実用と課題
                - 実装が容易，理論が整っている
                - 入力が画像の場合，VAEのサンプルはややぼやける傾向にあり，原因は不明
                    損失を小さくするために，出力が0,1ではっきりしたものよりも小数の値を持つことで画像がぼやける傾向にある
        1. ## <a id="">GAN Generated Adversarial Network</a>
            VAEの課題
            - 画像がぼやける
                1. ガウス分布による正規化のために，データに制約がかかり出力にノイズが載ったのか
                1. 鮮明な画像(0,1)よりもぼやけた画像(小数)の方が損失(MSE)が下がるのか

            鮮明な画像のために，ノイズの除去・損失関数を変える，が必要．

            GAN
            GeneratorがDiscriminatorを騙すように画像を生成
            [ノイズ z] - [Generator G(z)] - [偽データ ${\hat x}$ + 真データ x] - [Discriminator D(x)] - [真偽(0,1)]

            生成時
            [ノイズ z] - [Generator G(z)] - [偽データ ${\hat x}$]
            一様乱数からノイズzをサンプリング
            Generatorで新たなデータを生成する

            - 目的関数
                $$
                  \underset{G}{\textrm{min}}\,\underset{D}{\textrm{max}}\,V(D,G) =
                  \underset{G}{\textrm{min}}\,\underset{D}{\textrm{max}}\,
                  {\mathbb E}_{x\verb|~|p_{data}(x)}\left[\textrm{log}D(x)\right] +
                  {\mathbb E}_{z\verb|~|p_{z}(z)}\left[\textrm{log}(1-D(G(z)))\right]
                $$
                $G(z)$：ノイズzからGが生成したデータ
                $D(x)$：訓練データxが実在するデータである確率
                - DはVを最大化
                    - first term
                        訓練データ分布$p_{data}(x)$から得たデータxが「訓練データである」と判別する確率$D(x)$の最大化
                    - second term
                        生成データ分布$p_z(x)$から得たデータ$G(x)$が「生成データである」と判別する確率$(1 - D(G(z)))$の最大化
                - GはVを最小化
                    - first term
                        関係なし(Dが訓練データをどう判別するかは関係ない)
                    - second term
                        生成データ分布$p_z(x)$から得たデータ$G(x)$が「生成データである」とDに判別させる確率$(1 - D(G(z)))$の最小化

                - 最適化手法はSGD(確率的勾配降下法)
                    m : ミニバッチサイズ
                    1-1. m個のノイズzとデータxをサンプリング
                    1-2. SGDでDiscriminator更新
                        $$
                          \nabla_{\theta_d}\frac{1}{m}\sum_{i=1}^{m}
                          \left[\textrm{log}D({\bm x}^{(i)}) +
                          \textrm{log}\left(1 - D\left(G({\bm z}^{(i)})\right)\right)\right]
                        $$
                    2-1. mこのノイズzをサンプリング
                    2-2. SGDでGenerator更新
                        $$
                          \nabla_{\theta_g}\frac{1}{m}\sum_{i=1}^{m}
                          \left[\textrm{log}\left(1 - D\left(G({\bm z}^{(i)})\right)\right)\right]
                        $$
                    これらを全てのミニバッチに対して繰り返す

                - 学習が困難
                    - D-Gが拮抗するはずが，Dが圧勝し勾配が消失する
                        →Dを小さいネットワークにする
                        →DのDropout rateを大きめにする
                        →Unrolled GANを用いる
                    - mode collapse
                        →Minibatch Discriminator
                        →Wasserstein GAN

                    - Unrolled GAN
                        DをKステップ学習させた後に，Gを学習させる
                        →GがKステップ分先取り学習できる(良い勾配情報で学習できる)
                        →学習のバランスをとる(GよりDの方が早く学習してしまう問題を解決)

                        効果：Gに「質の良い勾配情報(hint)」を与えることになり，学習のバランスが取りやすくなる

                    - mode collapse(モード崩壊)
                        Gの能力不足により，全体の分布を近似しきれなく，ある一つのデータ(最頻値=mode)だけを出力しようとしている状態

                        対策
                        - Minibatch Discriminator
                            Dにmode collapseが起きているかのヒントを与える処理
                            →同バッチ内の画像間類似度を計算する
                            →mode collapseが起きている場合，同じ様なデータが多くなる=類似度が高くなる
                            →fakeデータと判別しやすくなる
                        - Wasserstein GAN
                            GANとは異なる距離を用いる

                            Gは生成データの分布を訓練データの分布に近づけようとする

                            - 通常GAN
                                JSD(イェンセンシャノンダイバーシティ)
                                →分布の裾が狭い時，全く異なる形状の分布同士でもJSDが低くなる問題
                            - Wasserstein GAN
                                EMD
                                →分布(砂山)を別の分布に移動する際，「どのくらいの量の砂を運ばなければいけないか」を数値化した指標
                                →EMDであれば異なる形状の分布間の距離は大きいと判断してくれる

## <a id = "cp8">Chapter 8</a>
1. ## <a id="">強化学習</a>
    最適な意思決定のルールを求める
    →教師ありでもなしでもない
    明確な正解は存在していないが，大きな目標や行動はある
    1. ## <a id="">用語</a>
        - エージェント
            環境で動く主体
        - 環境
            エージェントが動く範囲や状態
        - 状態
            行動をした後の環境の評価
        - 行動
            次に何をするか
        - 状態遷移確率
            次の行動で状態がどう変わるかの確率分布
            →ほとんどの場合で正確にわからない
        - 報酬
            行動を起こした際に得られる価値
    1. ## <a id="">マルコフ決定過程 MDP</a>
        従来の「状態」のみの確率過程(マルコフ過程)とは異なり，
        「行動」や「報酬」などを追加したものが「マルコフ決定過程」
        →行動選択ルールの最適化のために「行動」や「報酬」が必要

        $s_t$ : 時間ステップtの状態
        $a_t$ : 時間ステップtの行動
        $p(s_{t+1}|s_t,a_t)$ : 状態$s_t$で$a_t$を選択し，状態$s_{t+1}$になる状態遷移確率
        →行動をしてそのままの状態かもしれない．わからないので確率で吸収して表現．
        $r_t$ : 実行した$a_t$において得られる(即時)報酬

        $$
          p(s_{t+1},s_t,a_t) = p(s_{t+1}|s_t,a_t)p(s_t|a_t)p(a_t)\\
          (p(s_t,a_t) = p(s_t|a_t)p(a_t))
        $$
        $p(a,b)$ : aとbの同時確率
        $p(a|b)$ : bの下でaが起こる確率

        1. 時間ステップt=0に初期化．初期状態確率$p_{s_0}$に従い初期状態$s_t$を観測
        1. 状態$s_t$に対して行動$a_t$を選択
        1. 行動$a_t$を実行．得られた報酬$r_t$と状態遷移確率$p(\cdot|s_t,a_t)$により定まる，次の状態$s_{t+1}$を観測
        1. 時間ステップtを一つ進め，手順2に戻る

        - マルコフ性
            次の状態が，現在の状態のみによって決定する性質
            →$s_{t+1}$の決定にあたり，$t=t-1$以前の状態に依存しない．$s_t$のみによって決まる
            $$
              p(s_{t+1}|s_1,\cdots,s_{t-1},s_t) = p(s_{t+1}|s_t)
            $$

    1. ## <a id="">方策</a>
        どの様にして行動を選択するか
        →方策

        最適な方策$\pi$を探索することが強化学習の目的

        [状態 $s_t$] - <方策 $\pi(a_t|s_t)$> - [行動 $a_t$] - <状態遷移確率 p(s_{t+1|s_t,a_t})> - [状態 $s_{t+1}$] - [報酬 $r_t$]

    1. ## <a id="">報酬</a>
        ある状態でとった行動が学習に寄与した価値

        報酬関数は設計者が与えるもの
        $$
          r_t = g(s_t,a_t,s_{t+1})
        $$
        →設計が困難

        加えて，将来的な報酬を考慮する必要性
        →直近では学習に価値をもたらせないが，将来的に価値をもたらす行動を評価したい
        →一方で，将来が遠いほど不正確な評価になる
        →割引率$\gamma$を導入し報酬に反映

        割引累積報酬$R_t$
        $$
          R_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots
          = \underset{K\to \infty}{\textrm{lim}}\sum_{k=0}^{K}\gamma^k r_{t+k}
         $$


    1. ## <a id="">目的関数(価値関数)</a>
        報酬の期待値を目的関数とし，エージェントの行動を表現
        →累積報酬和$R_t$を状態$s_t$の価値とし，「価値の高い状態に遷移する」という方策が作れそう

        方策$\pi$の下での状態$s_t$の価値関数
        →ある方策$\pi$での状態$s$が初期状態$s_0$だった時の累積報酬和$R_0$の期待値が右辺
        $$
          V_{\pi}(s) = \mathbb{E}_\pi \left[R_0|s_0=s\right]
        $$
        ($\mathbb{E}_\pi[X|Y]$は，条件$Y$が与えられた確率変数$X$の$\pi$についての期待値)

        1. ## <a id="">最適な目的関数</a>
            最適な価値関数$V^*(s)$は，ある方策における累積報酬和の期待値の最大値
            $$
              V^*(s) = \underset{\pi}{\textrm{max}}\mathbb{E}_\pi \left[R_0|s_0=s\right]\\
              \cdots\\
              = \underset{\pi_0}{\textrm{max}}\sum_{a_0}\pi_0(a_0|s)
              \left[
              g(s,a_0,s_1) + \gamma\sum_{s_1}p(s_1|s,a_0)V^*(s_1)
              \right]
            $$
            ($\Sigma$ありうる次の状態への遷移確率$\times$その最適価値)

            1. ある状態の最適価値関数$V^*(s)$は，次の状態の最適価値関数$V^*(s^{\prime})$によってのみ再帰的に表現
            1. 最適な行動を選択できること$(\pi_0(a|s)=1)$＝最適な方策

            よって最適な方策$\pi=1$についてのみ考えればよい
            →それ以外の方策は最適でない$=0$
            $$
              V^*(s) = = \underset{a}{\textrm{max}}
              \left[
              g(s,a,s^{\prime}) + \gamma\sum_{s^{\prime}}p(s^{\prime}|s,a)V^*(s^{\prime})
              \right]
            $$
            →最適ベルマン方程式

            「価値を最大化する行動をとる」
            ＝
            「価値を最大化する最適な方策を見つける」


        1. ## <a id="">2種類のアプローチ</a>
            - 価値関数ベース
                価値を最大化するような「行動」をする
                価値(=モデル)を基準に，行動の選択は別のアルゴリズムで決定
                価値を正しく見積もることが重要
            - 方策勾配ベース
                価値を最大化する「最適な方策」を見つける
                行動の選択(方策(=モデル))そのものをモデルで決定する

            1. ## <a id="">価値関数ベース</a>
                価値を正確に見積もる
                どのような行動をとるかよりも，その時の状態を目的(勝利など)により近づける
                1. 現在の状態の価値を推測
                1. 一手先の状態を全て列挙．最も価値の高い状態を選択
                1. AI同士で対戦した結果から，状態価値を学習．
                    探索による先読みと組み合わせ，性能の高い方策を実現
            1. ## <a id="">方策勾配ベース</a>
                そもそもの価値関数導入の背景
                累積報酬和$R_t$を状態$s_t$の価値とし，「価値の高い状態へ行く」方策を考えるため

                方策$\pi$の下での状態$s_t$の価値関数
                $$
                  V_{\pi}(s) = \mathbb{E}_{\pi}[R_0|s_0=s]
                $$

                $V_{\pi}(s)$を全て調べることで，高い$V_{\pi}(s)$を持つ状態をたどるルートがわかる
                ＝
                最適な方策が間接的にわかる

                行動価値関数$Q_{\pi}(s,a)$の導入
                $$
                  Q_{\pi}(s,a) = \mathbb{E}_{\pi}[R_0|s_0=s, a_0=a]
                $$
                ある状態における，ある行動をとった時の価値
                →価値関数の一部(価値関数における，ある行動$a$をとった先の価値)
                →行動を一つ一つに分けて考えていく必要があるため，導入

                価値関数：状態$s$であらゆる行動をした時の報酬を全て計算
                行動価値関数：状態$s$である行動$a$をした時の報酬を全て計算

        1. ## <a id="">価値関数ベースのアプローチ</a>
            - 価値反復法
                環境ダイナミクス(状態遷移確率$p$と報酬$r$)が既知の問題で有効
                →最後の状態から最適ベルマン方程式に近づけていく
                e.g.) 迷路には適する．自動運転は不適．
            - 報酬のサンプリング
                未知の環境ダイナミクスを実際に行動をしてサンプルを集める
                →行動をした結果こうなった→遷移確率はこうだろう
                - モンテカルロ法
                    エピソディックに学習する(有限の時間ステップで終了する場合(エピソディック)に有効)
                    →何度も最後までプレイし，報酬をサンプリング
                    $$
                      V(s_t) \leftarrow V(s_t) + \alpha[\underline{R_t} - V(s_t)]\\
                      Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha[\underline{R_t} - Q(s_t,a_t)]
                    $$
                    とりあえずの累積報酬和$R_t$がわかるので，価値関数を仮の$R_t$に近づけていく($\alpha$が1に近い時)⇄価値関数に近づけていく($\alpha$が0に近い時)
                    その時の$R_t$が正しいかもわからないので，$\alpha$を乗じて補正をかける
                    e.g.) 五目並べは適合．テトリスは不適．(1エピソードが長い場合も不適)
                - TD法 Temporal Difference Learning
                    1ステップごとにサンプリング
                    →終了を待たずに，それまでに得た宝珠から価値関数を更新
                    $$
                      V(s_t) \leftarrow V(s_t) + \alpha[\underline{r_{t+1} + \gamma V(s_{t+1})} - V(s_t)]
                    $$
                    下線部がモンテカルロ法と異なる
                    →最後までの状態がわからないため，即時報酬$r_{t+1}$と補正した価値関数$\gamma V(s_{t+1})$で評価

                    - Q関数
                        方策オフ型：実際に進む行動(方策)と価値関数更新に用いる行動(方策)が異なる(こっちの方が直感的)
                        価値関数$V$に代わり，行動価値関数$Q$を用いて行動を選択
                        $$
                          Q_{\pi}(s,a) = \mathbb{E}_{\pi}[R_0|s_0=s,a_0=a]
                        $$
                        行動価値関数$Q$を用いて，未来の状態を計算しなくても最適な行動ができる
                        →学習が安定する(Q関数の収束が早い)
                    - SARSA
                        方策オン型：実際の方策($\epsilon$-greedy)に従って価値を更新
                        （エピソード更新と価値関数更新お方策が同じ）

                        Q学習では予測がある程度正確でないとmax Qが適切に定まらない
                        →五目並べで最初に盤面全てを予測できないのと同じ
                        →$\epsilon$-greedy方策に従って行動価値を学ぶ

                        SARSAはmax Qではなく，現時点での方策に従って遷移する$Q(s_{t+1},a_{t+1})$を更新に使用する
                        →その時点での最も良い選択は探索せず，方策にしたがて価値を更新する
                        →価値関数の更新にランダム性を取り込んでいるので局所界に陥りにくい
            - サンプリングの問題点
                初期値依存性が高い

                モンテカルロ法・TD法の流れ
                1. 価値関数$V$または行動価値関数$Q$に従って行動
                1. 報酬をサンプリング
                1. $V$や$Q$を更新
                偏ったサンプリングでの方策の更新では局所界に陥る
                →一定程度学習した方策を無視して行動する($\epsilon$-greedy方策，ソフトマックス)

                - $\epsilon$-greedy方策
                    確率$\epsilon$で完全にランダムに行動(大体0.01~0.1くらいの確率)

                - ソフトマックス方策
                    状態行動価値$Q(s,a)$をエネルギーと見做し，ボルツマン分布に従い行動を選択（ソフトマックス関数と同じ形）

                    全行動$\mathcal{A}$から行動aが選択される方策$\pi(s_t,a)$
                    $$
                      \pi(s_t,a) = \frac{\textrm{exp}\left(\frac{Q(s_t,a)}{T}\right)}
                      {\sum_{b\in \mathcal{A}}{\textrm{exp}\left(\frac{Q(s_t,b)}{T}\right)}}
                    $$
                    $Q(s,a)$が高いほど選択される
                    正の定数$T$(温度)を適切に設定し，確率に対する$Q(s,a)$の影響を制御する

                    ボルツマン分布
                    低エネルギー状態ほど，指数的に高い確率で出現する現象を表す確率分布
                    指数の肩の符号がソフトマックス分布と異なる
            - 価値関数ベース　まとめ
                - 価値関数
                    ある状態sから方策$\pi$に従って行動した際に，今後どのくらいの報酬を得ることができるかを表す指標
                - 代表例
                    Q学習
                - Q学習 問題点
                    状態・行動が増えすぎると処理できなくなる(全ての行動を考え，maxをとるため)
                    →DNNなどを用いてパラメトリックに価値関数を表現するDQNなどがある

        1. ## <a id=""方策勾配法ベースのアプローチ</a>
            方策を直接予測
            未来の状態を列挙する必要がない
            相性の良い(モンテカルロ木探索→alpha Go　など)と組み合わせることで高い性能の方策を実現

            復習（方策・価値ベースの違い）
            - 価値関数ベース：価値を基準にし，行動の選択は別のアルゴリズム(ソフトマックスや$\epsilon$-greedyなど)で決定
            - 方策勾配ベース：行動の選択そのもの(方策)をモデルで決定

            1. ## <a id="">方策勾配定理</a>
                パラメータ$\theta$を持つ方策を確率モデル$\pi_{\theta}$とする
                目的関数$J(\theta)$を最大化する$\theta$を，
                確率勾配$G_t^{\theta}$($J(\theta)$の$\theta$に関する確率方策勾配)を用いて，
                確率的勾配法に基づいて更新するアプローチ
                $$
                  \theta \leftarrow \theta + \alpha_t G_t^{\theta}\\
                  J(\theta) = V^{\pi_{\theta}}(s)
                  = \sum_{a} Q^{\pi_{\theta}}(s,a)\pi_{\theta}(a|s)
                $$
                $Q$ : 行動の価値
                $\pi$ : その行動をとる確率
                →を掛け合わせたもの(=期待値)の総和→価値関数→目的関数→最大化

                方策勾配定理
                目的関数$J$をパラメータ$\theta$で微分した式が以下で表されること
                $$
                    \nabla_{\theta} J(\theta)=
                    \mathbb{E}_{\pi_{\theta}}[
                    Q^{\pi_{\theta}}(s,a)\nabla_{\theta}\textrm{log}\pi_{\theta}(a|s)
                    ]
                $$
                (現在の方策における価値関数$times$行動の選択確率の対数の勾配)
                の方策に関する期待値
                →覚える

                この定理により，Q値のみ分かれば累積報酬を座右化させる方策の勾配が求まる

            1. ## <a id="">方策勾配法の工夫</a>
                方策勾配定理を元に方策を更新するが，$Q^{\pi_{\theta}}(s,a)$を正確に求めるのは困難
                →サンプリングした報酬の平均で近似→Q値の推定値
                $$
                  \nabla_{\theta} J(\theta) \approx
                  \sum_t r_t \nabla_{\theta}\textrm{log}\pi_{\theta}(a_t|s_t)
                $$
                →全ての時間ステップについて勾配計算を行うと，時間ステップ数分加算が行われ分散が大きくなる
                →学習が進みにくくなる(更新幅が大きく収束しにくい)
                →ベースライン関数$b(s)$を導入し，差し引くことで即時報酬を丸め込む
                →分散を小さくする
                $$
                  \nabla_{\theta} J(\theta) \approx
                  \left(r_t - b(s_t)\right) \nabla_{\theta}\textrm{log}\pi_{\theta}(a_t|s_t)
                $$
                →ベースライン関数に推定価値関数${\hat V}(s)$を用いることもあり，
                $\left({\hat Q}(s,a) - {\hat V}(s)\right)$をアドバンテージ関数${\hat A}(s,a)$という．
                (${\hat Q}(s,a) \approx r_t$)
                $$
                  \nabla_{\theta} J(\theta) \approx
                  {\hat A}(s_t,a_t) \nabla_{\theta}\textrm{log}\pi_{\theta}(a_t|s_t)\\
                  = \left({\hat Q}(s_t,a_t) - {\hat V}(s_t)\right)\nabla_{\theta}\textrm{log}\pi_{\theta}(a_t|s_t)
                $$

            1. ## <a id="">具体的なプローチ</a>
                - REINFORCEアルゴリズム
                    実際に得られた報酬の平均を使って近似した値をQ値の推定値とする
                    (方策勾配法を用いても，現状Q値の推定値は必要)

                    TステップからなるMエピソードを行った際の報酬の平均で近似する
                    $$
                      \nabla_{\theta} J(\theta) =
                      \frac{1}{M}\sum_{m=1}^M \frac{1}{T}\sum_{t=1}^T
                      \left(r_{m,t} - b\right) \nabla_{\theta}\textrm{log}\pi_{\theta}(a_t|s_t,\theta)\\
                      \\
                      b = \frac{1}{M}\sum_{m=1}^M \frac{1}{T}\sum_{t=1}^T r_{m,t}
                    $$
                - Actor-Critic
                    状態価値を学習(主にNN)→推定：Critic(推定器)
                    Criticの推定した${\hat V}(s_t)$をベースラインにしてActor(行動器)を改善していく

                    - A3C Asynchronous Advantage Actor-Critic
                        CriticをNNで学習したもの
                        $$
                          \nabla_{\theta} J(\theta,\phi) =
                          \left({\hat Q}(s,a) - {\hat V}_{\phi}(s)\right) \nabla_{\theta}\textrm{log}\pi_{\theta}(a|s)
                        $$
                        ($\phi$ : NNのパラメータ)

                        方策が決定的になった際，上式では更新が進みに悔いので
                        下式のようにエントロピー正則化項を加え，A3Cの勾配計算式を定義
                        →起こす行動が固まると，その行動の価値も固まり，QとVの差がほぼ0になる
                        →学習が進まない
                        $$
                          \nabla_{\theta} J(\theta,\phi) =
                          \left({\hat Q}(s,a) - {\hat V}_{\phi}(s)\right) \nabla_{\theta}\textrm{log}\pi_{\theta}(a|s)
                          \underline{- \lambda\sum_a \pi_{\theta}(a|s)\textrm{log}\pi_{\theta}(a|s)}
                        $$
                        ($\lambda( > 0)$は正則化係数)

                        ピーキーな方策の分布にしたくない(正則化項を導入)
                        →エントロピーを導入
                        →エントロピーを大きくする方向に学習が進む

## <a id = "cp9">Chapter 9</a>
1. ## <a id="">DL実用の工夫</a>
    - 事前学習
        大規模なデータセットで学習
    - 転移学習
        事前学習モデルにそうを追加して，追加した層のみ学習
    - ワンショット学習
        ラベル付きデータを一つだけ与えて学習
        ラベルなしデータの潜在的なクラス分けの表現を学習
        特徴空間上でラベル付きの周囲にあるデータのラベルを推論できる(ラベル付きが一つさえあれば良い)
    - ゼロショット学習
        全てのデータがラベルなし
        対象物ではなく，対象物を表現する特徴ベクトルを学習
        →訓練データに存在しない未知のクラスを識別学習
    - 半教師あり学習
        一部のデータのみにラベルをつけて学習
        アノテーションのコストを下げる
        1. ラベル付きで学習
        1. 1.のモデルでラベルなしを予測
        1. 2.の結果から，ラベルなしにラベル付け
            →再度ラベルあり/なしを学習
        学習データが不足の時に有効
        元のデータセットに偏りがあると不適
    - マルチタスク学習
        1つのモデルで複数のタスクを同時にこなせるよう学習
        (メインタスクとその共通のタスクを解くモデル)
        →メインタスクの汎化性能を向上させる
        →メインにとって有用となるはずのサブが特徴量から抜け落ちないように学習

        DLと相性が良い
        e.g.)物体検出　＝　物体のクラスを特定　＋　物体の位置を特定
        の共通タスクを解くモデル

## <a id = "cp10">Chapter 10</a>
1. ## <a id="">画像認識</a>
    2012年 ILSVRCでDLモデルが大差で優勝

    タスク
    - 画像分類
    - 物体検出
    - セグメンテーション

    有名なデータセット
    - 画像分類
        - MNIST
            手書き文字(アメリカの手書き郵便番号から？)
            - 規模：7万(6万/1万)
            - 28 * 28
            - 10クラス
          - Fashion-MNIST
              ファション画像
              - 規模：7万(6万/1万)
              - 28 * 28
              - 10クラス
        - CIFAR-10 / CIFAR-100
            乗り物や動物
            - 規模：6万(5万/1万)
            - 32 * 32
            - 10 / 100クラス
        - Food101
            食べ物(クラス数が多い/画像が大きい)
            - 規模：1000(750/250) / 各クラスごと
            - 最大512 * 512
            - 101クラス
        - ImageNET
            最大のデータセット
            ジャンルを問わないがぞデータセット
            - 規模：1400万以上
            - さまざま
            - 2万クラス
    - セグメンテーション・物体検出
        - Pascal VOC dataset
            基本的なデータセット
            - 規模：1万程度
            - カテゴリ数：20
        - COCO-Common Object in Context
            より難易度の高いデータセット
            - 規模：32.8万
            - カテゴリ数：91

    1. ## <a id="">画像分類</a>
        1. ## <a id="">ILSVRC</a>
            ImageNET Large Scale Visual Recognition Challenge

            ImageNETを用いた大規模がぞ認識コンテスト
            2010 スタート
            2012 DLモデルが優勝(ヒントン率いる)
            2015 人間の認識性能をこえる

            - AlexNet
                2012優勝モデル
                DLの火付け役
                Conv * 5 + FC * 3
            - VGG
                2014登場
                シンプル
                フィルタが3*3→多層化
            - GoogLeNet
                2014優勝モデル
                多重化
                Inception moduleの導入
            - ResNet
                2015優勝モデル
                残差
                Residual Blockの導入
            - DenseNet
                N個の残差
                Dense Blockの導入
            - Mobile Net
                計算量削減
                Depthwise Separable Conv.の採用

        1. ## <a id="">AlexNet</a>
            ILSVRC-2012で優勝
            Conv5層とFC3層

            活性化関数にReLUを採用
            GPUによる実装
            データ拡張
            Dropout
        1. ## <a id="">VGG</a>
            ILSVRC-2014で準優勝
            3 * 3のフィルターのみ採用して多層化
            (以前のフィルタータイズは様々であった + 縦横にpad1をすることで入出力の画像サイズが変わらない)
            Conv13層とFC3層(VGG16)
            (Conv数の違いでVGG19などがある)

            同じ数のフィルターに何回か通し，max poolingすることで
            中間画像を半分に縮小
            max pooling後の畳み込み層の出力チャネル数を二倍に

            - フィルタサイズによる利点
                パラメータ数を抑えつつ，大きなフィルタサイズと同じ(もしくはより広く)受容野をカバー
                5 * 5の畳み込みを1回(25→1)
                    パラメータ次元数：25(=5^2)
                3 * 3の畳み込みを2回(25→9→1)
                    パラメータ次元数：18(=3^2 * 2)
                →3 * 3の畳み込みに2回は5 * 5の1回分と同じ受容野
            - 多層化による利点
                非線形の活性化関数の適用回数を多くし，表現力が向上
        1. ## <a id="">GoogLeNet</a>
            ILSVRC-2014で優勝
            22層
            潤沢な計算リソースが必要で再現性が低い

            - Inception module
                フィルターサイズが異なる小さな畳み込み処理を統合させたブロック
                利点
                大きなフィルターを小さなフィルターたちで近似
                →パラメータ数を抑えつつ表現力を維持
                →密なパラメータを実現

                - 通常のパラメータ数
                    入力ch * kernel * 出力ch
                - Inception modele
                    入力ch * (5 * 5 * 出力1ch + 3 * 3 * 出力2ch + 1 * 1 * 出力3ch)
                    直方体からカーネルサイズが小さくなることでパラメータの体積が小さくなるイメージ
            - Auxiliary loss
                中間層における損失計算・逆伝播
                利点
                中間層における勾配消失防止
                正則化効果期待(ちょっとしたアンサンブル)
            - Global Average Pooling (GAP)
                チャネルごとにフィルターの画素平均をとる
                - 通常
                    学習パラメータ次元数：縦 * 横 * ch * 出力数
                    縦 * 横の部分でパラメータ数が一気に増え，過学習しやすい
                - GAP
                    学習パラメータ次元数：ch * 出力数
                    ch毎に平均とって1つする
            - Pointwise Convolution
                1 * 1フィルターを用いた畳み込み処理
                チャネル方向のみの畳み込み
                →次元数を削減できる
                - 通常(3 * 3)
                    3 * 3 * ch → 1 * 1 * 1の1マスにする
                - Pointwise Convolution
                    1 * 1 * ch → 1 * 1 * 1の1マスにする
                    →次元削減
        1. ## <a id="">ResNet</a>
            ILSVRC-2015で優勝
            152層
            Residual Blockが大きなブレイクスルー

            Residual Block
            [入力 x] - [残差 f(x)] - [出力 f(x)+x]
            入力xはそもそも正解に近く，正解との差f(x)を学習
            入力をフィルターの通したf(x)と
            出力までskip connectionしたxを足し合わせて出力とする
            →勾配消失が起きにくくなる
            →これまではBPの際に何回も微分をして(層の数分)いたが，skip connectionがあるため，仮にf(x)のBPで逆伝播誤差が小さくなっても，大きさそのままのxが戻ってくるため勾配消失が起きにくい

            Residual Blockの構造
            →パラメータ次元数を抑えつつ多層化することで精度を向上
            - Plain アーキテクチャ
                残差の計算に，[3 * 3, ch=64] - [3 * 3, ch=64]
                →パラメータ次元数：73728 (=(64 * 3 * 3 * 64)+(64 * 3 * 3 * 64))
            - Bottleneckアーキテクチャ
                残差の計算に，[1 * 1, ch=64] - [3 * 3, ch=64] - [1 * 1, ch=256]
                →パラメータ次元数：69632 (=(256 * 1 * 1 * 64)+(64 * 3 * 3 * 64)+(64 * 1 * 1 * 256))

            畳み込みに関わらず，skip connectionを入れることで勾配消失を解決
            →他の分野にも使える点がブレイクスルー
        1. ## <a id="">DenseNet</a>
            2016に登場
            ResNetを改良し，レイヤー間の情報伝達を強化
            →自分より前の層と全てskip connectionで繋がっている
            →Dense = 密

            Transition layer
            →Dense blockとDense blockの間
            Denseを通る毎に畳み込みとPoolingを通る
            →特徴マップを圧縮するためにダウンサンプリング(画像サイズを小さく)する
            (Dense block内ではskipの影響でダウンさんオウリングできないため，Dense間で行う)
        1. ## <a id="">MobileNet</a>
            2017に登場
            計算量を削減しつつ性能を発揮

            Depthwise Separable Convolution
            →Depthwiseとpointwise convを組み合わせた畳み込み
            表現力保ちつつ，パラメータ数を削減
            - 通常
                3 * 3 * 3 → 1 * 1 * 1
                →パラメータ次元数：27(=3 * 3 * 3)
                →パラメータ次元数：9(=3 * 3)
                $$
                  D_K\cdot D_K\cdot N\cdot D_F\cdot D_F \cdot M
                $$
            - Depthwise Separable Convolution
                →パラメータ次元数：12(=9 + 3)
                $$
                  D_K\cdot D_K\cdot M\cdot D_F\cdot D_F
                  + M\cdot N \cdot D_F\cdot D_F
                $$
                - Depthwise Conv.
                    平面方向の畳み込み
                    3 * 3 * 3 → 3 * 1 * 1
                - Pointwise Conv.
                    チャネル方向の畳み込み
                    3 * 1 * 1 → 1 * 1 * 1
                    →パラメータ次元数：3

            ハイパーパラメータによる計算量削減
            →計算コストは下がるが精度は犠牲になる
            - Width Multiplier
                モデルサイズの削減
                →チャネルサイズを間引く
                入力・出力チャネル数を(M,N)→($\alpha$M,$\alpha$N)とする
                ($0<\alpha<1$)
            - Resolutional Multiplier
                計算コストの削減
                →画像サイズを小さくする(解像度を荒くする)
                フィルターサイズを($D_F$,$D_F$)→($\rho D_F$,$\rho D_F$)とする
                ($0<\rho<1$)  

            調整後の計算量
            $$
              D_K\cdot D_K\cdot \alpha M\cdot \rho D_F\cdot \rho D_F +
               \alpha M\cdot \alpha N \cdot \rho D_F\cdot \rho D_F
            $$

    1. ## <a id="">物体検出</a>
        どこに何があるか(where, which)
        - 評価指標
            - どこに(where)
                IoU
                予測した領域に対する評価
                どのくらい領域をカバーできたか
                →予測領域∩正解領域 / 予測領域∪正解領域
            - 何が(which)
                accuracy
                分類結果に対する評価

        - ルールベース
        - DL登場以降
        に大別
        1. ## <a id="">Selective Search</a>
            ルールベースのアルゴリズム
            「本物に近い偽物」の候補領域の選定に使用
            - Selective Searchで候補領域(偽)を選定
            - SVMで真偽を混ぜて学習
            - 偽陽性を学習に加えてSVMを再学習

            候補のレベル感を階層でグルーピング
            下位層 細かいオブジェクト(耳・花・部品)
                  オブジェクトくらい(車・木・牛)
            上位層 大きく(空・それ以外)
            ↓
            上位層からは割合多く，下位層からは割合小さく選定
        1. ## <a id="">R-CNN</a>
            2013年登場
            CNNを用いた物体検出モデル
            Selective Searchで候補を探索し，それらをCNNで解析
            最後の分類はSVMで行われていた
            →特徴量の設計・抽出にCNNを用いた

            ROIs毎にCNNに通すので計算・学習速度が遅い
        1. ## <a id="">Fast R-CNN</a>
            2015年に登場
            CNN画像全体から特徴量を抽出→ROIsと対応する候補特徴マップを選定
            →CNNの対象を候補領域毎にするのではなく，画像全体にした(画像にいくつもROIsがある)

            CNNの出力(画像全体の特徴マップ)とROIsを対応させて，候補毎の特徴マップを選定
            →ROI poolingによりリサイズする
            →FCは入力の形が固定のため，全てのROIを同じ大きさにする

            ROI pooling
            FCの入力がN * Nの場合，ROIを縦横N分割にし，各領域でmax poolingなどを行いN * Nの大きさにする

            問題点
            Region proposal(領域提案)が従来アプローチを継承していて，精度が低い
            →ルールベース
            - region proposal自体の精度が低い
                提案を間違えてしまえば，後段のCNNがいくら優秀でも物体検出できない
                →Faster R-CNN
            - region proposalの演算コストが高い
        1. ## <a id="">Faster R-CNN</a>
            2015年に登場
            領域提案にCNNを用いる
            →精度・計算速度向上を実現

            End-to-Endな学習が可能に
            →以前までは途中までにselective searchとかが挟まっていた

            CNNを用いたRegion proposal
            ZFNetやVGG, ResNetで特徴抽出
            →特徴マップの断面(Anchor)毎に9個の候補領域を設ける
            →9個それぞれに，正解領域とのずれ(中心座標(x,y), 縦横の長さ(w,h))・背景フラグと物体フラグ，が存在
            →都合54個の値が出力値となる

        1. ## <a id="">YOLO</a>
            2016年に登場
            Faster R-CNNまでの課題
            認識速度が実用的なレベルでない(GPUをもしいて5fps(1秒間に5枚))
            →検出・抽出を分けたアーキテクチャが限界か

            YOLO(You Only Look Once)
            検出・識別を同時に行う
            →YOLOv2, 3などが存在
            →近年でも使われるモデル

            - 入力画像を複数セルに分割し同一のCNNで行う
                →1セルに対して$B*5+C$の出力
                - セル毎にB個(通常B=2)ずつバウンディングボックスを推定(信頼度を設定)
                    →1セルに対して，$B*5$の出力がある
                    - ボックス中心座標(x,y)，幅高さ(w,h)
                    - confidnce : 物体である確率 * IOU(正解領域との近さ)
                - セル毎に物体クラスを推定
                    物体クラス分類確率Cの推定(バウンディングボックスによらない)
                    →1セルに対してCの出力
            - non-maximal suppressionで，同じクラスとして推定された重複領域を抑制する
                各セルから共通としてもってこっられたバウンディングボックスの中から一番高いconfidenceを残す
                閾値以下のconfidenceを消す

            1. S * Sに分割
            1. a.大きさ推定
                b. 物体推定
            1. 最終決定

        1. ## <a id="">SSD</a>
            2016年に登場
            Single Shot Multibox Detector
            様々なバウンディングボックスを表現可能
            YOLOの親戚
            YOLOより精度向上，処理速度は若干劣る

            YOLOの制約(計算時間を重視した代償)
            - グリットサイズの固定
            - 1セルにつき1つのみ物体識別
            →物体が密集した画像
            →大きさが様々な物体が写る画像
            に弱い

            - バウンディングボックス推定で工夫
                縦横日の異なるデフォルトボックスをいくつか用意
                →後に微調整
            - 分割数を段階的にスケールダウン
                S * Sのように固定せず，少しずつ領域を大きくしていく
                →畳み込み途中のバウンディングボックスが小さい(分割数は大きい)段階でのconfidenceをそれぞれ出力に持ってくる
                (これまでは最後のS * Sの結果しか使っていなかった．S * Sになる以前の特徴マップ(サイズが大きい=グリッドが粗い)のconfidenceも使う)
                →それらを組み合わせることで様々な大きさの物体を検出できる(物体の大きさに合わせたバウンディングボックス)
                (これまでは一定の大きさの物体しか検出できなかった)
            - Hard negatice miningで，画像中の「背景」・「物体」の不均衡度を調整
                背景ばっかり持ってこないように調整
                (画像の中はほとんど背景のため)

    1. ## <a id="">セグメンテーション</a>
        画像に対してピクセルレベルでクラス分類

        画像分類との違い
        - 画像分類
            予測値は一つ(画像毎のクラス)
        - セマンティックセグメンテーション
            予測値は画像サイズ分(ピクセル毎のクラス)

        Encoder-Decoderネットワークと構造は似ている
        入力画像→画像サイズのクラス値を出力

        1. ## <a id="">FCN</a>
            all CNN　全層畳み込みネットワーク
            DeConv層の活用
            最後の全結合層が1 * 1の畳み込み
            →入力の画像を可変にできる(これまでは全結合層の次元数が決まっていたので入力サイズも固定になっていた)

            逆畳み込み Deconvolution
            小さくなった特徴マップを大きくする
            1. stride分だけ空白を設けて特徴マップの要素を配置
            1. kernel size - 1だけ特徴マップの周囲に空白を取る
            1. padding分だけ周囲の空白を削る
            1. 畳み込みを行う

            pooling層の特徴マップの活用
            32 * 32→1 * 1にたたみ込んだ後，32 * 32のdeconv.したい
            →1 * 1から直でも作れる
            →1 * 1を2 * 2にして手前のpoolingとsum．それを32 * 32へdeconv.
            →同様に上の足し合わせた2 * 2を4 * 4にして，poolingの4 * 4とsum．32 * 32へdeconv.
            →ここまでで作られた32 * 32をチャネル方向に1 * 1で畳み込み，ピクセル単位でのクラス分類に用いる

            FCNの課題
            poolingで圧縮した特徴マップを逆畳み込みしているので，
            元画像の空間情報が失われた特徴マップを使うことになる
            →U-Netへ
            特徴マップを一時的に保持し続ける必要があるので，メモリ効率が悪い(U-Netも同様)
            →SegNetへ
        1. ## <a id="">U-Net</a>
            shortcut接続の導入
            →空間情報補完

            目的は異様画像のsegmentation
            shortcutでとも画像の特徴を維持し続ける

        1. ## <a id="">SegNet</a>
            up-pooling使用
            →メモリ効率化

            基本的な構造はU-Netまでと同じ
            shortcutで渡す情報にメモリ効率がよくなる工夫が入る(up-sampling)

            up-sampling
            max-pooling indexを使用して特徴マップをアップサンプリング
            max-pooling index : Encoderのmax-pooling層で抽出したpixel位置を表す添字
            →添字のみの保存で，FCNに比べメモリ効率を大幅に低減
            →max-poolingで取れなかった情報は捨てる

## <a id = "cp11">Chapter 11</a>
1. ## <a id="">自然言語処理</a>
    自然言語：人がコミュニケーションに用いる言語
    - 文書分類
    - 翻訳
    などのタスクが存在

    1. ## <a id="">文書分類タスク</a>
        入力テキストの文脈を学習し，pos/negを分類

        単語はベクトルとして数値化
        →意味を表現

        単語埋め込み(word embedding)
        単語のベクトル化を「単語埋め込み」という
        →単語の意味を保ちながら数値化(ベクトル化)できる
        →MLPを用いて，ベクトル化を行う
        →[単語フラグデータ(疎)] - [MLP] - [単語埋め込み後(密)]
        →オートエンコーダーのように密なベクトルを作ることができる

        学習方法
        →下記二つを正しく行われていれば，正しく意味を理解できている考えられる
        - CBOW(Continuous Bag-of-Words)
            前後いくつかの単語間の単語を予測
            →myとisを入れて，name?が出力
        - Skip-gram
            一単語からその前後の単語を予測する
            →nameを入れて，my?, in?が出力

        Bi-LSTMで解く
        word embedding + Bi-LSTMでneg/pos分類
        →順方向・逆方向それぞれの最後の出力を集約したものをMLPとsigmoid(softmax)に通して分類

        enbedsの系列長が揃っていないと処理が困難になる
        →paddingを行って揃えることも
    1. ## <a id="">翻訳タスク</a>
        1. ## <a id="">モデル</a>
            ルールベースと統計的モデルに大別
            1. RBMT(Rule-Based Machine Translation)
                登録済みのルール(文法)を適用し，訳文を出力
                - メリット
                    - 文語に強い
                        あまり使われない単語なども正確に訳せる
                    - 訳文の解釈性が高い
                        どのように訳されたのかわかりやすい
                    - 専門書向き
                        マニアックな単語にも対応
                - デメリット
                    - 口語が難しい
                    - 辞書に依存
                    - 辞書の生成は人手
            1. SMT(Statistical Machine Translation)
                統計モデルを学習させ訳文を出力
                →DLを用いた翻訳はSMTに該当
                1. 原文と対訳を用意
                1. 関係を学習
                1. 学習した文法関係で訳文を推論
                - メリット
                    - 文語・口語問わず訳せる
                    - データが充実すれば人間の翻訳精度を超える
                    - 文脈を踏まえた翻訳
                        文法のみならず，細かなニュアンスも学習できる
                - デメリット
                    - データ依存
                    - 頻度の少ない用法・語句に弱い
                        マニアックな用語など
                    - 対訳データの収集が難しい
        1. ## <a id="">評価指標</a>
            - 分類タスク
                - precision
                - recall
            - 翻訳タスク
                - Perplexity：流暢かどうか
                - BLEU：意味が正しいか

            1. Perplexity
                「次の単語として絞り込めた候補単語数」を測る
                $$
                  Perplexity = 2^H ;\,\,
                   H = -\frac{1}{N}\sum_{n=1}^N \sum_{t=1}^T\log P(w_{nt})\\
                   \textrm{where;\, N:\,batchsize,\, T:\,words\,in\,one\,sentence}
                $$
                $P(w_{nt})$ : n番目の文章のt番目の単語の出現確率

                - Perplexityが低い→次の単語を絞り込めている
                    →文章の流暢さを表しているとも言える
                - 確率の逆数を算出することで候補単語数を導く
                - 正解データと照らし合わせる必要がない

            1. BLEU
                予測分と正解分の類似度を測る
                →定義式はスクショ参照

                - 前提として「プロの翻訳者の役bん(正解文)に近いほど，その機械翻訳の精度は高い」
                - 値域は0〜1．0.4以上であれば高品質

                注意点
                - 性能指標としての問題点
                    - 字面しか考慮されない
                        類語は全く異なるものとして認識
                        notなどが入る否定分と肯定文の区別がつかない
                    - バリエーションが多い
                        ライブラリによって定義式が異なり一貫性がない
                - 準備段階のコスト
                    - 正解分の用意が必要
                    - n-gramの種類数を指定する必要がある
                        多くの場合は1〜4-gram単位で計算される

                上記のような注意点はあるものの，デファクト・スタンダードである

                N-gram, n-gram
                - N-gram
                    手法の名前
                    ある文章を，連続するn文字/単語の塊に分割する
                - n-gram
                    分割された塊

                N-gram言語モデル
                N-1単語までを事前情報とする
                →前提：あるsentenceが得られる確率は，単語の条件付き確率の重ね合わせとなる
                →仮定：ある単語おの分布は直前のn-gramのみに依存して定まる
                →N-gramで分割された文章(n-gramの配列)を確率分布で表現したモデル
                （言語モデル：目的言語の文章が得られる確率を推論するモデル）
                $$
                  P(\textrm{senence}) = \prod_{i=1}^T P(w_i |\underset{全ての単語}{\underline{w_0,\dots,w_{i-1}}})
                $$
                →ある単語の発生確率は，それまでの単語に由来するはず

                N=1の場合，事前情報なしのモデルとなる
        1. ## <a id="">翻訳タスクとNN</a>
            前項目までで翻訳タスクを解く準備が整った
            - 入力文章をword embeddingで数値化
            - 精度を測る，微分可能な評価指標

            一方で，どのように文章(系列データ)を出力するのか，の問題が残る
            →系列データを出力する「生成モデル」

            e.g.) Encoder-Decoderモデル
            1. Encoderが潜在変数に落とし込み
            1. Decoderが潜在変数を元にデータを生成
            →文章を入れて文章を出すことに適している
            [系列データ] - [Encoder] - [文法文脈] - [Decoder] - [系列データ]

            Encodre, DecoderにはRNN系モデルが使われる
            →RNN, LSTM, BiLSTM, GRUなど
            - Encoder
                「前の単語からの文脈や文法」をエンコード
            - Decoder
                「エンコード情報」をデコードして文章を生成

            Encoderで圧縮する意味
            →入出力文で対応する単語の位置やn-gram数が異なる
            →それに対応できる

            1. ## <a id="">Sequence-to-Sequence (Seq2Seq)</a>
                系列データを入出力とするアーキテクチャ
                [Sequence] - [Encode] - [Context(文脈)] - [Decode] - [Sequence]
                のループ

                機械翻訳，自動要約，質疑応答，メールの自動返信などが応用先

                モデル構造
                - Encoder
                    埋め込みベクトルを順にRNNに入力
                    各タイムステップの出力は捨てる(次の入力に使わない)
                    最終単語の隠れ状態をcontextとする(Decoderへ)
                    →ここに全ての情報が含まれている
                - Decoder
                    最初の隠れ状態はcontextを受け取る
                    最初の入力は開始トークン<SOS>，終了トークン<EOS>が出力されるor指定の長さ以上で終了

                    各タイムステップの出力単語を再度embedding(埋め込みベクトル化)して次の入力へ
                    1. 各層の出力を確率ベクトルpに変換して，次にくる単語を求める
                        MLP(隠れ状態空間×語彙空間)に通し1本のベクトルにする
                        softmax関数を使って，語彙空間の確率分布に変換
                    1. pを離散化して，埋め込みベクトルを得る
                        離散化の例：単語出現確率$\bm{p}^t$が最大のものを1に，他は0に
                        →次にくる確率が最も高い単語
                        →それをyとして次のステップの入力とする

                損失関数
                下式の最小化→Pの最大化
                $$
                  L = -\log P(y^1,\dots,y^{T^{\prime}} | x^1, \dots,x^T)
                $$
                $x^t$ : 入力文のt番目の単語
                $y^t$ : 正解文のt番目の単語

                Seq2Seqの特徴
                - Encoder, Decoderの構造が異なっても良い
                - Decodreにcontextを渡す方法はいくつかある
                    初期値として与える
                    全てのタイムステップで隠れ層に接続する
                - 多くの派生モデル．目的に応じて使い分け
                    汎用性が高い
                    改良
                    - 入力データの反転　Reverse
                        エンコーダの入力文を反転させる
                        →デコードするまでの道のりが短くなるため，学習が進みやすく精度も期待できる
                    - 覗き見 Peeky
                        エンコード結果contextの有効活用
                        →contextを別のRNN系モデルにも「覗かせる」ことで，skip-connectionの効果がある

            1. ## <a id="">Attention</a>
                
