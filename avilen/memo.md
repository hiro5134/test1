<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });

</script>

# [My kicks](#mykicks)

## <a href="#index">Index</a><a id="index"></a>
* [Chapter 1](#cp1)
    * [Library](#lib)
    * [Higher API](#higherapi)
* [Chapter 2](#cp2)
    * [活性化関数](#mlp)
    * [万能近似定理 Universal Approximation Theorem](#uat)
    * [出力ユニットとタスク](#unit)
    * [損失関数](#loss)
* [Chapter 3](#cp3)
    * [バッチ処理](#batch)
    * [ミニバッチ処理](#minibatch)
    * [最適化手法](#adapt)
        * [最急降下法](#gd)
        * [確率的最急降下法](#sgd)
        * [モメンタム](#momentum)
        * [ネステロフのモメンタム](#momentum2)
        * [AdaGrad](#adagrad)
        * [RMSProp](#rmsp)
        * [Adam](#adam)
* [Chapter 4](#cp4)

## <a id = "cp1">Chapter 1</a>
1. ## <a id="lib">Library</a>
    | Lib | Graph | Company | Year | memo |
    | - | - | - | - | - |
    | TensorFlow | Define and Run | Google | 2015 | 計算グラフを自分で定義 |
    | Chainer | Define by Run | Preferred Networks | 2015 | 2019年開発終了 |
    | PyTorch | Define by Run | Facebook | 2016 | 計算方法がnumpyに似てる |
    | TensorFlow 2 | Define by Run | Google | 2019 | シンプルな実装可能 |
    | Apache MXNet | Define and Run <br> Define by Run| AWS | 2017 | 静的・動的どちらも可能|
    | Caffe2 | Define and Run | Facebook | 2017 | 2018年PyTorchへ統合 |
    | Congnitive Toolkit (CNTK) | Define and Run | Microsoft | 2016 | 音声認識特化 |
    | theano | Define and Run | モントリオール大学 | 2007 | 2017年開発終了 |
1. ## <a id="higherapi">Higher API</a>
    | API | Company | Year | memo |
    | - | - | - | - |
    | Keras |  Google | 2015 | TF用のAPI(TF2から標準API) |
    | Gluon | AWS, Microsoft | 2017 | MXNet用のAPI |

## <a id = "cp2">Chapter 2</a>
1. ## <a id="activation">活性化関数</a>
    中間層の表現を非線形にするため．  
    表現力の向上
    - step関数  
        勾配が0になり最適化が難しい
    - sigmoid関数
        0~1  
        勾配消失が起こり，中間層の最適化に不向き
    - softmax関数  
        複数の入力を扱える  
        出力の合計が1
        勾配消失が起こり，中間層の最適化に不向き
    - tanh関数  
        -1~1
        勾配消失が起こり，中間層の最適化に不向き
    - ReLU関数  
        x>0で勾配が計算可能で勾配が消失しない  
        x<0での勾配を計算するために以下の派生が存在  
        - Leaky ReLU  
            x<0に緩やかな傾き
        - Randomized ReLU  
            x<0の傾きをランダムに変更(予測時は平均値に固定)
        - Parametric ReLU  
            x<0の傾きを学習
    - MAXOUT関数  
        ReLU関数の一般化  
        要素数kのグループから各グループの最大値を抽出  
        ReLU関数や二次関数などに近似可能
        ReLu関数よりも表現力が高く勾配が消えない   
        活性化関数自体を学習

1. ## <a id="uat">万能近似定理 Universal Approximation Theorem</a>
    3層モデルの中間層のノードを極限まで増やせばあらゆる関数を近似可能  
    →現実的でない  
    →活性化関数により，層を横に伸ばすことで表現力をあげられる（計算コストも少なく収まる）  
    →万能近似定理+活性化関数でDLが飛躍

1. ## <a id="unit">出力ユニットとタスク</a>
    | Task |  | Unit | Activation Func. | Loss Func. | Probability | memo |
    | - | - | - | - | - | - | - |
    | 回帰 |  | 線形ユニット | 恒等関数 | MSE | ガウス分布 | 連続値を予測<br>目的変数（連続値）の分布をガウス分布と仮定<br>出力値は目的変数の分布（条件付きガウス分布）の平均値を返そうとする|
    | 分類 | 二値分類 | シグモイドユニット | sigmoid関数 | Binary Cross Entropy | ベルヌーイ分布 | ラベルが1である確率を予測<br>目的変数の分布は二項分布<br>ベルヌーイ分布を出力|
    | | 多値分類 | softmaxユニット | softmax関数 | Cross Entropy | | ラベルごとに確率を予測 |

1. ## <a id="loss">損失関数</a>
    尤度の最大化　損失関数の最小化　となるように設計

## <a id = "cp3">Chapter 3</a>
1. ## <a id="batch">バッチ処理</a>
    オンライン学習 入力１つ
    バッチ処理　入力nこ　あらゆる場所がn次元になる
        sum撮るときはdim=1
        基本サンプルが行，説明変数が列なので列方向に足していくことでサンプルごとのsumが取れる

1. ## <a id="minibatch">ミニバッチ処理</a>
    バッチ処理　データ数が膨大であれば計算量も膨大にありパラメータの更新が遅い
    →データを分割してバッチ処理をする
    | Task | 勾配推定 | 収束速度 |
    | - | - | - |
    | バッチサイズ大 | 正確 | 遅い |
    | バッチサイズ小 | 誤差を高める | 早い |
    　　
1. ## <a id="adapt">最適化手法</a>
    1. ## <a id="gd">最急降下法 Gradient descent</a>
        全体像はわからないが，その地点での最短経路方向に更新
        →求めた勾配方向とは逆向きに，その大きさ*η(学習率)を更新

        最急降下法ではlocal minimaに陥る可能性

        η小　時間がかかる　最適な場所を見つけられる可能性
        η大　早い　値がブレて最適な場所を見つけられない可能性
    1. ## <a id="sgd">SGD 確率的勾配降下法 stochastic gradient descent</a>
        確率的な山の一地点で最短経路方向に更新
        ミニバッチ抽出にランダム性を持たせ，損失関数が確率的になる

        最急降下法の弱点を克服
        （これまで）学習率を確率的に変化
        →損失関数が複雑すぎてうまくいかない

        そこでSDGはデータを確率的に変える
        →データをシャッフルして，それぞれの損失関数の挙動を変える
        →あるミニバッチでは上がり，他方では下がる，などができ最適な場所を見つけられる

        $$
          {\bm W} = {\bm W} - \eta\frac{\partial L}{\partial {\bm W}}
        $$

        一方で損失関数の形状が急峻な場合，振動してしまう

    1. ## <a id="momentum">モメンタム</a>
        移動平均を導入（慣性項）
        精度を保ちつつ，収束速度を早くする

        SDG＋慣性の法則→モメンタム

    1. ## <a id="momentum2">ネステロフのモメンタム</a>
        慣性項なしで更新した場合に慣性項をたしあわせ
        →ブレーキの役割

    1. ## <a id="adagrad">AdaGrad</a>
        過去の購買の二乗和を記憶して学習率を調整

        今までの勾配が大きい　学習率小さく更新
        今までの勾配が小さい　学習率大きく更新

        問題点
        学習率が0になる可能性
        →局所解

    1. ## <a id="rmsp">RMSProp</a>
        古い情報を忘れ，新しい情報を反映しやすくした

    1. ## <a id="adam">Adam</a>
        現在のスダンダード

        イテレーション数でバイアス補正
        →初期段階の不安定さを解消

        ${\bm m}$は速度の概念
        Momentum SDGにおける${\bm v}$
        減衰率$\beta_1$で過去の勾配情報${\bm m}$と現在の勾配情報$\frac{\partial L}{\partial {\bm W}}$を調整する
        $$
          {\bm m} = \beta_1{\bm m} + (1 - \beta_1)\frac{\partial L}{\partial {\bm W}}
        $$

        ${\bm v}$過去の勾配の二乗和
        $$
          {\bm v} = \beta_2{\bm v} + (1 - \beta_2)\left(\frac{\partial L}{\partial {\bm W}}\right)^2
        $$

        ${\bm m}, {\bm v}$は減衰率によって減衰させられた勾配の(二乗)和
        0.9が設定されると，勾配の１割しか学習に使われない
        下記で調整
        $$
          {\hat{\bm m}} = \frac{{\bm m}}{1-\beta_1^t},
          {\hat{\bm v}} = \frac{{\bm v}}{1-\beta_2^t}
        $$
        tはイテレーション数
        $\beta^t$は0に漸近，$\frac{1}{1-\beta^t}$は1に漸近
        ${\bm m, v}$に過去の情報が蓄積されるまでは勾配を利用
        更新が進むにつれ上式の影響は薄れる

        ${\hat{\bm m}},{\hat{\bm v}}$を用いてパラメータ${\bm W}$を更新
        $\epsilon$は微小項（0除算のため）
        $$
          {\bm W} = -\eta\frac{{\hat{\bm m}}}{\sqrt{{\hat{\bm v}}}+\epsilon}
        $$

        以上を総合して
        $$
          {\bm W} = -\eta\frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}
          \frac{{\bm m}}{\sqrt{{\bm v}}+\epsilon}
        $$

## <a id = "cp4">Chapter 4</a>
1. ## <a id="">勾配消失問題</a>
    backpropagationにて1より小さい値の掛け算により入力層に近い隠れ層に伝わる勾配がほぼ0になる

    activationに用いられるsigmoidやtanhの微分値は基本＜1であるため

    勾配が小さい→更新がほぼない

    ReLUにより解決の道
    微分値→常に1(x＞0)　→　勾配が消失しにくい
    計算が高速
1. ## <a id="">過学習</a>
    訓練データに過度に適合
    汎化性能が低い

    表現力の高さ→過学習しやすい

    正則化などで解決を図る
    1. ## <a id="">正則化</a>
        機械学習の目的：既知のデータから未知のデータを予測したい
        →既知データに対する損失関数の期待値を減少させる
        →予測値と期待値の差を最小化
        1. ## <a id="">バイアス-バリアンス分解</a>
            損失関数の期待値最小化にあたり，式要素を分解し各項から最小化を図る
            →回帰タスク（損失関数が二乗和誤差）のみ成り立つ

            バリアンス項
            →予測値の分散
            →大ならば過学習

            バイアス項
            →予測値の期待値と目的変数の期待値の差
            →大ならば未学習

            ノイズ項
            →データのノイズ
            →MLでのフィティングには無関係

            バリアンスとバイアスはトレードオフ
            基本的に学習されていることが望まれるため，バリアンスを抑えながら学習を図る
            →正則化
        1. ## <a id="">ノルムペナルティ</a>
            ノルム：ベクトルに対する距離を測る指標
            ハイパパラメータ
            $$
            ||x||_p = \left(|x_1|^p + |x_2|^p + \cdots + |x_D|^p\right)^{\frac{1}{p}}
            $$
            pの値によってL0,L1,L2,L$\infty$が存在

            パラメータが極端な値を取らないように損失関数に制限をかける

            ノルムつき損失関数＝損失関数＋$\lambda$(パラメータのLPノルム)

            ラムダ大　正則化強　過学習抑制　
            抑えすぎると精度が下がる

        1. ## <a id="">ラッソ回帰 Lasso Regression</a>
            L1ノルムによる正則化
            スパースな解を得やすい

            一般に
            通常損失関数とパラメータ制約領域の接点が最適解となりやすい

            ラッソ回帰
            領域が直線のため軸で接しやすい（それぞれのパラメータが0になることが多い）

            $$
              L_{norm}(y(X), t) = L(y(X), t) + \lambda(|w_0| + |w_1| + \cdots + |w_n|)
            $$

        1. ## <a id="">リッジ回帰 Lidge Regression</a>
            L2ノルムによる正則化
            汎化性能の高い解を得やすい

            リッジ回帰
            領域が円状なため，柔軟な値で最適解を取りやすい

            $$
              L_{norm}(y(X), t) = L(y(X), t) + \lambda\sqrt{w_0^2 + w_1^2 + \cdots + w_n^2}
            $$

        1. ## <a id="">elastic Net</a>
            ラッソ回帰とリッジ回帰の折衷案

            $$
              Lasso = \Omega(W) = (|w_0| + |w_1| + \cdots + |w_n|)
            $$
            $$
              Lidge = \Omega(W) = \sqrt{w_0^2 + w_1^2 + \cdots + w_n^2}
            $$
            $$
              elastic Net = \alpha\Omega(W) = \lambda(|w_0| + |w_1| + \cdots + |w_n|) + \frac{(1-\lambda)}{2}\sqrt{w_0^2 + w_1^2 + \cdots + w_n^2}
            $$

        1. ## <a id="">パラメータ拘束</a>
            類似したタスクのモデルのパラメータにノルムペナルティを課す
            モデルA(パラメータ$W^{(A)}$)とモデルB(パラメータ$W^{(B)}$)は類似タスク
            これらの距離を近くする
            下記項を最小化する
            $$
              \lambda\Omega(W^{(A)}), W^{(B)})=\lambda\left( |w_1^{(A)}-w_1^{(B)}|^2 + \cdots + |w_D^{(A)}-w_D^{(B)}|^2\right)^{\frac{1}{2}}
            $$

        1. ## <a id="">パラメータ共有</a>
            モデルの一部のパラメータを同じ値にする
            どこを同じにするかは開発者による
            例えば，MLPのノード間のパラメータを左右対象にする

            パラメータ共有・拘束においても，過剰な表現力に制限をかけるイメージ

        1. ## <a id="">早期終了 Early stopping</a>
            最適化への道を途中で打ち切り，パラメータ制約を行う効果がある

        1. ## <a id="">アンサンブル学習</a>
            複数のモデルを組み合わせて汎化性能向上

            | Name | 特徴 | 正則化効果 | 備考 |
            | - | - | - | - |
            | バギング | 同時多数学習モデル | varianceを下げる | 多数決モデル<br>訓練データから多数のデータセットを作りそれぞれのモデルで学習 |
            | ブースティング | 段階的改善モデル | biasを下げる | 徐々にupdate<br>前のモデルの誤った予測を重みづけ学習 |

        1. ## <a id="">ノイズによる正則化</a>
            ノイズ：ある現象の不確実性から生じるデータや情報

            ノイズを加える位置によって手法が異なる

            入力層：data augmentation
            隠れ層：dropout, dropconect
            目的変数：label smoothing
            1. ## <a id="">Data Augmentation</a>
                学習データに多様性を持たせ，汎化性能を向上させる
                回転拡大縮小切り抜きなど
            1. ## <a id="">Dropout</a>
                訓練時
                指定された割合のノードをランダムに非活性
                部分的なネットワークのアンサンブル学習
                (in PyTorch 指定した確率pに基づくベルヌーイ分布からのサンプルを持ちに消すノードを選択)

                評価時
                全てのノードを使用
                出力時には1-pを乗算（確率pで消すノードを選択しているため）

                実用性が高い
                学習パラメータが削減されるため計算コスト小さい
                ほぼ全てのモデルに適用可
                訓練データが少ないとうまくいかない傾向
            1. ## <a id="">Dropconnect</a>
                指定された割合のパラメータ（ブランチ　線）ランダムに非活性

                dropoutより優れた性能
                乱数のシードに大きく左右され再現性が困難
            1. ## <a id="">Label Smoothing ラベル平滑化</a>
                目的変数にノイズを加える

                分類タスク(2値分類)
                人のラベル付けに誤りがる可能性
                特定のカテゴリに過剰に適合しやすい（過学習しやすい）

                に対する効果
                多少のラベル付ミスを吸収
                正解カテゴリ以外にも値が入ることで，特定のカテゴリに過剰に適合することが減る

                $$
                  \{0, 0, 1, 0\}
                $$
                に対して各ラベルにノイズを付加(ノイズ*$\alpha$)
                $$
                  \{\alpha\xi_1, \alpha\xi_2, (1-\alpha) + \alpha\xi_3, \alpha\xi_4\}
                $$

    1. ## <a id="">バッチ正規化</a>
        正規化
        平均0, 分散1にスケーリングする
        効果
        スケールの異なる特徴量を同様な基準に扱える
        学習データの分布と検証データの分部生合成を維
        スケールの大きな特徴量に左右されない

        バッチ正気化
        バッチごとに正規化
        効果
        - 学習速度の向上
            - 共変量シフトの減少
            - 大きな学習率でも安定
        - 正則化効果がある
        - NNの重みの初期値に依存しづらい

        (内部)共変量シフト
        学習が難しくなる
        - 各層の入力時のbんぷが学習の過程で変化する現象
            1. パラメータは学習毎に更新
            1. 各層出力分布は学習毎に異なる
            1. 故に各層入力分布が学習毎に異なる→共変量シフトが起こる
        - 共変量シフトで学習が難しくなる理由
            - 学習率を下げる必要．（学習時間が長くなる）
            - パラメータの初期値依存性が高くなる（初期化に注意)

    1. ## <a id="">計算コスト</a>
        1. ## <a id="">高速化</a>
            GPUで並列処理高速化

            - 分散処理
                - データ並列
                    - 同期型
                        全ての並列処理で同じ勾配を用いる
                        勾配を常に一定に保つ
                    - 非同期型
                        アプローチ毎に用いる勾配が異なる
                        勾配を常に更新する
                - モデル並列
        1. ## <a id="">軽量化</a>
            1. ## <a id="">蒸留</a>
                学習済教師モデルの入出力を，軽量な生徒モデルで学習
                soft, hard target lossを最小化
                精度は落ちる反面，軽量なモデルで計算資源を大幅に削減

                soft target loss
                教師モデルと生徒モデルの出力の損失
                hard target loss
                生徒モデルの入出力分布の損失

                知識の蒸留
                精度の高い教師モデルは，粒度の細かい重要な知識を反映できる

                例）猫・犬・ハムスター・へびに分類
                {1, 0, 0, 0}に対して{0.85, 0.1, 0.04, 0.01}を教師モデルが出力
                猫には耳があり，丸い瞳
                犬には多少にているが，蛇には程遠い．→粒度の細かい知識

            1. ## <a id="">枝刈 Pruning</a>
                寄与の小さい重みを0にする（データの流れを切る）
                （値の小さい重みを0にする）

                学習の繰り返しの中で枝刈を実行し，パラメータ削減→精度を保ちつつモデル軽量化

            1. ## <a id="">量子化</a>
                浮動小数点で表現されるパラメータを低ビットで表現（近似）
                精度を落とさずモデル圧縮

                32bitを上位16bitで表現　など

      https://www.anarchive-beta.com/entry/2020/08/16/180000

      https://qiita.com/hikobotch/items/78b53de44069fb19d311

## <a id = "cp5">Chapter 5</a>
1. ## <a id="">画像認識</a>
    - 画像分類
    - 物体検出
    - セグメンテーション

1. ## <a id="">CNN</a>
    1. ## <a id="">畳み込み</a>
        複数チャネル
            カーネルを入力の次元分用意（カラー画像なら3カーネル）
            出力は各チャネルの総和

        複数カーネル
            前段のカーネルをn個用意
            出力が1次元増える

        ミニバッチ処理
            全段の出力がミニバッチ回数分増えそう，出力次元が1次元増える

        - メリット
            - 疎結合
                画像データ特有の特徴を考慮
                特に，離れた陽うくセル同士の関係を無視
            - パラメータ共有
                同じ重みのカーネルを使い回し，メモリ使用量を抑制
        - デメリット
            - データ加工技術は含まれない
                スケーリングや回転といった画像変換はできない
                回転された画像を別途用意

    1. ## <a id="">ハイパーパラメータ</a>
        1. ## <a id="">パディング</a>
            $$
              out = (H + padding*2 - Filter)/stride + 1
            $$
            - メリット
                - 端のデータの抽出
                    パディングがないと，恥の畳み込み回数が少なくなる→重要度が下がる
                - データサイズを調整可

    1. ## <a id="">プーリング</a>
        情報抽出

        学習パラメータなし
            最大値や平均値をとるだけ（マックスプーリングが主，特徴を取りやすいため）
        チャネル数が変化しない
            チャネル毎に独立な計算
        微小な一変化にロバスト

    1. ## <a id="">im2col</a>
        入力データのカーネル適用箇所をバッチサイズ方向に切り出し，それらを行方向に並べる

        カーネルを行方向に展開し，各カーネルを列方向に並べる

        内積を取り，整形して出力データの形にする

        行列計算に帰着（大きなマトリクスの計算を高速に）

    1. ## <a id="">画像における正規化</a>

        1. ## <a id="">batch normalization</a>
            これによりDLの精度が爆発的に向上

            パラメータの勾配計算は，他のパラメータに影響が出ない前提
            偏微分をし，損失を減らす方向へ計算している

            データは適度な広がり・多様性がある方がいい（ガウス分布に沿うような）
            →さまざまなパターンを学習できるため
            →偏ったデータでは勾配が消失する可能性

            ネットワークが深いと非線形変換が何度も行われ，各層の入力データの分布が大きく変わる（内部の共変量シフト）
            →学習が進まない
            →パラメータ初期値に注意を払う必要性

            よって，各ノードの値をミニバッチ単位で正規化（パラメータのスケールを揃える）

            線形変換-非線形変換の間で行われることが多い
            →FC - BN - ReLU - FC - BV - ReLU のように

            パラメータ初期値に払う注意が小さくなる
            正則化の効果も持つ（L2正則化やDropoutの必要性がsちいさくなる）
            バッチサイズが小さすぎると機能しにくい

            スケーリング・シフトを行う
            →必ず平均0分散1になるよりは，学習にあった分布にしたい
            →$\gamma, \beta$の学習によって決まる項を組み合わせ最適な分布にする

            テスト時
            全訓練データ（フルバッチ）の平均・分散で正規化
            →テストデータでやると，１つの予測だけの際に使えない
            ミニバッチの平均・分散と移動平均でフルバッチの平均・分散を推定
            $$
              \mu_{new} = m\mu_{old} + (1 - m)\mu
            $$

        1. ## <a id="">layer normalization</a>
            同じ層のニューロン間で正規化
            一つのサンプルに対して全てのチャネルにまたがって行う
            バッチ正規化と異なり，トレーニング・テストで同じ計算を実行
            オンライン学習やRNNに拡張（1サンプルに対して行うため）

        1. ## <a id="">instance normalization</a>
            各チャネルで独立に画像に対して行う
            画像生成の分野でバッチ正規化の代替として注目
            画像以外の分野へは拡張しづらい
            バッチサイズが十分であれば，バッチ正規化のみで十分である

        1. ## <a id="">group normalization</a>
            チャネルをG個にグルーピングし，layer, instance normalizationの中間的な処理を行う
            物体検出やセグメンテーションで効果を発揮

## <a id = "cp6">Chapter 6</a>
1. ## <a id="">RNN</a>
    回帰型NN

    時系列データの処理
    →循環構造
    →出力を入力に戻す
    →BPができない
    →過去の中間層の情報を次の入力に加える（ループの展開）
    →流れが一方通行になったので（循環でない）BPができる
    →BPTT(Back Propagation Through Time)

    1. ## <a id="">順伝播</a>
        $t$番目の入力と$t-1$層の出力が$t$層の入力となり，それぞれに重みとバイアスを計算し$t+1$層の入力へとつなげる．
        予測値算出・損失計算ではこの出力を活性化関数に通し，ラベルと比較する
        $$
          h_{next} = Act(x_t W_x + h_{pre} W_h + b)\\
          {\hat y_t} = Act(h_{next} W_o + c)
        $$

    1. ## <a id="">BPTTと教師強制</a>
        BPTT
        時間軸方向の逆伝播

        BPTTの課題
        - 並列処理が不可能
        - 全時刻の中間状態を保存→メモリコストが高い

        教師強制
        前層の中間層の代わりに前層の正解ラベルを用いる
        →計算コスト大幅減
        →並列処理可能（時系列的関係を切り離して学習可能）
        - 参照する前層の情報
            - 通常のRNN：前時刻のRNNユニットの状態（訓練・テスト時）
            - 教師強制
                - 訓練時：前時刻の正解ラベル
                - テスト時：前時刻の出力層の状態

        Truncated BPTT
        通常BPTTの全中間層保存によるメモリコストを改善
        FPの接続は切らず，BPの接続を切る
        →ユニット単位でのBPを実行できる

    1. ## <a id="">深層回帰</a>
        RNNで回帰を深くし，予測能力を高めたい

        - 階層別に回帰
            RNNの状態を複数の層に分解
            各層で回帰的な結合を持たせる
            →下部の層が入力データをより適切な隠れ層の状態変換できる
        - 中間層でより深い接続
            RNNを3つの演算ブロックに切り分け，各ブロックで深い計算を持たせる（MLPなど）
            - 入力 - 隠れ層
            - 隠れ層 - 隠れ層
            - 隠れ層 - 出力層
        - スキップ接続
            3ブロックで深層化することで最適化が困難に
            →前層よりも前の状態と接続し，パラメータ数を抑える
            →データの最短経路が短くなり最適化しやすくなる

1. ## <a id="">再帰型RNN (Tree-RNN)</a>
    回帰結合型NNのもう一つの一般形
    （最近の使用例は少ない）

    子の表現特徴ベクトルを用いて親の表現特徴ベクトルを計算

    応用例
    - 自然言語処理
    - プログラミング言語の意味解析

    - 回帰型NN
        - RNN (Recurrent Neural Network)
            入力を前（あるいは後）から順番に受け取り，表現ベクトルを構成
    - 再帰型NN
        - Tree-RNN
            入力を木構造に沿って処理し，表現ベクトルを構成

    再帰型RNNもRNNと記載されることもあるので内容に注意

1. ## <a id="">長期依存性の解消　LSTM</a>
    長期依存性
    言語処理などにおいて，予測位置から遠い入力が予測に影響することがある
    →通常RNNでは位置が遠ければ依存性も低くなる
    →長期的な依存関係を学習するために「勾配爆発・勾配消失」の解決が必要

    - 勾配爆発
        各時刻でのBPの度に$W_f (if\,\,>1)$が乗されると，勾配が指数関数的に大きくなる
        - 勾配クリッピング
            BPにて勾配の上限を設定
            勾配ベクトル$\Delta\omega^{(t)}$のノルム(L2が一般的)が上限値$g$を超えた時，下式でパラメータ更新の大きさを調整
            $$
              \Delta\omega^{(t)} = \frac{g}{||\Delta\omega^{(t)}||}\Delta\omega^{(t)}
            $$

    - 勾配消失
        各時刻でのBPの度に活性化関数の微分が乗されるので，勾配が指数関数的に小さくなる
        （ReLUを除く）
        - スキップ接続・Leaky接続
            - スキップ接続(skip connection)
                隠れ層の状態をより先の層へ伝える
                →粗い時間スケールで動作し，遠い過去から現在までの情報伝達を効率化
                →これまで捉えられなかった長期依存性に希望がさす
                →通常の接続とスキップ接続両者が存在するため，勾配爆発の可能性は残る
            - Leaky接続
                前時刻からの入力を$\alpha$倍，入力層からの接続を$1-\alpha$倍して接続
                $\alpha$が1に近ければ，過去の記憶を長期間記憶
                $\alpha$が0に近ければ，過去の記憶は急速に破棄される
                $\alpha$は初期値で
                何らかの分布からサンプリングし固定
                適当な値として学習
                のいずれか
                Leakyユニットで異なる時間スケールを持つことは，長期依存性を扱う上で役に立つ
        - ゲート付きRNN
            記憶すべき情報を記憶しきれない
            →メモ（記憶セル）を追加
            →ユニットに入力層，前の隠れ層，前の記憶セルを入力
            →次の隠れ層，記憶セル(CEC)を出力
            （記憶セルは活性化関数を通さないので，勾配消失を防ぐことができる）
            →LSTM
            - 忘却ゲート forget gate
                CECから不要な記憶を忘却
                $$
                  f = \sigma\left( x_t W_x^{(f)} + h_{pre} W_h^{(f)} + b^{(f)}\right)\\
                  c_{pre} = f\odot c_{pre}
                $$
                $\sigma$はsigmoid関数（0~1）で$c_{pre}$の情報を削ぎ落とす
            - インプットゲート input gate
                情報を取捨選択し，CECにメモ
                $$
                  g = \textrm{tanh}\left( x_t W_x^{(g)} + h_{pre} W_h^{(g)} + b^{(g)}\right)\\
                  i = \sigma\left( x_t W_x^{(i)} + h_{pre} W_h^{(i)} + b^{(i)}\right)\\
                  c_{pre} += g\odot i
                $$
                $g$：記憶する情報
                $i$：どれだけメモするかの割合
                もともとあった情報に足すだけ
            - アウトプットゲート output gate
                CECの情報のうち，次の隠れそうにどの程度流すか調整
                $$
                  o = \sigma\left( x_t W_x^{(o)} + h_{pre} W_h^{(o)} + b^{(o)}\right)\\
                  h_{next} = o\odot \textrm{tanh}(c_{pre})
                $$
                $o$は0~1なのでCECをどれだけ次の層に渡すか調整される
            - 補足　ピープホール付きLSTM
                各ゲートでCECを覗き見るLSTM
        - GRU
            LSTMの表現力を保ちつつ，計算コストを低減
            - 残った機能
                隠れ状態に忘却機能と記憶機能を付与
                →勾配消失が起きづらい
            - 失った機能
                忘却・記憶機能がトレードオフ
                CECがなく，メモができない
                →過去の情報が残りづらい

            - reset gate
                過去の隠れ状態をどれだけ反映させるか
                $$
                  r = \sigma\left( x_t W_x^{(r)} + h_{pre} W_h^{(r)} + b^{(r)}\right)\\
                  h_{reset} = r\odot h_{pre}
                $$
                LSTMの忘却でゲートと同様の動き
            - update gate
                $$
                  \tilde{h} = \textrm{tanh}\left( x_t W_x + h_{reset} W_h + b\right)\\
                  z = \sigma\left( x_t W_x^{(z)} + h_{pre} W_h^{(z)} + b^{(z)}\right)\\
                  h_{next} = (1-z)\odot h_{pre} + z\odot \tilde{h}
                $$
                $\textrm{tanh}$：記憶する情報
                $\sigma$：記憶する割合
                $z$：$\sigma$
                $(1-z)\odot h_{pre}$：forget機能
                $z\odot \tilde{h}$：input機能

                forget, inputがトレードオフ

    - RNN系ユニットまとめ
        - simple RNN
            時系列データを用いる
            過去の情報を循環
        - LSTMユニット
            長期依存性（特に勾配消失）の解消
            CECによるメモを導入
        - GRUユニット
            LSTMの計算コスト低減

1. ## <a id="">双方向RNN</a>
    順方向RNN，逆方向RNNの結果をマージ
    - メリット
        後ろからの文脈情報を得られる
        →RNNよりも精度が向上することも

    出力と隠れ層の次元が単方向と比べ倍になる
    →順・逆方向の結果それぞれを保持するため

1. ## <a id="">RNN アーキテクチャ</a>
    1. ## <a id="">双方向伝播</a>
        双方向RNNは，simple RNNユニットを二つ重ねて
        互いに逆方向の伝播をさせる
        →ユニットを取り替え，双方向LSTMも可能

    1. ## <a id="">ユニット数を増やす</a>
        どのユニットもユニット数を増やしてNNを深くすることが可能
        縦横に伸ばせる
        →計算コスト高くなる

## <a id = "cp7">Chapter 7</a>
1. ## <a id="">生成モデル</a>
    1. ## <a id="">潜在変数</a>
        潜在的な特徴を表す変数

        次元圧縮後の状態
        e.g.) 説明変数（月間平均気温・湿度）→潜在変数（季節）

    1. ## <a id="">Encoder-Decoder modele</a>
        [データ] - [Encoder] - [潜在変数] - [Decoder] - [生成データ]

        1. ## <a id="">Auto Encoder</a>
            入力と同じ物を出力させる
            →中間層の次元圧縮された情報が重要

            応用例
            - 画像のノイズ除去(Denoising AE)
            - クラスタリング
                Encoderで圧縮した特徴分布上でクラスタリング
                →より重要な特徴をもとにクラスタリングできる
        1. ## <a id="">VAE Variational Auto Encoder</a>
            - AE
                存在するデータを忠実に再現したい
                →潜在変数にあそびがない
            - VAE
                実在しないデータを生成したい
                →潜在変数にランダム性や連続性を持たせ，中間の状態などを作り出せる

            潜在変数が正規分布に従うように調整

            - 学習時
                [Encoder] - [$\mu, \sigma^2$] - [潜在変数]
                この過程で潜在変数の平均$\mu$と分散$\sigma^2$を出力
                (AEでは直接潜在変数を求めていた)

                出力された$\mu, \sigma^2$から正規分布${\mathcal N}(\mu, \sigma^2)$に従う潜在変数をサンプリング

                後の流れは同じ

                BPではランダムにサンプリング($z \verb|~| N(\mu, \sigma^2)$)している点を
                ガウシアンノイズ$\epsilon\verb|~|{\mathcal N}(0,1)$に代替
                FPでの$\epsilon$を記憶すればBP可能
                $z = \mu + \sigma^2\times\epsilon$

            - 損失関数
                $$
                  {\mathcal L} = D_{KL}[q(z|{bm X})||p(z)]
                   - {\mathbb E}_{q(z|{\bm X})}[\textrm{log}\,p({\bm X}|z)]\\
                   = D_{KL}[{\mathcal N}(\mu({\bm X}),\Sigma({\bm X}))||{\mathcal N}(0,1)] + \beta||{\bm Y} - {\bm X}||^2
                $$
                第一項：Encoderが求めた分布と${\mathcal N}(0,1)$との近さ
                第二項：入力データと出力データの近さ(MSE)

                AEの損失関数
                $$
                  {\mathcal L} = \beta||{\bm Y} - {\bm X}||^2
                $$

            - 実用と課題
                - 実装が容易，理論が整っている
                - 入力が画像の場合，VAEのサンプルはややぼやける傾向にあり，原因は不明
                    損失を小さくするために，出力が0,1ではっきりしたものよりも小数の値を持つことで画像がぼやける傾向にある
        1. ## <a id="">GAN Generated Adversarial Network</a>
            VAEの課題
            - 画像がぼやける
                1. ガウス分布による正規化のために，データに制約がかかり出力にノイズが載ったのか
                1. 鮮明な画像(0,1)よりもぼやけた画像(小数)の方が損失(MSE)が下がるのか

            鮮明な画像のために，ノイズの除去・損失関数を変える，が必要．

            GAN
            GeneratorがDiscriminatorを騙すように画像を生成
            [ノイズ z] - [Generator G(z)] - [偽データ ${\hat x}$ + 真データ x] - [Discriminator D(x)] - [真偽(0,1)]

            生成時
            [ノイズ z] - [Generator G(z)] - [偽データ ${\hat x}$]
            一様乱数からノイズzをサンプリング
            Generatorで新たなデータを生成する

            - 目的関数
                $$
                  \underset{G}{\textrm{min}}\,\underset{D}{\textrm{max}}\,V(D,G) =
                  \underset{G}{\textrm{min}}\,\underset{D}{\textrm{max}}\,
                  {\mathbb E}_{x\verb|~|p_{data}(x)}\left[\textrm{log}D(x)\right] +
                  {\mathbb E}_{z\verb|~|p_{z}(z)}\left[\textrm{log}(1-D(G(z)))\right]
                $$
                $G(z)$：ノイズzからGが生成したデータ
                $D(x)$：訓練データxが実在するデータである確率
                - DはVを最大化
                    - first term
                        訓練データ分布$p_{data}(x)$から得たデータxが「訓練データである」と判別する確率$D(x)$の最大化
                    - second term
                        生成データ分布$p_z(x)$から得たデータ$G(x)$が「生成データである」と判別する確率$(1 - D(G(z)))$の最大化
                - GはVを最小化
                    - first term
                        関係なし(Dが訓練データをどう判別するかは関係ない)
                    - second term
                        生成データ分布$p_z(x)$から得たデータ$G(x)$が「生成データである」とDに判別させる確率$(1 - D(G(z)))$の最小化

                - 最適化手法はSGD(確率的勾配降下法)
                    m : ミニバッチサイズ
                    1-1. m個のノイズzとデータxをサンプリング
                    1-2. SGDでDiscriminator更新
                        $$
                          \nabla_{\theta_d}\frac{1}{m}\sum_{i=1}^{m}
                          \left[\textrm{log}D({\bm x}^{(i)}) +
                          \textrm{log}\left(1 - D\left(G({\bm z}^{(i)})\right)\right)\right]
                        $$
                    2-1. mこのノイズzをサンプリング
                    2-2. SGDでGenerator更新
                        $$
                          \nabla_{\theta_g}\frac{1}{m}\sum_{i=1}^{m}
                          \left[\textrm{log}\left(1 - D\left(G({\bm z}^{(i)})\right)\right)\right]
                        $$
                    これらを全てのミニバッチに対して繰り返す

                - 学習が困難
                    - D-Gが拮抗するはずが，Dが圧勝し勾配が消失する
                        →Dを小さいネットワークにする
                        →DのDropout rateを大きめにする
                        →Unrolled GANを用いる
                    - mode collapse
                        →Minibatch Discriminator
                        →Wasserstein GAN

                    - Unrolled GAN
                        DをKステップ学習させた後に，Gを学習させる
                        →GがKステップ分先取り学習できる(良い勾配情報で学習できる)
                        →学習のバランスをとる(GよりDの方が早く学習してしまう問題を解決)

                        効果：Gに「質の良い勾配情報(hint)」を与えることになり，学習のバランスが取りやすくなる

                    - mode collapse(モード崩壊)
                        Gの能力不足により，全体の分布を近似しきれなく，ある一つのデータ(最頻値=mode)だけを出力しようとしている状態

                        対策
                        - Minibatch Discriminator
                            Dにmode collapseが起きているかのヒントを与える処理
                            →同バッチ内の画像間類似度を計算する
                            →mode collapseが起きている場合，同じ様なデータが多くなる=類似度が高くなる
                            →fakeデータと判別しやすくなる
                        - Wasserstein GAN
                            GANとは異なる距離を用いる

                            Gは生成データの分布を訓練データの分布に近づけようとする

                            - 通常GAN
                                JSD(イェンセンシャノンダイバーシティ)
                                →分布の裾が狭い時，全く異なる形状の分布同士でもJSDが低くなる問題
                            - Wasserstein GAN
                                EMD
                                →分布(砂山)を別の分布に移動する際，「どのくらいの量の砂を運ばなければいけないか」を数値化した指標
                                →EMDであれば異なる形状の分布間の距離は大きいと判断してくれる

## <a id = "cp8">Chapter 8</a>
1. ## <a id="">強化学習</a>
    最適な意思決定のルールを求める
    →教師ありでもなしでもない
    明確な正解は存在していないが，大きな目標や行動はある
    1. ## <a id="">用語</a>
        - エージェント
            環境で動く主体
        - 環境
            エージェントが動く範囲や状態
        - 状態
            行動をした後の環境の評価
        - 行動
            次に何をするか
        - 状態遷移確率
            次の行動で状態がどう変わるかの確率分布
            →ほとんどの場合で正確にわからない
        - 報酬
            行動を起こした際に得られる価値
    1. ## <a id="">マルコフ決定過程 MDP</a>
        従来の「状態」のみの確率過程(マルコフ過程)とは異なり，
        「行動」や「報酬」などを追加したものが「マルコフ決定過程」
        →行動選択ルールの最適化のために「行動」や「報酬」が必要

        $s_t$ : 時間ステップtの状態
        $a_t$ : 時間ステップtの行動
        $p(s_{t+1}|s_t,a_t)$ : 状態$s_t$で$a_t$を選択し，状態$s_{t+1}$になる状態遷移確率
        →行動をしてそのままの状態かもしれない．わからないので確率で吸収して表現．
        $r_t$ : 実行した$a_t$において得られる(即時)報酬

        $$
          p(s_{t+1},s_t,a_t) = p(s_{t+1}|s_t,a_t)p(s_t|a_t)p(a_t)\\
          (p(s_t,a_t) = p(s_t|a_t)p(a_t))
        $$
        $p(a,b)$ : aとbの同時確率
        $p(a|b)$ : bの下でaが起こる確率

        1. 時間ステップt=0に初期化．初期状態確率$p_{s_0}$に従い初期状態$s_t$を観測
        1. 状態$s_t$に対して行動$a_t$を選択
        1. 行動$a_t$を実行．得られた報酬$r_t$と状態遷移確率$p(\cdot|s_t,a_t)$により定まる，次の状態$s_{t+1}$を観測
        1. 時間ステップtを一つ進め，手順2に戻る

        - マルコフ性
            次の状態が，現在の状態のみによって決定する性質
            →$s_{t+1}$の決定にあたり，$t=t-1$以前の状態に依存しない．$s_t$のみによって決まる
            $$
              p(s_{t+1}|s_1,\cdots,s_{t-1},s_t) = p(s_{t+1}|s_t)
            $$

    1. ## <a id="">方策</a>
        どの様にして行動を選択するか
        →方策

        最適な方策$\pi$を探索することが強化学習の目的

        [状態 $s_t$] - <方策 $\pi(a_t|s_t)$> - [行動 $a_t$] - <状態遷移確率 p(s_{t+1|s_t,a_t})> - [状態 $s_{t+1}$] - [報酬 $r_t$]

    1. ## <a id="">報酬</a>
        ある状態でとった行動が学習に寄与した価値

        報酬関数は設計者が与えるもの
        $$
          r_t = g(s_t,a_t,s_{t+1})
        $$
        →設計が困難

        加えて，将来的な報酬を考慮する必要性
        →直近では学習に価値をもたらせないが，将来的に価値をもたらす行動を評価したい
        →一方で，将来が遠いほど不正確な評価になる
        →割引率$\gamma$を導入し報酬に反映

        割引累積報酬$R_t$
        $$
          R_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots
          = \underset{K\to \infty}{\textrm{lim}}\sum_{k=0}^{K}\gamma^k r_{t+k}
         $$


    1. ## <a id="">目的関数(価値関数)</a>
        報酬の期待値を目的関数とし，エージェントの行動を表現
        →累積報酬和$R_t$を状態$s_t$の価値とし，「価値の高い状態に遷移する」という方策が作れそう

        方策$\pi$の下での状態$s_t$の価値関数
        →ある方策$\pi$での状態$s$が初期状態$s_0$だった時の累積報酬和$R_0$の期待値が右辺
        $$
          V_{\pi}(s) = \mathbb{E}_\pi \left[R_0|s_0=s\right]
        $$
        ($\mathbb{E}_\pi[X|Y]$は，条件$Y$が与えられた確率変数$X$の$\pi$についての期待値)

        1. ## <a id="">最適な目的関数</a>
            最適な価値関数$V^*(s)$は，ある方策における累積報酬和の期待値の最大値
            $$
              V^*(s) = \underset{\pi}{\textrm{max}}\mathbb{E}_\pi \left[R_0|s_0=s\right]\\
              \cdots\\
              = \underset{\pi_0}{\textrm{max}}\sum_{a_0}\pi_0(a_0|s)
              \left[
              g(s,a_0,s_1) + \gamma\sum_{s_1}p(s_1|s,a_0)V^*(s_1)
              \right]
            $$
            ($\Sigma$ありうる次の状態への遷移確率$\times$その最適価値)

            1. ある状態の最適価値関数$V^*(s)$は，次の状態の最適価値関数$V^*(s^{\prime})$によってのみ再帰的に表現
            1. 最適な行動を選択できること$(\pi_0(a|s)=1)$＝最適な方策

            よって最適な方策$\pi=1$についてのみ考えればよい
            →それ以外の方策は最適でない$=0$
            $$
              V^*(s) = = \underset{a}{\textrm{max}}
              \left[
              g(s,a,s^{\prime}) + \gamma\sum_{s^{\prime}}p(s^{\prime}|s,a)V^*(s^{\prime})
              \right]
            $$
            →最適ベルマン方程式

            「価値を最大化する行動をとる」
            ＝
            「価値を最大化する最適な方策を見つける」


        1. ## <a id="">2種類のアプローチ</a>
            - 価値関数ベース
                価値を最大化するような「行動」をする
                価値(=モデル)を基準に，行動の選択は別のアルゴリズムで決定
                価値を正しく見積もることが重要
            - 方策勾配ベース
                価値を最大化する「最適な方策」を見つける
                行動の選択(方策(=モデル))そのものをモデルで決定する

            1. ## <a id="">価値関数ベース</a>
                価値を正確に見積もる
                どのような行動をとるかよりも，その時の状態を目的(勝利など)により近づける
                1. 現在の状態の価値を推測
                1. 一手先の状態を全て列挙．最も価値の高い状態を選択
                1. AI同士で対戦した結果から，状態価値を学習．
                    探索による先読みと組み合わせ，性能の高い方策を実現
            1. ## <a id="">方策勾配ベース</a>
                そもそもの価値関数導入の背景
                累積報酬和$R_t$を状態$s_t$の価値とし，「価値の高い状態へ行く」方策を考えるため

                方策$\pi$の下での状態$s_t$の価値関数
                $$
                  V_{\pi}(s) = \mathbb{E}_{\pi}[R_0|s_0=s]
                $$

                $V_{\pi}(s)$を全て調べることで，高い$V_{\pi}(s)$を持つ状態をたどるルートがわかる
                ＝
                最適な方策が間接的にわかる

                行動価値関数$Q_{\pi}(s,a)$の導入
                $$
                  Q_{\pi}(s,a) = \mathbb{E}_{\pi}[R_0|s_0=s, a_0=a]
                $$
                ある状態における，ある行動をとった時の価値
                →価値関数の一部(価値関数における，ある行動$a$をとった先の価値)
                →行動を一つ一つに分けて考えていく必要があるため，導入

                価値関数：状態$s$であらゆる行動をした時の報酬を全て計算
                行動価値関数：状態$s$である行動$a$をした時の報酬を全て計算

        1. ## <a id="">価値関数ベースのアプローチ</a>
            - 価値反復法
                環境ダイナミクス(状態遷移確率$p$と報酬$r$)が既知の問題で有効
                →最後の状態から最適ベルマン方程式に近づけていく
                e.g.) 迷路には適する．自動運転は不適．
            - 報酬のサンプリング
                未知の環境ダイナミクスを実際に行動をしてサンプルを集める
                →行動をした結果こうなった→遷移確率はこうだろう
                - モンテカルロ法
                    エピソディックに学習する(有限の時間ステップで終了する場合(エピソディック)に有効)
                    →何度も最後までプレイし，報酬をサンプリング
                    $$
                      V(s_t) \leftarrow V(s_t) + \alpha[\underline{R_t} - V(s_t)]\\
                      Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha[\underline{R_t} - Q(s_t,a_t)]
                    $$
                    とりあえずの累積報酬和$R_t$がわかるので，価値関数を仮の$R_t$に近づけていく($\alpha$が1に近い時)⇄価値関数に近づけていく($\alpha$が0に近い時)
                    その時の$R_t$が正しいかもわからないので，$\alpha$を乗じて補正をかける
                    e.g.) 五目並べは適合．テトリスは不適．(1エピソードが長い場合も不適)
                - TD法 Temporal Difference Learning
                    1ステップごとにサンプリング
                    →終了を待たずに，それまでに得た宝珠から価値関数を更新
                    $$
                      V(s_t) \leftarrow V(s_t) + \alpha[\underline{r_{t+1} + \gamma V(s_{t+1})} - V(s_t)]
                    $$
                    下線部がモンテカルロ法と異なる
                    →最後までの状態がわからないため，即時報酬$r_{t+1}$と補正した価値関数$\gamma V(s_{t+1})$で評価

                    - Q関数
                        方策オフ型：実際に進む行動(方策)と価値関数更新に用いる行動(方策)が異なる(こっちの方が直感的)
                        価値関数$V$に代わり，行動価値関数$Q$を用いて行動を選択
                        $$
                          Q_{\pi}(s,a) = \mathbb{E}_{\pi}[R_0|s_0=s,a_0=a]
                        $$
                        行動価値関数$Q$を用いて，未来の状態を計算しなくても最適な行動ができる
                        →学習が安定する(Q関数の収束が早い)
                    - SARSA
                        方策オン型：実際の方策($\epsilon$-greedy)に従って価値を更新
                        （エピソード更新と価値関数更新お方策が同じ）

                        Q学習では予測がある程度正確でないとmax Qが適切に定まらない
                        →五目並べで最初に盤面全てを予測できないのと同じ
                        →$\epsilon$-greedy方策に従って行動価値を学ぶ

                        SARSAはmax Qではなく，現時点での方策に従って遷移する$Q(s_{t+1},a_{t+1})$を更新に使用する
                        →その時点での最も良い選択は探索せず，方策にしたがて価値を更新する
                        →価値関数の更新にランダム性を取り込んでいるので局所界に陥りにくい
            - サンプリングの問題点
                初期値依存性が高い

                モンテカルロ法・TD法の流れ
                1. 価値関数$V$または行動価値関数$Q$に従って行動
                1. 報酬をサンプリング
                1. $V$や$Q$を更新
                偏ったサンプリングでの方策の更新では局所界に陥る
                →一定程度学習した方策を無視して行動する($\epsilon$-greedy方策，ソフトマックス)

                - $\epsilon$-greedy方策
                    確率$\epsilon$で完全にランダムに行動(大体0.01~0.1くらいの確率)

                - ソフトマックス方策
                    状態行動価値$Q(s,a)$をエネルギーと見做し，ボルツマン分布に従い行動を選択（ソフトマックス関数と同じ形）

                    全行動$\mathcal{A}$から行動aが選択される方策$\pi(s_t,a)$
                    $$
                      \pi(s_t,a) = \frac{\textrm{exp}\left(\frac{Q(s_t,a)}{T}\right)}
                      {\sum_{b\in \mathcal{A}}{\textrm{exp}\left(\frac{Q(s_t,b)}{T}\right)}}
                    $$
                    $Q(s,a)$が高いほど選択される
                    正の定数$T$(温度)を適切に設定し，確率に対する$Q(s,a)$の影響を制御する

                    ボルツマン分布
                    低エネルギー状態ほど，指数的に高い確率で出現する現象を表す確率分布
                    指数の肩の符号がソフトマックス分布と異なる
            - 価値関数ベース　まとめ
                - 価値関数
                    ある状態sから方策$\pi$に従って行動した際に，今後どのくらいの報酬を得ることができるかを表す指標
                - 代表例
                    Q学習
                - Q学習 問題点
                    状態・行動が増えすぎると処理できなくなる(全ての行動を考え，maxをとるため)
                    →DNNなどを用いてパラメトリックに価値関数を表現するDQNなどがある

        1. ## <a id=""方策勾配法ベースのアプローチ</a>
            方策を直接予測
            未来の状態を列挙する必要がない
            相性の良い(モンテカルロ木探索→alpha Go　など)と組み合わせることで高い性能の方策を実現

            復習（方策・価値ベースの違い）
            - 価値関数ベース：価値を基準にし，行動の選択は別のアルゴリズム(ソフトマックスや$\epsilon$-greedyなど)で決定
            - 方策勾配ベース：行動の選択そのもの(方策)をモデルで決定

            1. ## <a id="">方策勾配定理</a>
                パラメータ$\theta$を持つ方策を確率モデル$\pi_{\theta}$とする
                目的関数$J(\theta)$を最大化する$\theta$を，
                確率勾配$G_t^{\theta}$($J(\theta)$の$\theta$に関する確率方策勾配)を用いて，
                確率的勾配法に基づいて更新するアプローチ
                $$
                  \theta \leftarrow \theta + \alpha_t G_t^{\theta}\\
                  J(\theta) = V^{\pi_{\theta}}(s)
                  = \sum_{a} Q^{\pi_{\theta}}(s,a)\pi_{\theta}(a|s)
                $$
                $Q$ : 行動の価値
                $\pi$ : その行動をとる確率
                →を掛け合わせたもの(=期待値)の総和→価値関数→目的関数→最大化

                方策勾配定理
                目的関数$J$をパラメータ$\theta$で微分した式が以下で表されること
                $$
                    \nabla_{\theta} J(\theta)=
                    \mathbb{E}_{\pi_{\theta}}[
                    Q^{\pi_{\theta}}(s,a)\nabla_{\theta}\textrm{log}\pi_{\theta}(a|s)
                    ]
                $$
                (現在の方策における価値関数$times$行動の選択確率の対数の勾配)
                の方策に関する期待値
                →覚える

                この定理により，Q値のみ分かれば累積報酬を座右化させる方策の勾配が求まる

            1. ## <a id="">方策勾配法の工夫</a>
                方策勾配定理を元に方策を更新するが，$Q^{\pi_{\theta}}(s,a)$を正確に求めるのは困難
                →サンプリングした報酬の平均で近似→Q値の推定値
                $$
                  \nabla_{\theta} J(\theta) \approx
                  \sum_t r_t \nabla_{\theta}\textrm{log}\pi_{\theta}(a_t|s_t)
                $$
                →全ての時間ステップについて勾配計算を行うと，時間ステップ数分加算が行われ分散が大きくなる
                →学習が進みにくくなる(更新幅が大きく収束しにくい)
                →ベースライン関数$b(s)$を導入し，差し引くことで即時報酬を丸め込む
                →分散を小さくする
                $$
                  \nabla_{\theta} J(\theta) \approx
                  \left(r_t - b(s_t)\right) \nabla_{\theta}\textrm{log}\pi_{\theta}(a_t|s_t)
                $$
                →ベースライン関数に推定価値関数${\hat V}(s)$を用いることもあり，
                $\left({\hat Q}(s,a) - {\hat V}(s)\right)$をアドバンテージ関数${\hat A}(s,a)$という．
                (${\hat Q}(s,a) \approx r_t$)
                $$
                  \nabla_{\theta} J(\theta) \approx
                  {\hat A}(s_t,a_t) \nabla_{\theta}\textrm{log}\pi_{\theta}(a_t|s_t)\\
                  = \left({\hat Q}(s_t,a_t) - {\hat V}(s_t)\right)\nabla_{\theta}\textrm{log}\pi_{\theta}(a_t|s_t)
                $$

            1. ## <a id="">具体的なプローチ</a>
                - REINFORCEアルゴリズム
                    実際に得られた報酬の平均を使って近似した値をQ値の推定値とする
                    (方策勾配法を用いても，現状Q値の推定値は必要)

                    TステップからなるMエピソードを行った際の報酬の平均で近似する
                    $$
                      \nabla_{\theta} J(\theta) =
                      \frac{1}{M}\sum_{m=1}^M \frac{1}{T}\sum_{t=1}^T
                      \left(r_{m,t} - b\right) \nabla_{\theta}\textrm{log}\pi_{\theta}(a_t|s_t,\theta)\\
                      \\
                      b = \frac{1}{M}\sum_{m=1}^M \frac{1}{T}\sum_{t=1}^T r_{m,t}
                    $$
                - Actor-Critic
                    状態価値を学習(主にNN)→推定：Critic(推定器)
                    Criticの推定した${\hat V}(s_t)$をベースラインにしてActor(行動器)を改善していく

                    - A3C Asynchronous Advantage Actor-Critic
                        CriticをNNで学習したもの
                        $$
                          \nabla_{\theta} J(\theta,\phi) =
                          \left({\hat Q}(s,a) - {\hat V}_{\phi}(s)\right) \nabla_{\theta}\textrm{log}\pi_{\theta}(a|s)
                        $$
                        ($\phi$ : NNのパラメータ)

                        方策が決定的になった際，上式では更新が進みに悔いので
                        下式のようにエントロピー正則化項を加え，A3Cの勾配計算式を定義
                        →起こす行動が固まると，その行動の価値も固まり，QとVの差がほぼ0になる
                        →学習が進まない
                        $$
                          \nabla_{\theta} J(\theta,\phi) =
                          \left({\hat Q}(s,a) - {\hat V}_{\phi}(s)\right) \nabla_{\theta}\textrm{log}\pi_{\theta}(a|s)
                          \underline{- \lambda\sum_a \pi_{\theta}(a|s)\textrm{log}\pi_{\theta}(a|s)}
                        $$
                        ($\lambda( > 0)$は正則化係数)

                        ピーキーな方策の分布にしたくない(正則化項を導入)
                        →エントロピーを導入
                        →エントロピーを大きくする方向に学習が進む
